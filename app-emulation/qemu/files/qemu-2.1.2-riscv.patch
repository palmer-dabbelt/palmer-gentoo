diff --git a/.gitignore b/.gitignore
index 2286d0a..1b617a5 100644
--- a/.gitignore
+++ b/.gitignore
@@ -105,3 +105,4 @@ cscope.*
 tags
 TAGS
 *~
+/build/*
diff --git a/.travis.yml b/.travis.yml
index 89c30ae..d9b0ea0 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -1,3 +1,4 @@
+# add a comment
 language: c
 python:
   - "2.4"
@@ -5,11 +6,8 @@ compiler:
   - gcc
   - clang
 notifications:
-  irc:
-    channels:
-      - "irc.oftc.net#qemu"
-    on_success: change
-    on_failure: always
+    email:
+        - skarandikar@berkeley.edu
 env:
   global:
     - TEST_CMD="make check"
@@ -20,23 +18,24 @@ env:
     - GUI_PKGS="libgtk-3-dev libvte-2.90-dev libsdl1.2-dev libpng12-dev libpixman-1-dev"
     - EXTRA_PKGS=""
   matrix:
-    - TARGETS=alpha-softmmu,alpha-linux-user
-    - TARGETS=arm-softmmu,arm-linux-user
-    - TARGETS=aarch64-softmmu,aarch64-linux-user
-    - TARGETS=cris-softmmu
-    - TARGETS=i386-softmmu,x86_64-softmmu
-    - TARGETS=lm32-softmmu
-    - TARGETS=m68k-softmmu
-    - TARGETS=microblaze-softmmu,microblazeel-softmmu
-    - TARGETS=mips-softmmu,mips64-softmmu,mips64el-softmmu,mipsel-softmmu
-    - TARGETS=moxie-softmmu
-    - TARGETS=or32-softmmu,
-    - TARGETS=ppc-softmmu,ppc64-softmmu,ppcemb-softmmu
-    - TARGETS=s390x-softmmu
-    - TARGETS=sh4-softmmu,sh4eb-softmmu
-    - TARGETS=sparc-softmmu,sparc64-softmmu
-    - TARGETS=unicore32-softmmu
-    - TARGETS=xtensa-softmmu,xtensaeb-softmmu
+      #    - TARGETS=alpha-softmmu,alpha-linux-user
+      #    - TARGETS=arm-softmmu,arm-linux-user
+      #    - TARGETS=aarch64-softmmu,aarch64-linux-user
+      #    - TARGETS=cris-softmmu
+      #    - TARGETS=i386-softmmu,x86_64-softmmu
+      #    - TARGETS=lm32-softmmu
+      #    - TARGETS=m68k-softmmu
+      #    - TARGETS=microblaze-softmmu,microblazeel-softmmu
+      ##    - TARGETS=mips-softmmu,mips64-softmmu,mips64el-softmmu,mipsel-softmmu
+      #    - TARGETS=moxie-softmmu
+      #    - TARGETS=or32-softmmu,
+      #    - TARGETS=ppc-softmmu,ppc64-softmmu,ppcemb-softmmu
+      #    - TARGETS=s390x-softmmu
+      #    - TARGETS=sh4-softmmu,sh4eb-softmmu
+      #    - TARGETS=sparc-softmmu,sparc64-softmmu
+      #    - TARGETS=unicore32-softmmu
+      #    - TARGETS=xtensa-softmmu,xtensaeb-softmmu
+    - TARGETS=riscv-softmmu
 before_install:
   - git submodule update --init --recursive
   - sudo apt-get update -qq
@@ -46,36 +45,36 @@ matrix:
   # We manually include a number of additional build for non-standard bits
   include:
     # Debug related options
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
+    - env: TARGETS=riscv-softmmu
            EXTRA_CONFIG="--enable-debug"
       compiler: gcc
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
+    - env: TARGETS=riscv-softmmu
            EXTRA_CONFIG="--enable-debug --enable-tcg-interpreter"
       compiler: gcc
-    # All the extra -dev packages
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-           EXTRA_PKGS="libaio-dev libcap-ng-dev libattr1-dev libbrlapi-dev uuid-dev libusb-1.0.0-dev"
-      compiler: gcc
-    # Currently configure doesn't force --disable-pie
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-           EXTRA_CONFIG="--enable-gprof --enable-gcov --disable-pie"
-      compiler: gcc
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-           EXTRA_PKGS="sparse"
-           EXTRA_CONFIG="--enable-sparse"
-      compiler: gcc
-    # All the trace backends (apart from dtrace)
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-           EXTRA_CONFIG="--enable-trace-backends=stderr"
-      compiler: gcc
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-           EXTRA_CONFIG="--enable-trace-backends=simple"
-      compiler: gcc
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-           EXTRA_CONFIG="--enable-trace-backends=ftrace"
-           TEST_CMD=""
-      compiler: gcc
-    - env: TARGETS=i386-softmmu,x86_64-softmmu
-          EXTRA_PKGS="liblttng-ust-dev liburcu-dev"
-          EXTRA_CONFIG="--enable-trace-backends=ust"
-      compiler: gcc
+#    # All the extra -dev packages
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#           EXTRA_PKGS="libaio-dev libcap-ng-dev libattr1-dev libbrlapi-dev uuid-dev libusb-1.0.0-dev"
+#      compiler: gcc
+#    # Currently configure doesn't force --disable-pie
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#           EXTRA_CONFIG="--enable-gprof --enable-gcov --disable-pie"
+#      compiler: gcc
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#           EXTRA_PKGS="sparse"
+#           EXTRA_CONFIG="--enable-sparse"
+#      compiler: gcc
+#    # All the trace backends (apart from dtrace)
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#           EXTRA_CONFIG="--enable-trace-backends=stderr"
+#      compiler: gcc
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#           EXTRA_CONFIG="--enable-trace-backends=simple"
+#      compiler: gcc
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#           EXTRA_CONFIG="--enable-trace-backends=ftrace"
+#           TEST_CMD=""
+#      compiler: gcc
+#    - env: TARGETS=i386-softmmu,x86_64-softmmu
+#          EXTRA_PKGS="liblttng-ust-dev liburcu-dev"
+#          EXTRA_CONFIG="--enable-trace-backends=ust"
+#      compiler: gcc
diff --git a/README b/README
deleted file mode 100644
index c7c990d..0000000
--- a/README
+++ /dev/null
@@ -1,3 +0,0 @@
-Read the documentation in qemu-doc.html or on http://wiki.qemu-project.org
-
-- QEMU team
diff --git a/README.md b/README.md
new file mode 100644
index 0000000..7b8439c
--- /dev/null
+++ b/README.md
@@ -0,0 +1,74 @@
+riscv-qemu [![Build Status](https://travis-ci.org/ucb-bar/riscv-qemu.svg?branch=master)](https://travis-ci.org/ucb-bar/riscv-qemu)
+=========
+
+The riscv-softmmu target for full system emulation is currently supported. 
+It supports booting [riscv-linux] \(currently requires building from the 
+[qemu branch]\). A precompiled copy of the kernel is included in the 
+"hacking_files" directory for convenience (see Method 1 under installation).
+
+Prereqs:
+
+    $ sudo apt-get install gcc libc6-dev pkg-config bridge-utils uml-utilities zlib1g-dev libglib2.0-dev autoconf automake libtool libsdl1.2-dev
+
+Installation 
+--------------
+
+### Method 1 \(the quick way\):
+
+A sample kernel with an initramfs is included in the "hacking_files"
+directory. You can easily test out riscv-qemu this way:
+
+    $ git clone https://github.com/ucb-bar/riscv-qemu
+    $ cd riscv-qemu
+    $ git submodule update --init pixman
+    $ ./configure --target-list=riscv-softmmu
+    $ make
+    $ cd riscv-softmmu
+    $ # now, start qemu
+    $ ./qemu-system-riscv -kernel ../hacking_files/vmlinux/vmlinux -nographic
+
+To exit this system, hit `ctrl-a x`.
+
+### Method 2 \(system with persistent storage\): 
+
+Booting from a block device is also supported. A more extensive guide for 
+configuring the kernel/building a root fs will be available soon.
+
+####Step 1:
+
+    $ git clone https://github.com/ucb-bar/riscv-qemu
+    $ cd riscv-qemu
+    $ git submodule update --init pixman
+    $ ./configure --target-list=riscv-softmmu
+    $ make
+    $ cd riscv-softmmu
+
+####Step 2:
+
+Instructions for the following two steps are coming soon:
+
+**a)** Build linux kernel from the qemu branch of riscv-linux with htif block device support.
+
+**b)** Build the `root.bin` root filesystem.
+
+
+####Step 3:
+
+Now from the `riscv-softmmu/` directory, start `qemu-system-riscv`:
+
+    $ ./qemu-system-riscv -hda [your root.bin location] -kernel [your vmlinux location] -nographic
+
+**IMPORTANT**: To cleanly exit this system, you must enter `halt -f` at the prompt
+and then hit `ctrl-a x`. Otherwise, the root filesystem will likely be corrupted.
+
+Notes
+-----
+
+- Qemu also supports a "linux-user" mode, however this is currently not implemented for RISC-V.
+- Once in a while, you may see a message from qemu of the form `main-loop: WARNING: I/O thread spun for N iterations`. You may safely ignore this message without consequence.
+- Files/directories of interest:
+  - target-riscv/
+  - hw/riscv/
+
+[riscv-linux]:https://github.com/ucb-bar/riscv-linux
+[qemu branch]:https://github.com/ucb-bar/riscv-linux/tree/qemu
diff --git a/arch_init.c b/arch_init.c
index 8ddaf35..7276f8f 100644
--- a/arch_init.c
+++ b/arch_init.c
@@ -88,6 +88,8 @@ int graphic_depth = 32;
 #define QEMU_ARCH QEMU_ARCH_MICROBLAZE
 #elif defined(TARGET_MIPS)
 #define QEMU_ARCH QEMU_ARCH_MIPS
+#elif defined(TARGET_RISCV)
+#define QEMU_ARCH QEMU_ARCH_RISCV
 #elif defined(TARGET_MOXIE)
 #define QEMU_ARCH QEMU_ARCH_MOXIE
 #elif defined(TARGET_OPENRISC)
diff --git a/configure b/configure
index f49e618..6b36c7b 100755
--- a/configure
+++ b/configure
@@ -335,6 +335,7 @@ libssh2=""
 vhdx=""
 quorum=""
 numa=""
+riscv_htif=""
 
 # parse CC options first
 for opt do
@@ -408,7 +409,7 @@ QEMU_CFLAGS="-Wstrict-prototypes -Wredundant-decls $QEMU_CFLAGS"
 QEMU_CFLAGS="-D_GNU_SOURCE -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE $QEMU_CFLAGS"
 QEMU_INCLUDES="-I. -I\$(SRC_PATH) -I\$(SRC_PATH)/include"
 if test "$debug_info" = "yes"; then
-    CFLAGS="-g $CFLAGS"
+    CFLAGS="-g3 $CFLAGS"
     LDFLAGS="-g $LDFLAGS"
 fi
 
@@ -493,6 +494,8 @@ elif check_define _ARCH_PPC ; then
   fi
 elif check_define __mips__ ; then
   cpu="mips"
+elif check_define __riscv__ ; then
+  cpu="riscv"
 elif check_define __ia64__ ; then
   cpu="ia64"
 elif check_define __s390__ ; then
@@ -533,6 +536,9 @@ case "$cpu" in
   mips*)
     cpu="mips"
   ;;
+  riscv*)
+    cpu="riscv"
+  ;;
   sparc|sun4[cdmuv])
     cpu="sparc"
   ;;
@@ -1091,6 +1097,10 @@ for opt do
   ;;
   --enable-virtio-blk-data-plane) virtio_blk_data_plane="yes"
   ;;
+  --disable-riscv-htif) riscv_htif="no"
+  ;;
+  --enable-riscv-htif) riscv_htif="yes"
+  ;;
   --disable-gtk) gtk="no"
   ;;
   --enable-gtk) gtk="yes"
@@ -4114,6 +4124,9 @@ if test "$pie" = "no" ; then
       # room for the code_gen_buffer above it before the stack.
       textseg_addr=0x60000000
       ;;
+    riscv)
+      textseg_addr=0x400000
+      ;;
   esac
   if [ -n "$textseg_addr" ]; then
     cat > $TMPC <<EOF
@@ -4707,6 +4720,10 @@ if test "$vhdx" = "yes" ; then
   echo "CONFIG_VHDX=y" >> $config_host_mak
 fi
 
+if test "$riscv_htif" = "yes" ; then
+  echo 'CONFIG_RISCV_HTIF=y' >> $config_host_mak
+fi
+
 # USB host support
 if test "$libusb" = "yes"; then
   echo "HOST_USB=libusb legacy" >> $config_host_mak
@@ -4966,6 +4983,10 @@ case "$target_name" in
     TARGET_BASE_ARCH=mips
     echo "TARGET_ABI_MIPSN64=y" >> $config_target_mak
   ;;
+  riscv)
+    TARGET_ARCH=riscv
+    echo "TARGET_ABI_RISCV=y" >> $config_target_mak
+  ;;
   moxie)
   ;;
   or32)
@@ -5175,6 +5196,10 @@ for i in $ARCH $TARGET_BASE_ARCH ; do
     echo "CONFIG_PPC_DIS=y"  >> $config_target_mak
     echo "CONFIG_PPC_DIS=y"  >> config-all-disas.mak
   ;;
+  riscv*)
+    echo "CONFIG_RISCV_DIS=y"  >> $config_target_mak
+    echo "CONFIG_RISCV_DIS=y"  >> config-all-disas.mak
+  ;;
   s390*)
     echo "CONFIG_S390_DIS=y"  >> $config_target_mak
     echo "CONFIG_S390_DIS=y"  >> config-all-disas.mak
diff --git a/cpu-exec.c b/cpu-exec.c
index 38e5f02..cd28590 100644
--- a/cpu-exec.c
+++ b/cpu-exec.c
@@ -271,6 +271,7 @@ int cpu_exec(CPUArchState *env)
 #elif defined(TARGET_LM32)
 #elif defined(TARGET_MICROBLAZE)
 #elif defined(TARGET_MIPS)
+#elif defined(TARGET_RISCV)
 #elif defined(TARGET_MOXIE)
 #elif defined(TARGET_OPENRISC)
 #elif defined(TARGET_SH4)
@@ -327,7 +328,8 @@ int cpu_exec(CPUArchState *env)
                     }
 #if defined(TARGET_ARM) || defined(TARGET_SPARC) || defined(TARGET_MIPS) || \
     defined(TARGET_PPC) || defined(TARGET_ALPHA) || defined(TARGET_CRIS) || \
-    defined(TARGET_MICROBLAZE) || defined(TARGET_LM32) || defined(TARGET_UNICORE32)
+    defined(TARGET_MICROBLAZE) || defined(TARGET_LM32) || \
+    defined(TARGET_UNICORE32) || defined(TARGET_RISCV)
                     if (interrupt_request & CPU_INTERRUPT_HALT) {
                         cpu->interrupt_request &= ~CPU_INTERRUPT_HALT;
                         cpu->halted = 1;
@@ -443,6 +445,38 @@ int cpu_exec(CPUArchState *env)
                         cc->do_interrupt(cpu);
                         next_tb = 0;
                     }
+#elif defined(TARGET_RISCV) // TODO put back original mips
+                    if ((interrupt_request & CPU_INTERRUPT_HARD) &&
+                        cpu_riscv_hw_interrupts_pending(env)) {
+                        /* Raise it */
+#ifndef __GNUC__
+                        /* Lookup table indexed by the hash function
+                         * h(x) = (x * y) >> (n - log2(n)), where
+                         * x contains the least significant 1 bit,
+                         * y is the the De Bruijn sequence 00011101b,
+                         * and n is the number of bit positions (8).
+                         */
+                        static const unsigned char pos[8] = { 0, 1, 6, 2, 7, 5, 4, 3 };
+#endif
+                        uint32_t status, irq;
+                        status = env->helper_csr[CSR_STATUS];
+                        status = (status >> 24) & (status >> 16) & 0xFF;
+#ifdef __GNUC__
+                        irq = __builtin_ctz(status);
+#else
+                        /* Count the consecutive zero bits (trailing)
+                         * with multiply and lookup.
+                         * Refer to "Using de Bruijn Sequences to Index a 1
+                         * in a Computer Word" (1998) by Charles Leiserson,
+                         * Harald Prokop, and Keith Randall.
+                         * (v & -v) extracts the least significant 1 bit.
+                         */
+                        irq = pos[(((status & -status) * 0x1DU) >> 5) & 0x7];
+#endif
+                        cpu->exception_index = 0x80000000U | irq;
+                        cc->do_interrupt(cpu);
+                        next_tb = 0;
+                    }
 #elif defined(TARGET_OPENRISC)
                     {
                         int idx = -1;
@@ -724,6 +758,7 @@ int cpu_exec(CPUArchState *env)
               | env->cc_dest | (env->cc_x << 4);
 #elif defined(TARGET_MICROBLAZE)
 #elif defined(TARGET_MIPS)
+#elif defined(TARGET_RISCV)
 #elif defined(TARGET_MOXIE)
 #elif defined(TARGET_OPENRISC)
 #elif defined(TARGET_SH4)
diff --git a/cpus.c b/cpus.c
index 5e7f2cf..070851e 100644
--- a/cpus.c
+++ b/cpus.c
@@ -1339,6 +1339,9 @@ CpuInfoList *qmp_query_cpus(Error **errp)
 #elif defined(TARGET_SPARC)
         SPARCCPU *sparc_cpu = SPARC_CPU(cpu);
         CPUSPARCState *env = &sparc_cpu->env;
+#elif defined(TARGET_RISCV)
+        RISCVCPU *mips_cpu = RISCV_CPU(cpu);
+        CPURISCVState *env = &mips_cpu->env;
 #elif defined(TARGET_MIPS)
         MIPSCPU *mips_cpu = MIPS_CPU(cpu);
         CPUMIPSState *env = &mips_cpu->env;
@@ -1366,6 +1369,9 @@ CpuInfoList *qmp_query_cpus(Error **errp)
 #elif defined(TARGET_MIPS)
         info->value->has_PC = true;
         info->value->PC = env->active_tc.PC;
+#elif defined(TARGET_RISCV)
+        info->value->has_PC = true;
+        info->value->PC = env->active_tc.PC;
 #endif
 
         /* XXX: waiting for the qapi to support GSList */
diff --git a/default-configs/riscv-linux-user.mak b/default-configs/riscv-linux-user.mak
new file mode 100644
index 0000000..865b362
--- /dev/null
+++ b/default-configs/riscv-linux-user.mak
@@ -0,0 +1 @@
+# Default configuration for riscv-linux-user
diff --git a/default-configs/riscv-softmmu.mak b/default-configs/riscv-softmmu.mak
new file mode 100644
index 0000000..c8b7fa1
--- /dev/null
+++ b/default-configs/riscv-softmmu.mak
@@ -0,0 +1,38 @@
+# Default configuration for riscv-softmmu
+
+#include pci.mak
+#include sound.mak
+#include usb.mak
+#CONFIG_ESP=y
+#CONFIG_VGA=y
+#CONFIG_VGA_PCI=y
+#CONFIG_VGA_ISA=y
+#CONFIG_VGA_ISA_MM=y
+#CONFIG_VGA_CIRRUS=y
+#CONFIG_VMWARE_VGA=y
+CONFIG_SERIAL=y
+#CONFIG_PARALLEL=y
+#CONFIG_I8254=y
+#CONFIG_PCSPK=y
+#CONFIG_PCKBD=y
+#CONFIG_FDC=y
+#CONFIG_ACPI=y
+#CONFIG_APM=y
+#CONFIG_I8257=y
+#CONFIG_PIIX4=y
+#CONFIG_IDE_ISA=y
+#CONFIG_IDE_PIIX=y
+#CONFIG_NE2000_ISA=y
+#CONFIG_RC4030=y
+#CONFIG_DP8393X=y
+#CONFIG_DS1225Y=y
+#CONFIG_MIPSNET=y
+#CONFIG_PFLASH_CFI01=y
+#CONFIG_G364FB=y
+CONFIG_I8259=y
+#CONFIG_JAZZ_LED=y
+#CONFIG_MC146818RTC=y
+#CONFIG_VT82C686=y
+#CONFIG_ISA_TESTDEV=y
+#CONFIG_EMPTY_SLOT=y
+CONFIG_VIRTIO=y
diff --git a/hacking_files/debug.sh b/hacking_files/debug.sh
new file mode 100644
index 0000000..c186d98
--- /dev/null
+++ b/hacking_files/debug.sh
@@ -0,0 +1,3 @@
+#!/bin/bash
+
+cgdb qemu-system-riscv -x startqemu
diff --git a/hacking_files/debug_write.sh b/hacking_files/debug_write.sh
new file mode 100644
index 0000000..94e0410
--- /dev/null
+++ b/hacking_files/debug_write.sh
@@ -0,0 +1,3 @@
+#!/bin/bash
+
+./qemu-system-riscv -kernel ../hacking/vmlinux/vmlinux -nographic
diff --git a/hacking_files/startqemu b/hacking_files/startqemu
new file mode 100644
index 0000000..219df21
--- /dev/null
+++ b/hacking_files/startqemu
@@ -0,0 +1,3 @@
+handle SIGUSR1 noprint nostop
+break main
+run -kernel ../hacking/vmlinux/vmlinux -nographic
diff --git a/hw/riscv/Makefile.objs b/hw/riscv/Makefile.objs
new file mode 100644
index 0000000..edfcfaa
--- /dev/null
+++ b/hw/riscv/Makefile.objs
@@ -0,0 +1,3 @@
+obj-y += riscv_board.o
+obj-y += cputimer.o riscv_int.o
+obj-y += htif/htif.o
diff --git a/hw/riscv/cputimer.c b/hw/riscv/cputimer.c
new file mode 100644
index 0000000..3ad4763
--- /dev/null
+++ b/hw/riscv/cputimer.c
@@ -0,0 +1,102 @@
+/*
+ *  QEMU RISC-V timer support
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target timer support
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include "hw/hw.h"
+#include "hw/riscv/cpudevs.h"
+#include "qemu/timer.h"
+
+static uint64_t last_count_update;
+
+// should be the cpu freq
+#define TIMER_FREQ	500 * 1000 * 1000
+
+uint64_t cpu_riscv_get_cycle (CPURISCVState *env) {
+    uint64_t now;
+    now = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL);
+    // first, convert _now_ to seconds by dividing by get_ticks_per_sec
+    // and then multiply by the timer freq.
+    return muldiv64(now, TIMER_FREQ, get_ticks_per_sec());
+}
+
+static void cpu_riscv_timer_update(CPURISCVState *env)
+{
+    uint64_t now, next;
+    uint32_t diff;
+
+    now = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL);
+    diff = (uint32_t)(env->helper_csr[CSR_COMPARE] - env->helper_csr[CSR_COUNT]);
+    next = now + muldiv64(diff, get_ticks_per_sec(), TIMER_FREQ);
+    timer_mod(env->timer, next);
+}
+
+static void cpu_riscv_timer_expire(CPURISCVState *env)
+{
+    cpu_riscv_timer_update(env);
+    qemu_irq_raise(env->irq[7]);
+}
+
+uint32_t cpu_riscv_get_count (CPURISCVState *env)
+{
+    uint64_t diff;
+
+    diff = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL) - last_count_update;
+    return env->helper_csr[CSR_COUNT] +
+        (uint32_t)muldiv64(diff, TIMER_FREQ, get_ticks_per_sec());
+}
+
+void cpu_riscv_store_count (CPURISCVState *env, uint32_t count)
+{
+    /* Store new count register */
+    env->helper_csr[CSR_COUNT] = count;
+    last_count_update = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL);
+
+    /* Update timer timer */
+    cpu_riscv_timer_update(env);
+}
+
+void cpu_riscv_store_compare (CPURISCVState *env, uint32_t value)
+{
+    env->helper_csr[CSR_COMPARE] = value;
+    qemu_irq_lower(env->irq[7]);
+    // according to RISCV spec, any write to compare clears timer interrupt
+    cpu_riscv_timer_update(env);
+}
+
+static void riscv_timer_cb (void *opaque)
+{
+    CPURISCVState *env;
+    env = opaque;
+
+    env->helper_csr[CSR_COUNT]++;
+    cpu_riscv_timer_expire(env);
+    env->helper_csr[CSR_COUNT]--;
+}
+
+void cpu_riscv_clock_init (CPURISCVState *env)
+{
+    env->timer = timer_new_ns(QEMU_CLOCK_VIRTUAL, &riscv_timer_cb, env);
+    env->helper_csr[CSR_COMPARE] = 0;
+    cpu_riscv_store_count(env, 1);
+}
diff --git a/hw/riscv/htif/htif.c b/hw/riscv/htif/htif.c
new file mode 100644
index 0000000..c7913fd
--- /dev/null
+++ b/hw/riscv/htif/htif.c
@@ -0,0 +1,256 @@
+/*
+ *  QEMU RISC-V Host Target Interface (HTIF) Emulation
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *
+ * Since directly accessing CPU registers is not ideal, we map reads and writes 
+ * to the tohost/fromhost registers onto memory addresses 0x400 and 0x408 
+ * respectively (see the csr instructions in target-riscv/translate.c).
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include "hw/riscv/htif/htif.h"
+#include "qemu/timer.h"
+#include "exec/address-spaces.h"
+#include "qemu/error-report.h"
+#include <fcntl.h>
+#include <unistd.h>
+#include <sys/stat.h>
+
+static void htif_pre_save(void *opaque)
+{
+    return;
+}
+
+static int htif_post_load(void *opaque, int version_id)
+{
+    return 0;
+}
+
+const VMStateDescription vmstate_htif = {
+    .name = "htif",
+    .version_id = 1,
+    .minimum_version_id = 1,
+    .pre_save = htif_pre_save,
+    .post_load = htif_post_load,
+    .fields      = (VMStateField []) { // TODO what
+        VMSTATE_UINT64(tohost, HTIFState),
+        VMSTATE_UINT64(fromhost, HTIFState),
+        VMSTATE_UINT64(tohost_addr, HTIFState),
+        VMSTATE_UINT64(fromhost_addr, HTIFState),
+        VMSTATE_END_OF_LIST()
+    },
+};
+
+static void dma_strcopy(HTIFState *htifstate, char *str, hwaddr phys_addr) {
+    int i = 0;
+    void* base_copy_addr = htifstate->main_mem_ram_ptr+phys_addr;
+    while(*(str+i)) {
+        stb_p((void*)(base_copy_addr + i), *(str + i));
+        i++;
+    }
+    stb_p((void*)(base_copy_addr + i), 0); // store null term
+}
+
+static int htif_block_device_read(HTIFState *htifstate, uint64_t payload) {
+    request_t req;
+    int i;
+    uint8_t* reqptr = (uint8_t*)&req;
+    void *base = htifstate->main_mem_ram_ptr+payload;
+    for (i = 0; i < sizeof(req); i++) {
+        // TODO: potential endianness issues here
+        *(reqptr + i) = ldub_p((void*)(base + i));
+    }
+
+    uint8_t copybuf[req.size];
+    if (pread(htifstate->block_fd, copybuf, req.size, req.offset) != req.size) {
+        printf("FAILED READ\n");
+        exit(1);
+    }
+
+    base = htifstate->main_mem_ram_ptr + req.addr;
+    for (i = 0; i < req.size; i++) {
+        stb_p((void*)(base + i), copybuf[i]);
+    }
+    return req.tag;
+}
+
+static int htif_block_device_write(HTIFState *htifstate, uint64_t payload) {
+    request_t req;
+    int i;
+    uint8_t* reqptr = (uint8_t*)&req;
+    void* base = htifstate->main_mem_ram_ptr + payload;
+    for (i = 0; i < sizeof(req); i++) {
+        // TODO: potential endianness issues here
+        *(reqptr + i) = ldub_p((void*)(base + i));
+    }
+
+    uint8_t copybuf[req.size];
+
+    base = htifstate->main_mem_ram_ptr + req.addr;
+    for (i = 0; i < req.size; i++) {
+        copybuf[i] = ldub_p((void*)(base + i));
+    }
+
+    if (pwrite(htifstate->block_fd, copybuf, req.size, req.offset) != req.size) {
+        printf("FAILED WRITE\n");
+        exit(1);
+    }
+    return req.tag;
+}
+
+static void htif_handle_tohost_write(HTIFState *htifstate, uint64_t val_written) {
+
+    uint8_t device = val_written >> 56;
+    uint8_t cmd = val_written >> 48;
+    uint64_t payload = val_written & 0xFFFFFFFFFFFFULL;
+
+    uint64_t addr = payload >> 8;
+    hwaddr real_addr = (hwaddr)addr;
+    uint8_t what = payload & 0xFF;
+    int resp;
+
+    resp = 0; // stop gcc complaining
+
+    if (likely(device == 0x1 && htifstate->block_dev_present)) { 
+        // assume device 0x1 is permanently hooked to block dev for now
+        if (unlikely(cmd == 0xFF)) { 
+            if (what == 0xFF) { // register
+                dma_strcopy(htifstate, htifstate->real_name, real_addr);
+            } else if (what == 0x0) {
+                dma_strcopy(htifstate, (char*)"read", real_addr);
+            } else if (what == 0x1) {
+                dma_strcopy(htifstate, (char*)"write", real_addr);
+            } else {
+                dma_strcopy(htifstate, (char*)"", real_addr);
+            }
+            resp = 0x1; // write to indicate device name placed
+        } else if (cmd == 0x0) {
+            // handle disk read
+            resp = htif_block_device_read(htifstate, payload);
+        } else if (cmd == 0x1) {
+            // handle disk write
+            resp = htif_block_device_write(htifstate, payload);
+        } else {
+            printf("INVALID HTIFBD COMMAND. exiting\n");
+            exit(1);
+        }
+    } else if (cmd == 0xFF && what == 0xFF) { // all other devices
+        stb_p((void*)(htifstate->main_mem_ram_ptr+real_addr), 0);
+        resp = 0x1; // write to indicate device name placed
+    }
+    htifstate->fromhost = (val_written >> 48 << 48) | (resp << 16 >> 16);
+    htifstate->tohost = 0; // clear to indicate we read
+}
+
+// CPU wants to read an HTIF register
+static uint64_t htif_mm_read(void *opaque, hwaddr addr, unsigned size)
+{
+    HTIFState *htifstate = opaque;
+    if (addr == 0x0) {
+        return htifstate->tohost & 0xFFFFFFFF;
+    } else if (addr == 0x4) {
+        return (htifstate->tohost >> 32) & 0xFFFFFFFF;
+    } else if (addr == 0x8) {
+        return htifstate->fromhost & 0xFFFFFFFF;
+    } else if (addr == 0xc) {
+        return (htifstate->fromhost >> 32) & 0xFFFFFFFF;
+    } else {
+        printf("Invalid htif register address %016lx\n", (uint64_t)addr);
+        exit(1);
+    }
+}
+
+// CPU wrote to an HTIF register
+static void htif_mm_write(void *opaque, hwaddr addr,
+                            uint64_t value, unsigned size)
+{
+    HTIFState *htifstate = opaque;
+    if (addr == 0x0) {
+        htifstate->tohost = value & 0xFFFFFFFF;
+    } else if (addr == 0x4) {
+        htif_handle_tohost_write(htifstate, htifstate->tohost | (value << 32));
+    } else if (addr == 0x8) {
+        htifstate->fromhost = value & 0xFFFFFFFF;
+    } else if (addr == 0xc) {
+        htifstate->fromhost |= value << 32;
+    } else {
+        printf("Invalid htif register address %016lx\n", (uint64_t)addr);
+        exit(1);
+    }
+}
+
+static const MemoryRegionOps htif_mm_ops[3] = {
+    [DEVICE_LITTLE_ENDIAN] = {
+        .read = htif_mm_read,
+        .write = htif_mm_write,
+        .endianness = DEVICE_LITTLE_ENDIAN,
+    },
+};
+
+HTIFState *htif_mm_init(MemoryRegion *address_space, hwaddr base, qemu_irq irq, 
+                        MemoryRegion *main_mem, char* htifbd_fname)
+{
+    // TODO: cleanup the constant buffer sizes
+    HTIFState *htifstate;
+    size_t size;
+    char *rname;
+    char size_str_buf[400];
+
+    htifstate = g_malloc0(sizeof(HTIFState));
+    rname = g_malloc0(sizeof(char)*500);
+    htifstate->tohost = 0;
+    htifstate->fromhost = 0;
+    htifstate->tohost_addr = base;
+    htifstate->fromhost_addr = base + 0x8;
+    htifstate->irq = irq;
+    htifstate->address_space = address_space;
+    htifstate->main_mem = main_mem;
+    htifstate->main_mem_ram_ptr = memory_region_get_ram_ptr(main_mem);
+
+    vmstate_register(NULL, base, &vmstate_htif, htifstate);
+
+    memory_region_init_io(&htifstate->io, NULL, &htif_mm_ops[DEVICE_LITTLE_ENDIAN], 
+            htifstate, "htif", 16 /* 2 64-bit registers */);
+    memory_region_add_subregion(address_space, base, &htifstate->io);
+
+    if (NULL == htifbd_fname) { // NULL means no -hda specified
+        htifstate->block_dev_present = 0;
+        return htifstate;
+    }
+
+    htifstate->block_fname = htifbd_fname;
+    htifstate->block_fd = open(htifstate->block_fname, O_RDWR);
+
+    struct stat st;
+    if (fstat(htifstate->block_fd, &st) < 0) {
+        printf("WARN: Could not stat %s, continuing without block device.\n", 
+                htifstate->block_fname);
+        htifstate->block_dev_present = 0;
+        return htifstate;
+    }
+    size = st.st_size;
+    strcpy(rname, "disk size=");
+    snprintf(size_str_buf, sizeof(size_str_buf), "%zu", size);
+    strcat(rname, size_str_buf);
+    htifstate->real_name = rname;
+    htifstate->block_dev_present = 1;
+    return htifstate;
+}
diff --git a/hw/riscv/riscv_board.c b/hw/riscv/riscv_board.c
new file mode 100644
index 0000000..2484cda
--- /dev/null
+++ b/hw/riscv/riscv_board.c
@@ -0,0 +1,238 @@
+/* 
+ *  QEMU RISC-V sample-board support
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Originally based on hw/mips/mips_malta.c
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include "hw/hw.h"
+#include "hw/i386/pc.h"
+#include "hw/char/serial.h"
+#include "hw/riscv/htif/htif.h"
+#include "hw/block/fdc.h"
+#include "net/net.h"
+#include "hw/boards.h"
+#include "hw/i2c/smbus.h"
+#include "block/block.h"
+#include "hw/block/flash.h"
+#include "block/block_int.h" // move later
+#include "hw/riscv/riscv.h"
+#include "hw/riscv/cpudevs.h"
+#include "hw/pci/pci.h"
+#include "sysemu/char.h"
+#include "sysemu/sysemu.h"
+#include "sysemu/arch_init.h"
+#include "qemu/log.h"
+#include "hw/riscv/bios.h"
+#include "hw/ide.h"
+#include "hw/loader.h"
+#include "elf.h"
+#include "hw/timer/mc146818rtc.h"
+#include "hw/timer/i8254.h"
+#include "sysemu/blockdev.h"
+#include "exec/address-spaces.h"
+#include "hw/sysbus.h"             /* SysBusDevice */
+#include "qemu/host-utils.h"
+#include "sysemu/qtest.h"
+#include "qemu/error-report.h"
+#include "hw/empty_slot.h"
+
+#define TYPE_RISCV_BOARD "riscv-board"
+#define RISCV_BOARD(obj) OBJECT_CHECK(BoardState, (obj), TYPE_RISCV_BOARD)
+
+typedef struct {
+    SysBusDevice parent_obj;
+} BoardState;
+
+static struct _loaderparams {
+    int ram_size;
+    const char *kernel_filename;
+    const char *kernel_cmdline;
+    const char *initrd_filename;
+} loaderparams;
+
+uint64_t identity_translate(void *opaque, uint64_t addr)
+{
+    return addr;
+}
+
+static int64_t load_kernel (void)
+{
+    int64_t kernel_entry, kernel_high;
+    int big_endian;
+    big_endian = 0;
+
+    if (load_elf(loaderparams.kernel_filename, identity_translate, NULL,
+                 (uint64_t *)&kernel_entry, NULL, (uint64_t *)&kernel_high,
+                 big_endian, ELF_MACHINE, 1) < 0) {
+        fprintf(stderr, "qemu: could not load kernel '%s'\n",
+                loaderparams.kernel_filename);
+        exit(1);
+    }
+    return kernel_entry;
+}
+
+static void main_cpu_reset(void *opaque)
+{
+    RISCVCPU *cpu = opaque;
+    cpu_reset(CPU(cpu));
+}
+
+static void riscv_board_init(MachineState *machine)
+{
+    ram_addr_t ram_size = machine->ram_size;
+    const char *cpu_model = machine->cpu_model;
+    const char *kernel_filename = machine->kernel_filename;
+    const char *kernel_cmdline = machine->kernel_cmdline;
+    const char *initrd_filename = machine->initrd_filename;
+    MemoryRegion *system_memory = get_system_memory();
+    MemoryRegion *main_mem = g_new(MemoryRegion, 1);
+    RISCVCPU *cpu;
+    CPURISCVState *env;
+    int i;
+#ifdef CONFIG_RISCV_HTIF
+    DriveInfo *htifbd_drive;
+    char *htifbd_fname; // htif block device filename
+#endif
+
+    DeviceState *dev = qdev_create(NULL, TYPE_RISCV_BOARD);
+
+    object_property_set_bool(OBJECT(dev), true, "realized", NULL);
+
+    /* Make sure the first 3 serial ports are associated with a device. */
+    for(i = 0; i < 3; i++) {
+        if (!serial_hds[i]) {
+            char label[32];
+            snprintf(label, sizeof(label), "serial%d", i);
+            serial_hds[i] = qemu_chr_new(label, "null", NULL);
+        }
+    }
+
+    /* init CPUs */
+    if (cpu_model == NULL) {
+        cpu_model = "riscv-generic";
+    }
+
+    for (i = 0; i < smp_cpus; i++) {
+        cpu = cpu_riscv_init(cpu_model);
+        if (cpu == NULL) {
+            fprintf(stderr, "Unable to find CPU definition\n");
+            exit(1);
+        }
+        env = &cpu->env;
+
+        /* Init internal devices */
+        cpu_riscv_irq_init_cpu(env);
+        cpu_riscv_clock_init(env);
+        qemu_register_reset(main_cpu_reset, cpu);
+    }
+    cpu = RISCV_CPU(first_cpu);
+    env = &cpu->env;
+
+    /* register system main memory (actual RAM) */
+    memory_region_init_ram(main_mem, NULL, "riscv_board.ram", ram_size);
+    vmstate_register_ram_global(main_mem);
+    memory_region_add_subregion(system_memory, 0x0, main_mem);
+
+    if (kernel_filename) {
+        /* Write a small bootloader to the flash location. */
+        loaderparams.ram_size = ram_size;
+        loaderparams.kernel_filename = kernel_filename;
+        loaderparams.kernel_cmdline = kernel_cmdline;
+        loaderparams.initrd_filename = initrd_filename;
+        load_kernel();
+    }
+
+    // write memory amount in MiB to 0x0
+    stl_p(memory_region_get_ram_ptr(main_mem), loaderparams.ram_size >> 20);
+
+#ifdef CONFIG_RISCV_HTIF
+    serial_mm_init(system_memory, 0x3f8, 0, env->irq[4], 1843200/16, serial_hds[0],
+        DEVICE_NATIVE_ENDIAN);
+
+    // setup HTIF Block Device if one is specified as -hda FILENAME
+    htifbd_drive = drive_get_by_index(IF_IDE, 0);
+    if (NULL == htifbd_drive) {
+        htifbd_fname = NULL;
+    } else {
+        htifbd_fname = (*(htifbd_drive->bdrv)).filename;
+    }
+
+    // add htif device 0x400 - 0x410
+    htif_mm_init(system_memory, 0x400, env->irq[0], main_mem, htifbd_fname);
+#else
+    // add serial device 0x3f8-0x3ff
+    serial_mm_init(system_memory, 0x3f8, 0, env->irq[1], 1843200/16, serial_hds[0],
+        DEVICE_NATIVE_ENDIAN);
+
+    /* Create MMIO transports, to which virtio backends created by the
+     * user are automatically connected as needed.  If no backend is
+     * present, the transport simply remains harmlessly idle.
+     * Each memory-mapped region is 0x200 bytes in size.
+     */
+    sysbus_create_simple("virtio-mmio", 0x400, env->irq[2]);
+    sysbus_create_simple("virtio-mmio", 0x600, env->irq[3]);
+    sysbus_create_simple("virtio-mmio", 0x800, env->irq[4]);
+#endif
+
+    /* Init internal devices */
+    cpu_riscv_irq_init_cpu(env);
+    cpu_riscv_clock_init(env);
+}
+
+static int riscv_board_sysbus_device_init(SysBusDevice *sysbusdev)
+{
+    return 0;
+}
+
+static void riscv_board_class_init(ObjectClass *klass, void *data)
+{
+    SysBusDeviceClass *k = SYS_BUS_DEVICE_CLASS(klass);
+    k->init = riscv_board_sysbus_device_init;
+}
+
+static const TypeInfo riscv_board_device = {
+    .name          = TYPE_RISCV_BOARD,
+    .parent        = TYPE_SYS_BUS_DEVICE,
+    .instance_size = sizeof(BoardState),
+    .class_init    = riscv_board_class_init,
+};
+
+static QEMUMachine riscv_board_machine = {
+    .name = "board",
+    .desc = "RISCV Board",
+    .init = riscv_board_init,
+    .max_cpus = 1,
+    .is_default = 1,
+};
+
+static void riscv_board_register_types(void)
+{
+    type_register_static(&riscv_board_device);
+}
+
+static void riscv_board_machine_init(void)
+{
+    qemu_register_machine(&riscv_board_machine);
+}
+
+type_init(riscv_board_register_types)
+machine_init(riscv_board_machine_init);
diff --git a/hw/riscv/riscv_int.c b/hw/riscv/riscv_int.c
new file mode 100644
index 0000000..5b19a1e
--- /dev/null
+++ b/hw/riscv/riscv_int.c
@@ -0,0 +1,87 @@
+/*
+ *  QEMU RISC-V interrupt support
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target interrupt support
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+
+#include "hw/hw.h"
+#include "hw/riscv/cpudevs.h"
+#include "cpu.h"
+
+// TODO: move constants to cpu.h
+
+/* irq request function, called in hw/irq.h by qemu_irq_raise (level = 1), 
+ * qemu_irq_lower (level = 0), qemu_irq_pulse (level = 1, then 0) 
+ *
+ * The device will call this once to raise the interrupt line and once to 
+ * lower the interrupt line for level-trigerring
+ *
+ */
+static void cpu_riscv_irq_request(void *opaque, int irq, int level)
+{
+    RISCVCPU *cpu = opaque;
+    CPURISCVState *env = &cpu->env;
+    CPUState *cs = CPU(cpu);
+
+    if (unlikely(irq < 0 || irq > 7)) {
+        return;
+    }
+
+    if (level) {
+        // level high, set the interrupt in CSR_STATUS
+        env->helper_csr[CSR_STATUS] |= (1 << (irq + 24));
+    } else {
+        // level low, turn off the interrupt in CSR_STATUS
+        env->helper_csr[CSR_STATUS] &= ~(1 << (irq + 24));
+    }
+
+    if (env->helper_csr[CSR_STATUS] & (0xFF << (24))) {
+        // call cpu_interrupt from include/qom/cpu.h
+        // this will call cpu_interrupt_handler aka
+        // tcg_handle_interrupt from translate-all.c
+        cpu_interrupt(cs, CPU_INTERRUPT_HARD);
+    } else {
+        // call cpu_reset_interrupt from qom/cpu.c
+        // this just turns off the relevant bits
+        // in cpu->interrupt_request
+        cpu_reset_interrupt(cs, CPU_INTERRUPT_HARD);
+    }
+}
+
+void cpu_riscv_irq_init_cpu(CPURISCVState *env)
+{
+    qemu_irq *qi;
+    int i;
+
+    qi = qemu_allocate_irqs(cpu_riscv_irq_request, riscv_env_get_cpu(env), 8);
+    for (i = 0; i < 8; i++) {
+        env->irq[i] = qi[i];
+    }
+}
+
+void cpu_riscv_soft_irq(CPURISCVState *env, int irq, int level)
+{
+    if (irq != 0) {
+        return;
+    }
+    qemu_set_irq(env->irq[irq], level);
+}
diff --git a/include/elf.h b/include/elf.h
index e88d52f..d852ff1 100644
--- a/include/elf.h
+++ b/include/elf.h
@@ -110,6 +110,8 @@ typedef int64_t  Elf64_Sxword;
 
 #define EM_UNICORE32    110     /* UniCore32 */
 
+#define EM_RISCV	243	/* RISC-V */
+
 /*
  * This is an interim value that we will use until the committee comes
  * up with a final number.
diff --git a/include/exec/poison.h b/include/exec/poison.h
index a4b1eca..7012c51 100644
--- a/include/exec/poison.h
+++ b/include/exec/poison.h
@@ -19,6 +19,7 @@
 #pragma GCC poison TARGET_PPCEMB
 #pragma GCC poison TARGET_PPC64
 #pragma GCC poison TARGET_ABI32
+#pragma GCC poison TARGET_RISCV
 #pragma GCC poison TARGET_SH4
 #pragma GCC poison TARGET_SPARC
 #pragma GCC poison TARGET_SPARC64
diff --git a/include/exec/user/thunk.h b/include/exec/user/thunk.h
index 87025c3..13bdf6a 100644
--- a/include/exec/user/thunk.h
+++ b/include/exec/user/thunk.h
@@ -121,7 +121,7 @@ static inline int thunk_type_size(const argtype *type_ptr, int is_host)
 #if defined(TARGET_X86_64)
             return 8;
 #elif defined(TARGET_ALPHA) || defined(TARGET_IA64) || defined(TARGET_MIPS) || \
-      defined(TARGET_PARISC) || defined(TARGET_SPARC64)
+      defined(TARGET_PARISC) || defined(TARGET_SPARC64) || defined(TARGET_RISCV)
             return 4;
 #elif defined(TARGET_PPC)
             return TARGET_ABI_BITS / 8;
diff --git a/include/hw/riscv/bios.h b/include/hw/riscv/bios.h
new file mode 100644
index 0000000..4e64818
--- /dev/null
+++ b/include/hw/riscv/bios.h
@@ -0,0 +1,4 @@
+#include "cpu.h"
+
+#define BIOS_SIZE (4 * 1024 * 1024)
+#define BIOS_FILENAME "riscv_bios.bin"
diff --git a/include/hw/riscv/cpudevs.h b/include/hw/riscv/cpudevs.h
new file mode 100644
index 0000000..54cee6d
--- /dev/null
+++ b/include/hw/riscv/cpudevs.h
@@ -0,0 +1,14 @@
+#ifndef HW_RISCV_CPUDEVS_H
+#define HW_RISCV_CPUDEVS_H
+/* Definitions for RISCV CPU internal devices.  */
+
+/* riscv_board.c */
+uint64_t identity_translate(void *opaque, uint64_t addr);
+
+/* riscv_int.c */
+void cpu_riscv_irq_init_cpu(CPURISCVState *env);
+
+/* cputimer.c */
+void cpu_riscv_clock_init(CPURISCVState *);
+
+#endif
diff --git a/include/hw/riscv/htif/htif.h b/include/hw/riscv/htif/htif.h
new file mode 100644
index 0000000..2fe03f5
--- /dev/null
+++ b/include/hw/riscv/htif/htif.h
@@ -0,0 +1,68 @@
+/*
+ * QEMU RISCV Host Target Interface (HTIF) Emulation
+ *
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ */
+#ifndef HW_RISCV_HTIF_H
+#define HW_RISCV_HTIF_H 1
+
+#include "hw/hw.h"
+#include "sysemu/sysemu.h"
+#include "exec/memory.h"
+
+typedef struct HTIFState HTIFState;
+
+struct HTIFState {
+    uint64_t tohost; // mapped to address base passed into htif_mm_init
+    uint64_t fromhost; // mapped to address base + 0x8 passed into htif_mm_init
+    hwaddr tohost_addr;
+    hwaddr fromhost_addr;
+    qemu_irq irq; // host interrupt line
+    MemoryRegion io;
+    MemoryRegion* address_space;
+    MemoryRegion* main_mem;
+    void* main_mem_ram_ptr;
+
+    int block_dev_present;
+    // TODO: eventually move the following to a separate HTIF block device driver
+    const char *block_fname;
+    int block_fd;
+    char *real_name;
+
+};
+
+typedef struct request_t request_t;
+
+struct request_t
+{ 
+    uint64_t addr;
+    uint64_t offset;
+    uint64_t size;
+    uint64_t tag;
+};
+
+extern const VMStateDescription vmstate_htif;
+extern const MemoryRegionOps htif_io_ops;
+
+/* legacy pre qom */
+HTIFState *htif_mm_init(MemoryRegion *address_space, hwaddr base, 
+                    qemu_irq irq, MemoryRegion *main_mem, char *htifbd_fname);
+
+#endif
diff --git a/include/hw/riscv/riscv.h b/include/hw/riscv/riscv.h
new file mode 100644
index 0000000..31adb20
--- /dev/null
+++ b/include/hw/riscv/riscv.h
@@ -0,0 +1,10 @@
+#ifndef HW_RISCV_H
+#define HW_RISCV_H
+/* Definitions for riscv board emulation.  */
+
+/* Kernels can be configured with 64KB pages */
+#define INITRD_PAGE_MASK (~((1 << 16) - 1))
+
+#include "exec/memory.h"
+
+#endif
diff --git a/include/sysemu/arch_init.h b/include/sysemu/arch_init.h
index 182d48d..5b04b2c 100644
--- a/include/sysemu/arch_init.h
+++ b/include/sysemu/arch_init.h
@@ -22,6 +22,7 @@ enum {
     QEMU_ARCH_OPENRISC = 8192,
     QEMU_ARCH_UNICORE32 = 0x4000,
     QEMU_ARCH_MOXIE = 0x8000,
+    QEMU_ARCH_RISCV = 0x10000,
 };
 
 extern const uint32_t arch_type;
diff --git a/monitor.c b/monitor.c
index 5bc70a6..179cc80 100644
--- a/monitor.c
+++ b/monitor.c
@@ -2663,7 +2663,8 @@ static mon_cmd_t info_cmds[] = {
         .mhandler.cmd = do_info_history,
     },
 #if defined(TARGET_I386) || defined(TARGET_PPC) || defined(TARGET_MIPS) || \
-    defined(TARGET_LM32) || (defined(TARGET_SPARC) && !defined(TARGET_SPARC64))
+    defined(TARGET_LM32) || defined(TARGET_RISCV) || \
+    (defined(TARGET_SPARC) && !defined(TARGET_SPARC64))
     {
         .name       = "irq",
         .args_type  = "",
diff --git a/target-riscv/Makefile.objs b/target-riscv/Makefile.objs
new file mode 100644
index 0000000..b983221
--- /dev/null
+++ b/target-riscv/Makefile.objs
@@ -0,0 +1,4 @@
+obj-y += fpu-custom-riscv/f32_add.o fpu-custom-riscv/f32_classify.o fpu-custom-riscv/f32_div.o fpu-custom-riscv/f32_eq.o fpu-custom-riscv/f32_eq_signaling.o fpu-custom-riscv/f32_isSignalingNaN.o fpu-custom-riscv/f32_le.o fpu-custom-riscv/f32_le_quiet.o fpu-custom-riscv/f32_lt.o fpu-custom-riscv/f32_lt_quiet.o fpu-custom-riscv/f32_mulAdd.o fpu-custom-riscv/f32_mul.o fpu-custom-riscv/f32_rem.o fpu-custom-riscv/f32_roundToInt.o fpu-custom-riscv/f32_sqrt.o fpu-custom-riscv/f32_sub.o fpu-custom-riscv/f32_to_f64.o fpu-custom-riscv/f32_to_i32.o fpu-custom-riscv/f32_to_i32_r_minMag.o fpu-custom-riscv/f32_to_i64.o fpu-custom-riscv/f32_to_i64_r_minMag.o fpu-custom-riscv/f32_to_ui32.o fpu-custom-riscv/f32_to_ui32_r_minMag.o fpu-custom-riscv/f32_to_ui64.o fpu-custom-riscv/f32_to_ui64_r_minMag.o fpu-custom-riscv/f64_add.o fpu-custom-riscv/f64_classify.o fpu-custom-riscv/f64_div.o fpu-custom-riscv/f64_eq.o fpu-custom-riscv/f64_eq_signaling.o fpu-custom-riscv/f64_isSignalingNaN.o fpu-custom-riscv/f64_le.o fpu-custom-riscv/f64_le_quiet.o fpu-custom-riscv/f64_lt.o fpu-custom-riscv/f64_lt_quiet.o fpu-custom-riscv/f64_mulAdd.o fpu-custom-riscv/f64_mul.o fpu-custom-riscv/f64_rem.o fpu-custom-riscv/f64_roundToInt.o fpu-custom-riscv/f64_sqrt.o fpu-custom-riscv/f64_sub.o fpu-custom-riscv/f64_to_f32.o fpu-custom-riscv/f64_to_i32.o fpu-custom-riscv/f64_to_i32_r_minMag.o fpu-custom-riscv/f64_to_i64.o fpu-custom-riscv/f64_to_i64_r_minMag.o fpu-custom-riscv/f64_to_ui32.o fpu-custom-riscv/f64_to_ui32_r_minMag.o fpu-custom-riscv/f64_to_ui64.o fpu-custom-riscv/f64_to_ui64_r_minMag.o fpu-custom-riscv/i32_to_f32.o fpu-custom-riscv/i32_to_f64.o fpu-custom-riscv/i64_to_f32.o fpu-custom-riscv/i64_to_f64.o fpu-custom-riscv/s_add128.o fpu-custom-riscv/s_add192.o fpu-custom-riscv/s_addMagsF32.o fpu-custom-riscv/s_addMagsF64.o fpu-custom-riscv/s_commonNaNToF32UI.o fpu-custom-riscv/s_commonNaNToF64UI.o fpu-custom-riscv/s_countLeadingZeros32.o fpu-custom-riscv/s_countLeadingZeros64.o fpu-custom-riscv/s_countLeadingZeros8.o fpu-custom-riscv/s_eq128.o fpu-custom-riscv/s_estimateDiv128To64.o fpu-custom-riscv/s_estimateSqrt32.o fpu-custom-riscv/s_f32UIToCommonNaN.o fpu-custom-riscv/s_f64UIToCommonNaN.o fpu-custom-riscv/s_isSigNaNF32UI.o fpu-custom-riscv/s_isSigNaNF64UI.o fpu-custom-riscv/s_le128.o fpu-custom-riscv/s_lt128.o fpu-custom-riscv/s_mul128By64To192.o fpu-custom-riscv/s_mul128To256.o fpu-custom-riscv/s_mul64To128.o fpu-custom-riscv/s_mulAddF32.o fpu-custom-riscv/s_mulAddF64.o fpu-custom-riscv/s_normRoundPackToF32.o fpu-custom-riscv/s_normRoundPackToF64.o fpu-custom-riscv/s_normSubnormalF32Sig.o fpu-custom-riscv/s_normSubnormalF64Sig.o fpu-custom-riscv/softfloat_raiseFlags.o fpu-custom-riscv/softfloat_state.o fpu-custom-riscv/s_propagateNaNF32UI.o fpu-custom-riscv/s_propagateNaNF64UI.o fpu-custom-riscv/s_roundPackToF32.o fpu-custom-riscv/s_roundPackToF64.o fpu-custom-riscv/s_roundPackToI32.o fpu-custom-riscv/s_roundPackToI64.o fpu-custom-riscv/s_roundPackToUI32.o fpu-custom-riscv/s_roundPackToUI64.o fpu-custom-riscv/s_shift128ExtraRightJam.o fpu-custom-riscv/s_shift128RightJam.o fpu-custom-riscv/s_shift32RightJam.o fpu-custom-riscv/s_shift64ExtraRightJam.o fpu-custom-riscv/s_shift64RightJam.o fpu-custom-riscv/s_shortShift128ExtraRightJam.o fpu-custom-riscv/s_shortShift128Left.o fpu-custom-riscv/s_shortShift128Right.o fpu-custom-riscv/s_shortShift192Left.o fpu-custom-riscv/s_shortShift32Right1Jam.o fpu-custom-riscv/s_shortShift64ExtraRightJam.o fpu-custom-riscv/s_shortShift64RightJam.o fpu-custom-riscv/s_sub128.o fpu-custom-riscv/s_sub192.o fpu-custom-riscv/s_subMagsF32.o fpu-custom-riscv/s_subMagsF64.o fpu-custom-riscv/ui32_to_f32.o fpu-custom-riscv/ui32_to_f64.o fpu-custom-riscv/ui64_to_f32.o fpu-custom-riscv/ui64_to_f64.o 
+obj-y += translate.o op_helper.o helper.o cpu.o
+obj-y += gdbstub.o
+obj-$(CONFIG_SOFTMMU) += machine.o
diff --git a/target-riscv/TODO b/target-riscv/TODO
new file mode 100644
index 0000000..bdb920f
--- /dev/null
+++ b/target-riscv/TODO
@@ -0,0 +1,27 @@
+Notes for RISCV, adapted from the MIPS TODO
+-----------------------------------------------
+
+General
+-------
+- The TLB emulation is very inefficient:
+  QEMU's softmmu implements a x86-style MMU, with separate entries
+  for read/write/execute, a TLB index which is just a modulo of the
+  virtual address, and a set of TLBs for each user/kernel/supervisor
+  MMU mode.
+  MIPS has a single entry for read/write/execute and only one MMU mode.
+  But it is fully associative with randomized entry indices, and uses
+  up to 256 ASID tags as additional matching criterion (which roughly
+  equates to 256 MMU modes). It also has a global flag which causes
+  entries to match regardless of ASID.
+  To cope with these differences, QEMU currently flushes the TLB at
+  each ASID change. Using the MMU modes to implement ASIDs hinges on
+  implementing the global bit efficiently.
+- save/restore of the CPU state is not implemented (see machine.c). -- WHERE IS THIS USED?
+
+MALTA system emulation (simulated RISCV board is based off of hw/mips_malta.c)
+----------------------
+- We fake firmware support instead of doing the real thing
+- Real firmware (YAMON) falls over when trying to init RAM, presumably
+  due to lacking system controller emulation.
+- Bonito system controller not implemented
+- MSC1 system controller not implemented
diff --git a/target-riscv/cpu-qom.h b/target-riscv/cpu-qom.h
new file mode 100644
index 0000000..268361e
--- /dev/null
+++ b/target-riscv/cpu-qom.h
@@ -0,0 +1,80 @@
+/*
+ * QEMU RISC-V CPU
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see
+ * <http://www.gnu.org/licenses/lgpl-2.1.html>
+ */
+#ifndef QEMU_RISCV_CPU_QOM_H
+#define QEMU_RISCV_CPU_QOM_H
+
+#include "qom/cpu.h"
+
+#define TYPE_RISCV_CPU "riscv-cpu"
+
+#define RISCV_CPU_CLASS(klass) \
+    OBJECT_CLASS_CHECK(RISCVCPUClass, (klass), TYPE_RISCV_CPU)
+#define RISCV_CPU(obj) \
+    OBJECT_CHECK(RISCVCPU, (obj), TYPE_RISCV_CPU)
+#define RISCV_CPU_GET_CLASS(obj) \
+    OBJECT_GET_CLASS(RISCVCPUClass, (obj), TYPE_RISCV_CPU)
+
+/**
+ * RISCVCPUClass:
+ * @parent_realize: The parent class' realize handler.
+ * @parent_reset: The parent class' reset handler.
+ *
+ * A RISCV CPU model.
+ */
+typedef struct RISCVCPUClass {
+    /*< private >*/
+    CPUClass parent_class;
+    /*< public >*/
+
+    DeviceRealize parent_realize;
+    void (*parent_reset)(CPUState *cpu);
+} RISCVCPUClass;
+
+/**
+ * RISCVCPU:
+ * @env: #CPURISCVState
+ *
+ * A RISCV CPU.
+ */
+typedef struct RISCVCPU {
+    /*< private >*/
+    CPUState parent_obj;
+    /*< public >*/
+
+    CPURISCVState env;
+} RISCVCPU;
+
+static inline RISCVCPU *riscv_env_get_cpu(CPURISCVState *env)
+{
+    return container_of(env, RISCVCPU, env);
+}
+
+#define ENV_GET_CPU(e) CPU(riscv_env_get_cpu(e))
+
+#define ENV_OFFSET offsetof(RISCVCPU, env)
+
+void riscv_cpu_do_interrupt(CPUState *cpu);
+void riscv_cpu_dump_state(CPUState *cpu, FILE *f, fprintf_function cpu_fprintf,
+                         int flags);
+hwaddr riscv_cpu_get_phys_page_debug(CPUState *cpu, vaddr addr);
+int riscv_cpu_gdb_read_register(CPUState *cpu, uint8_t *buf, int reg);
+int riscv_cpu_gdb_write_register(CPUState *cpu, uint8_t *buf, int reg);
+void riscv_cpu_do_unaligned_access(CPUState *cpu, vaddr addr,
+                                   int is_write, int is_user, uintptr_t retaddr);
+
+#endif
diff --git a/target-riscv/cpu.c b/target-riscv/cpu.c
new file mode 100644
index 0000000..3862344
--- /dev/null
+++ b/target-riscv/cpu.c
@@ -0,0 +1,137 @@
+/*
+ *  QEMU RISC-V CPU
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see
+ * <http://www.gnu.org/licenses/lgpl-2.1.html>
+ */
+
+#include "cpu.h"
+#include "qemu-common.h"
+
+static void riscv_cpu_set_pc(CPUState *cs, vaddr value)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+    env->active_tc.PC = value;
+}
+
+static void riscv_cpu_synchronize_from_tb(CPUState *cs, TranslationBlock *tb)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+    env->active_tc.PC = tb->pc;
+}
+
+static bool riscv_cpu_has_work(CPUState *cs)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+    bool has_work = false;
+
+    /* It is implementation dependent if non-enabled interrupts
+       wake-up the CPU, however most of the implementations only
+       check for interrupts that can be taken. */
+    if ((cs->interrupt_request & CPU_INTERRUPT_HARD) &&
+        cpu_riscv_hw_interrupts_pending(env)) {
+        has_work = true;
+    }
+
+    return has_work;
+}
+
+static void riscv_cpu_reset(CPUState *s)
+{
+    RISCVCPU *cpu = RISCV_CPU(s);
+    RISCVCPUClass *mcc = RISCV_CPU_GET_CLASS(cpu);
+    CPURISCVState *env = &cpu->env;
+
+    mcc->parent_reset(s);
+    tlb_flush(s, 1);
+    cpu_state_reset(env);
+}
+
+static void riscv_cpu_realizefn(DeviceState *dev, Error **errp)
+{
+    CPUState *cs = CPU(dev);
+    RISCVCPUClass *mcc = RISCV_CPU_GET_CLASS(dev);
+
+    cpu_reset(cs);
+    qemu_init_vcpu(cs);
+
+    mcc->parent_realize(dev, errp);
+}
+
+static void riscv_cpu_initfn(Object *obj)
+{
+    CPUState *cs = CPU(obj);
+    RISCVCPU *cpu = RISCV_CPU(obj);
+    CPURISCVState *env = &cpu->env;
+
+    cs->env_ptr = env;
+    cpu_exec_init(env);
+
+    if (tcg_enabled()) {
+        riscv_tcg_init();
+    }
+}
+
+static void riscv_cpu_class_init(ObjectClass *c, void *data)
+{
+    RISCVCPUClass *mcc = RISCV_CPU_CLASS(c);
+    CPUClass *cc = CPU_CLASS(c);
+    DeviceClass *dc = DEVICE_CLASS(c);
+
+    mcc->parent_realize = dc->realize;
+    dc->realize = riscv_cpu_realizefn;
+
+    mcc->parent_reset = cc->reset;
+    cc->reset = riscv_cpu_reset;
+
+    cc->has_work = riscv_cpu_has_work;
+    cc->do_interrupt = riscv_cpu_do_interrupt;
+    cc->dump_state = riscv_cpu_dump_state;
+    cc->set_pc = riscv_cpu_set_pc;
+    cc->synchronize_from_tb = riscv_cpu_synchronize_from_tb;
+    cc->gdb_read_register = riscv_cpu_gdb_read_register;
+    cc->gdb_write_register = riscv_cpu_gdb_write_register;
+#ifdef CONFIG_USER_ONLY
+    cc->handle_mmu_fault = riscv_cpu_handle_mmu_fault;
+#else
+    cc->do_unassigned_access = riscv_cpu_unassigned_access;
+    cc->do_unaligned_access = riscv_cpu_do_unaligned_access;
+    cc->get_phys_page_debug = riscv_cpu_get_phys_page_debug;
+#endif
+
+    cc->gdb_num_core_regs = 73;
+}
+
+static const TypeInfo riscv_cpu_type_info = {
+    .name = TYPE_RISCV_CPU,
+    .parent = TYPE_CPU,
+    .instance_size = sizeof(RISCVCPU),
+    .instance_init = riscv_cpu_initfn,
+    .abstract = false,
+    .class_size = sizeof(RISCVCPUClass),
+    .class_init = riscv_cpu_class_init,
+};
+
+static void riscv_cpu_register_types(void)
+{
+    type_register_static(&riscv_cpu_type_info);
+}
+
+type_init(riscv_cpu_register_types)
diff --git a/target-riscv/cpu.h b/target-riscv/cpu.h
new file mode 100644
index 0000000..b6b0dcf
--- /dev/null
+++ b/target-riscv/cpu.h
@@ -0,0 +1,230 @@
+#if !defined (__RISCV_CPU_H__)
+#define __RISCV_CPU_H__
+
+//#define DEBUG_OP
+
+#define TARGET_HAS_ICE 1
+
+#define ELF_MACHINE	EM_RISCV
+
+#define CPUArchState struct CPURISCVState
+
+#define RISCV_START_PC 0x2000
+
+#include "config.h"
+#include "qemu-common.h"
+#include "riscv-defs.h"
+#include "exec/cpu-defs.h"
+
+#define NB_MMU_MODES 2
+
+struct CPURISCVState;
+
+// RISCV CSR mappings. These are not the "real" mappings defined by the isa.
+// Instead, they are the indices into our csr array (ie the output given when
+// calling translate.c:csr_regno(REAL_CSR_REGNO)).
+#define CSR_SUP0       0x0
+#define CSR_SUP1       0x1
+#define CSR_EPC        0x2
+#define CSR_BADVADDR   0x3
+#define CSR_PTBR       0x4
+#define CSR_ASID       0x5
+#define CSR_COUNT      0x6
+#define CSR_COMPARE    0x7
+#define CSR_EVEC       0x8
+#define CSR_CAUSE      0x9
+#define CSR_STATUS     0xa
+#define CSR_HARTID     0xb
+#define CSR_IMPL       0xc
+#define CSR_FATC       0xd
+#define CSR_SEND_IPI   0xe
+#define CSR_CLEAR_IPI  0xf
+#define CSR_CYCLE     0x10
+#define CSR_TIME      0x11
+#define CSR_INSTRET   0x12
+#define CSR_FFLAGS    0x13
+#define CSR_FRM       0x14
+#define CSR_FCSR      0x15
+//...
+#define CSR_TOHOST    0x1e
+#define CSR_FROMHOST  0x1f
+
+// RISCV Exception Codes
+#define EXCP_NONE                       -1   // not a real RISCV exception code
+#define RISCV_EXCP_INST_ADDR_MIS        0x0
+#define RISCV_EXCP_INST_ACCESS_FAULT    0x1
+#define RISCV_EXCP_ILLEGAL_INST         0x2
+#define RISCV_EXCP_PRIV_INST            0x3
+#define RISCV_EXCP_FP_DISABLED          0x4
+#define RISCV_EXCP_SCALL                0x6
+#define RISCV_EXCP_BREAK                0x7
+#define RISCV_EXCP_LOAD_ADDR_MIS        0x8
+#define RISCV_EXCP_STORE_ADDR_MIS       0x9
+#define RISCV_EXCP_LOAD_ACCESS_FAULT    0xa
+#define RISCV_EXCP_STORE_ACCESS_FAULT   0xb
+#define RISCV_EXCP_STORE_ACCEL_DISABLED 0xc
+#define RISCV_EXCP_TIMER_INTERRUPT      (0x7 | (1 << 31)) 
+#define RISCV_EXCP_HOST_INTERRUPT       (0x6 | (1 << 31)) 
+#define RISCV_EXCP_SERIAL_INTERRUPT     (0x4 | (1 << 31)) // not part of ISA
+
+// RISCV Status Reg Bits
+#define SR_S           0x1
+#define SR_PS          0x2
+#define SR_EI          0x4
+#define SR_PEI         0x8
+#define SR_EF         0x10
+#define SR_U64        0x20
+#define SR_S64        0x40
+#define SR_VM         0x80
+#define SR_EA        0x100
+#define SR_IM     0xFF0000
+#define SR_IP   0xFF000000
+
+// RISCV pte bits
+#define PTE_V    0x1
+#define PTE_T    0x2
+#define PTE_G    0x4
+#define PTE_UR   0x8
+#define PTE_UW  0x10
+#define PTE_UX  0x20
+#define PTE_SR  0x40
+#define PTE_SW  0x80
+#define PTE_SX 0x100
+
+typedef struct riscv_def_t riscv_def_t;
+
+typedef struct TCState TCState;
+struct TCState {
+    target_ulong gpr[32];
+    target_ulong fpr[32];
+    target_ulong PC;
+};
+
+typedef struct CPURISCVState CPURISCVState;
+struct CPURISCVState {
+    TCState active_tc;
+    uint32_t current_tc;
+    uint32_t SEGBITS;
+    uint32_t PABITS;
+    target_ulong SEGMask;
+    target_ulong PAMask;
+
+    uint64_t helper_csr[32]; // RISCV CSR registers
+
+    /* QEMU */
+    CPU_COMMON
+
+    /* Fields from here on are preserved across CPU reset. */
+    const riscv_def_t *cpu_model;
+    void *irq[8];
+    QEMUTimer *timer; /* Internal timer */
+};
+
+#include "cpu-qom.h"
+
+#if !defined(CONFIG_USER_ONLY)
+void riscv_cpu_unassigned_access(CPUState *cpu, hwaddr addr,
+                                bool is_write, bool is_exec, int unused,
+                                unsigned size);
+#endif
+
+void riscv_cpu_list (FILE *f, fprintf_function cpu_fprintf);
+
+#define cpu_exec cpu_riscv_exec
+#define cpu_gen_code cpu_riscv_gen_code
+#define cpu_signal_handler cpu_riscv_signal_handler
+#define cpu_list riscv_cpu_list
+
+extern void cpu_wrdsp(uint32_t rs, uint32_t mask_num, CPURISCVState *env);
+extern uint32_t cpu_rddsp(uint32_t mask_num, CPURISCVState *env);
+
+#define CPU_SAVE_VERSION 3
+
+static inline int cpu_mmu_index (CPURISCVState *env)
+{
+    return env->helper_csr[CSR_STATUS] & SR_S;
+}
+
+static inline int cpu_riscv_hw_interrupts_pending(CPURISCVState *env)
+{
+    int32_t pending;
+    int32_t status;
+    int r;
+
+    /* first check if interrupts are disabled */
+    if (!((env->helper_csr[CSR_STATUS] >> 2) & 0x1)) {
+        // interrupts disabled
+        return 0;
+    }
+
+    pending = (env->helper_csr[CSR_STATUS] >> 24) & 0xFF;
+    status = (env->helper_csr[CSR_STATUS] >> 16) & 0xFF;
+
+    r = pending & status;
+    return r;
+}
+
+#include "exec/cpu-all.h"
+
+/* Memory access type :
+ * may be needed for precise access rights control and precise exceptions.
+ */
+enum {
+    /* 1 bit to define user level / supervisor access */
+    ACCESS_USER  = 0x00,
+    ACCESS_SUPER = 0x01,
+    /* 1 bit to indicate direction */
+    ACCESS_STORE = 0x02,
+    /* Type of instruction that generated the access */
+    ACCESS_CODE  = 0x10, /* Code fetch access                */
+    ACCESS_INT   = 0x20, /* Integer load/store access        */
+    ACCESS_FLOAT = 0x30, /* floating point load/store access */
+};
+
+int cpu_riscv_exec(CPURISCVState *s);
+void riscv_tcg_init(void);
+RISCVCPU *cpu_riscv_init(const char *cpu_model);
+int cpu_riscv_signal_handler(int host_signum, void *pinfo, void *puc);
+
+static inline CPURISCVState *cpu_init(const char *cpu_model)
+{
+    RISCVCPU *cpu = cpu_riscv_init(cpu_model);
+    if (cpu == NULL) {
+        return NULL;
+    }
+    return &cpu->env;
+}
+
+/* TODO QOM'ify CPU reset and remove */
+void cpu_state_reset(CPURISCVState *s);
+
+/* hw/riscv/cputimer.c */
+uint64_t cpu_riscv_get_cycle (CPURISCVState *env);
+uint32_t cpu_riscv_get_random (CPURISCVState *env);
+uint32_t cpu_riscv_get_count (CPURISCVState *env);
+void cpu_riscv_store_count (CPURISCVState *env, uint32_t value);
+void cpu_riscv_store_compare (CPURISCVState *env, uint32_t value);
+void cpu_riscv_start_count(CPURISCVState *env);
+
+/* hw/riscv/riscv_int.c */
+void cpu_riscv_soft_irq(CPURISCVState *env, int irq, int level);
+
+/* helper.c */
+int riscv_cpu_handle_mmu_fault(CPUState *cpu, vaddr address, int rw,
+                              int mmu_idx);
+#if !defined(CONFIG_USER_ONLY)
+hwaddr cpu_riscv_translate_address (CPURISCVState *env, target_ulong address,
+		                               int rw);
+#endif
+
+static inline void cpu_get_tb_cpu_state(CPURISCVState *env, target_ulong *pc,
+                                        target_ulong *cs_base, int *flags)
+{
+    *pc = env->active_tc.PC;
+    *cs_base = 0;
+    *flags = 0; // necessary to avoid compiler warning
+}
+
+#include "exec/exec-all.h"
+
+#endif /* !defined (__RISCV_CPU_H__) */
diff --git a/target-riscv/fpu-custom-riscv/8086/OLD-specialize.c b/target-riscv/fpu-custom-riscv/8086/OLD-specialize.c
new file mode 100644
index 0000000..ffb306d
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/OLD-specialize.c
@@ -0,0 +1,40 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+/*----------------------------------------------------------------------------
+| Underflow tininess-detection mode, statically initialized to default value.
+| (The declaration in `softfloat.h' must match the `int8' type here.)
+*----------------------------------------------------------------------------*/
+bool float_detectTininess = float_tininess_afterRounding;
+
diff --git a/target-riscv/fpu-custom-riscv/8086/OLD-specialize.h b/target-riscv/fpu-custom-riscv/8086/OLD-specialize.h
new file mode 100644
index 0000000..9e4461c
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/OLD-specialize.h
@@ -0,0 +1,379 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+/*----------------------------------------------------------------------------
+| Internal canonical NaN format.
+*----------------------------------------------------------------------------*/
+*** COMMON
+typedef struct {
+    flag sign;
+    uint128_t bits;
+} commonNaNT;
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated single-precision NaN.
+*----------------------------------------------------------------------------*/
+#define float32Bits_defaultNaN 0xFFC00000
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the single-precision floating-point value `a' is a NaN;
+| otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+*** COMMON
+#define softfloat_isNaNFloat32Bits( a ) ( 0xFF000000 < (uint32_t) ( a )<<1 )
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the single-precision floating-point value `a' is a signaling
+| NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+inline bool softfloat_isSigNaNFloat32Bits( uint32_t a )
+    { return ( ( a>>22 & 0x1FF ) == 0x1FE ) && ( a & 0x003FFFFF ); }
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+commonNaNT softfloat_NaNFromFloat32Bits( uint32_t );
+uint32_t softfloat_float32BitsFromNaN( commonNaNT );
+uint32_t softfloat_propNaNFloat32Bits( uint32_t, uint32_t );
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated double-precision NaN.
+*----------------------------------------------------------------------------*/
+#define float64Bits_defaultNaN 0xFFF8000000000000
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the double-precision floating-point value `a' is a NaN;
+| otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+*** COMMON
+#define softfloat_isNaNFloat64Bits( a ) ( 0xFFE0000000000000 < (uint64_t) ( a )<<1 )
+
+
+
+
+
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the double-precision floating-point value `a' is a signaling
+| NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+
+flag float64_is_signaling_nan( float64 a )
+{
+
+    return
+           ( ( ( a>>51 ) & 0xFFF ) == 0xFFE )
+        && ( a & LIT64( 0x0007FFFFFFFFFFFF ) );
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the double-precision floating-point NaN
+| `a' to the canonical NaN format.  If `a' is a signaling NaN, the invalid
+| exception is raised.
+*----------------------------------------------------------------------------*/
+
+static commonNaNT float64ToCommonNaN( float64 a )
+{
+    commonNaNT z;
+
+    if ( float64_is_signaling_nan( a ) ) float_raise( float_flag_invalid );
+    z.sign = a>>63;
+    z.low = 0;
+    z.high = a<<12;
+    return z;
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the double-
+| precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+static float64 commonNaNToFloat64( commonNaNT a )
+{
+
+    return
+          ( ( (bits64) a.sign )<<63 )
+        | LIT64( 0x7FF8000000000000 )
+        | ( a.high>>12 );
+
+}
+
+/*----------------------------------------------------------------------------
+| Takes two double-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+static float64 propagateFloat64NaN( float64 a, float64 b )
+{
+    flag aIsNaN, aIsSignalingNaN, bIsNaN, bIsSignalingNaN;
+
+    aIsNaN = float64_is_nan( a );
+    aIsSignalingNaN = float64_is_signaling_nan( a );
+    bIsNaN = float64_is_nan( b );
+    bIsSignalingNaN = float64_is_signaling_nan( b );
+    a |= LIT64( 0x0008000000000000 );
+    b |= LIT64( 0x0008000000000000 );
+    if ( aIsSignalingNaN | bIsSignalingNaN ) float_raise( float_flag_invalid );
+    if ( aIsSignalingNaN ) {
+        if ( bIsSignalingNaN ) goto returnLargerSignificand;
+        return bIsNaN ? b : a;
+    }
+    else if ( aIsNaN ) {
+        if ( bIsSignalingNaN | ! bIsNaN ) return a;
+ returnLargerSignificand:
+        if ( (bits64) ( a<<1 ) < (bits64) ( b<<1 ) ) return b;
+        if ( (bits64) ( b<<1 ) < (bits64) ( a<<1 ) ) return a;
+        return ( a < b ) ? a : b;
+    }
+    else {
+        return b;
+    }
+
+}
+
+#ifdef FLOATX80
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated extended double-precision NaN.  The
+| `high' and `low' values hold the most- and least-significant bits,
+| respectively.
+*----------------------------------------------------------------------------*/
+#define floatx80_default_nan_high 0xFFFF
+#define floatx80_default_nan_low  LIT64( 0xC000000000000000 )
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the extended double-precision floating-point value `a' is a
+| NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+
+flag floatx80_is_nan( floatx80 a )
+{
+
+    return ( ( a.high & 0x7FFF ) == 0x7FFF ) && (bits64) ( a.low<<1 );
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the extended double-precision floating-point value `a' is a
+| signaling NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+
+flag floatx80_is_signaling_nan( floatx80 a )
+{
+    bits64 aLow;
+
+    aLow = a.low & ~ LIT64( 0x4000000000000000 );
+    return
+           ( ( a.high & 0x7FFF ) == 0x7FFF )
+        && (bits64) ( aLow<<1 )
+        && ( a.low == aLow );
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the extended double-precision floating-
+| point NaN `a' to the canonical NaN format.  If `a' is a signaling NaN, the
+| invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+static commonNaNT floatx80ToCommonNaN( floatx80 a )
+{
+    commonNaNT z;
+
+    if ( floatx80_is_signaling_nan( a ) ) float_raise( float_flag_invalid );
+    z.sign = a.high>>15;
+    z.low = 0;
+    z.high = a.low<<1;
+    return z;
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the extended
+| double-precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+static floatx80 commonNaNToFloatx80( commonNaNT a )
+{
+    floatx80 z;
+
+    z.low = LIT64( 0xC000000000000000 ) | ( a.high>>1 );
+    z.high = ( ( (bits16) a.sign )<<15 ) | 0x7FFF;
+    return z;
+
+}
+
+/*----------------------------------------------------------------------------
+| Takes two extended double-precision floating-point values `a' and `b', one
+| of which is a NaN, and returns the appropriate NaN result.  If either `a' or
+| `b' is a signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+static floatx80 propagateFloatx80NaN( floatx80 a, floatx80 b )
+{
+    flag aIsNaN, aIsSignalingNaN, bIsNaN, bIsSignalingNaN;
+
+    aIsNaN = floatx80_is_nan( a );
+    aIsSignalingNaN = floatx80_is_signaling_nan( a );
+    bIsNaN = floatx80_is_nan( b );
+    bIsSignalingNaN = floatx80_is_signaling_nan( b );
+    a.low |= LIT64( 0xC000000000000000 );
+    b.low |= LIT64( 0xC000000000000000 );
+    if ( aIsSignalingNaN | bIsSignalingNaN ) float_raise( float_flag_invalid );
+    if ( aIsSignalingNaN ) {
+        if ( bIsSignalingNaN ) goto returnLargerSignificand;
+        return bIsNaN ? b : a;
+    }
+    else if ( aIsNaN ) {
+        if ( bIsSignalingNaN | ! bIsNaN ) return a;
+ returnLargerSignificand:
+        if ( a.low < b.low ) return b;
+        if ( b.low < a.low ) return a;
+        return ( a.high < b.high ) ? a : b;
+    }
+    else {
+        return b;
+    }
+
+}
+
+#endif
+
+#ifdef FLOAT128
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated quadruple-precision NaN.  The `high' and
+| `low' values hold the most- and least-significant bits, respectively.
+*----------------------------------------------------------------------------*/
+#define float128_default_nan_high LIT64( 0xFFFF800000000000 )
+#define float128_default_nan_low  LIT64( 0x0000000000000000 )
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the quadruple-precision floating-point value `a' is a NaN;
+| otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+
+flag float128_is_nan( float128 a )
+{
+
+    return
+           ( LIT64( 0xFFFE000000000000 ) <= (bits64) ( a.high<<1 ) )
+        && ( a.low || ( a.high & LIT64( 0x0000FFFFFFFFFFFF ) ) );
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the quadruple-precision floating-point value `a' is a
+| signaling NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+
+flag float128_is_signaling_nan( float128 a )
+{
+
+    return
+           ( ( ( a.high>>47 ) & 0xFFFF ) == 0xFFFE )
+        && ( a.low || ( a.high & LIT64( 0x00007FFFFFFFFFFF ) ) );
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the quadruple-precision floating-point NaN
+| `a' to the canonical NaN format.  If `a' is a signaling NaN, the invalid
+| exception is raised.
+*----------------------------------------------------------------------------*/
+
+static commonNaNT float128ToCommonNaN( float128 a )
+{
+    commonNaNT z;
+
+    if ( float128_is_signaling_nan( a ) ) float_raise( float_flag_invalid );
+    z.sign = a.high>>63;
+    shortShift128Left( a.high, a.low, 16, &z.high, &z.low );
+    return z;
+
+}
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the quadruple-
+| precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+static float128 commonNaNToFloat128( commonNaNT a )
+{
+    float128 z;
+
+    shift128Right( a.high, a.low, 16, &z.high, &z.low );
+    z.high |= ( ( (bits64) a.sign )<<63 ) | LIT64( 0x7FFF800000000000 );
+    return z;
+
+}
+
+/*----------------------------------------------------------------------------
+| Takes two quadruple-precision floating-point values `a' and `b', one of
+| which is a NaN, and returns the appropriate NaN result.  If either `a' or
+| `b' is a signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+static float128 propagateFloat128NaN( float128 a, float128 b )
+{
+    flag aIsNaN, aIsSignalingNaN, bIsNaN, bIsSignalingNaN;
+
+    aIsNaN = float128_is_nan( a );
+    aIsSignalingNaN = float128_is_signaling_nan( a );
+    bIsNaN = float128_is_nan( b );
+    bIsSignalingNaN = float128_is_signaling_nan( b );
+    a.high |= LIT64( 0x0000800000000000 );
+    b.high |= LIT64( 0x0000800000000000 );
+    if ( aIsSignalingNaN | bIsSignalingNaN ) float_raise( float_flag_invalid );
+    if ( aIsSignalingNaN ) {
+        if ( bIsSignalingNaN ) goto returnLargerSignificand;
+        return bIsNaN ? b : a;
+    }
+    else if ( aIsNaN ) {
+        if ( bIsSignalingNaN | ! bIsNaN ) return a;
+ returnLargerSignificand:
+        if ( lt128( a.high<<1, a.low, b.high<<1, b.low ) ) return b;
+        if ( lt128( b.high<<1, b.low, a.high<<1, a.low ) ) return a;
+        return ( a.high < b.high ) ? a : b;
+    }
+    else {
+        return b;
+    }
+
+}
+
+#endif
+
diff --git a/target-riscv/fpu-custom-riscv/8086/platform.h b/target-riscv/fpu-custom-riscv/8086/platform.h
new file mode 100644
index 0000000..9355edf
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/platform.h
@@ -0,0 +1,38 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+#define LITTLEENDIAN
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_commonNaNToF32UI.c b/target-riscv/fpu-custom-riscv/8086/s_commonNaNToF32UI.c
new file mode 100644
index 0000000..3b96c41
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_commonNaNToF32UI.c
@@ -0,0 +1,17 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the single-
+| precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+uint_fast32_t softfloat_commonNaNToF32UI( struct commonNaN a )
+{
+
+    return (uint_fast32_t) a.sign<<31 | 0x7FC00000 | a.v64>>41;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_commonNaNToF64UI.c b/target-riscv/fpu-custom-riscv/8086/s_commonNaNToF64UI.c
new file mode 100644
index 0000000..474ceee
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_commonNaNToF64UI.c
@@ -0,0 +1,19 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the double-
+| precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+uint_fast64_t softfloat_commonNaNToF64UI( struct commonNaN a )
+{
+
+    return
+        (uint_fast64_t) a.sign<<63 | UINT64_C( 0x7FF8000000000000 )
+            | a.v64>>12;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_f32UIToCommonNaN.c b/target-riscv/fpu-custom-riscv/8086/s_f32UIToCommonNaN.c
new file mode 100644
index 0000000..067e8da
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_f32UIToCommonNaN.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the single-precision floating-point NaN
+| `a' to the canonical NaN format.  If `a' is a signaling NaN, the invalid
+| exception is raised.
+*----------------------------------------------------------------------------*/
+struct commonNaN softfloat_f32UIToCommonNaN( uint_fast32_t uiA )
+{
+    struct commonNaN z;
+
+    if ( softfloat_isSigNaNF32UI( uiA ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    z.sign = uiA>>31;
+    z.v64 = (uint_fast64_t) uiA<<41;
+    z.v0 = 0;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_f64UIToCommonNaN.c b/target-riscv/fpu-custom-riscv/8086/s_f64UIToCommonNaN.c
new file mode 100644
index 0000000..f933ded
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_f64UIToCommonNaN.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the double-precision floating-point NaN
+| `a' to the canonical NaN format.  If `a' is a signaling NaN, the invalid
+| exception is raised.
+*----------------------------------------------------------------------------*/
+struct commonNaN softfloat_f64UIToCommonNaN( uint_fast64_t uiA )
+{
+    struct commonNaN z;
+
+    if ( softfloat_isSigNaNF64UI( uiA ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    z.sign = uiA>>63;
+    z.v64 = uiA<<12;
+    z.v0 = 0;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_isSigNaNF32UI.c b/target-riscv/fpu-custom-riscv/8086/s_isSigNaNF32UI.c
new file mode 100644
index 0000000..0a9c33f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_isSigNaNF32UI.c
@@ -0,0 +1,13 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+bool softfloat_isSigNaNF32UI( uint_fast32_t ui )
+{
+
+    return ( ( ui>>22 & 0x1FF ) == 0x1FE ) && ( ui & 0x003FFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_isSigNaNF64UI.c b/target-riscv/fpu-custom-riscv/8086/s_isSigNaNF64UI.c
new file mode 100644
index 0000000..d255213
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_isSigNaNF64UI.c
@@ -0,0 +1,15 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+bool softfloat_isSigNaNF64UI( uint_fast64_t ui )
+{
+
+    return
+        ( ( ui>>51 & 0xFFF ) == 0xFFE )
+            && ( ui & UINT64_C( 0x0007FFFFFFFFFFFF ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_propagateNaNF32UI.c b/target-riscv/fpu-custom-riscv/8086/s_propagateNaNF32UI.c
new file mode 100644
index 0000000..07774e8
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_propagateNaNF32UI.c
@@ -0,0 +1,55 @@
+
+/*** UPDATE COMMENTS. ***/
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Takes two single-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+uint_fast32_t
+ softfloat_propagateNaNF32UI( uint_fast32_t uiA, uint_fast32_t uiB )
+{
+    bool isNaNA, isSigNaNA, isNaNB, isSigNaNB;
+    uint_fast32_t uiMagA, uiMagB;
+
+    /*------------------------------------------------------------------------
+    *------------------------------------------------------------------------*/
+    isNaNA = isNaNF32UI( uiA );
+    isSigNaNA = softfloat_isSigNaNF32UI( uiA );
+    isNaNB = isNaNF32UI( uiB );
+    isSigNaNB = softfloat_isSigNaNF32UI( uiB );
+    /*------------------------------------------------------------------------
+    | Make NaNs non-signaling.
+    *------------------------------------------------------------------------*/
+    uiA |= 0x00400000;
+    uiB |= 0x00400000;
+    /*------------------------------------------------------------------------
+    *------------------------------------------------------------------------*/
+    if ( isSigNaNA | isSigNaNB ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    if ( isSigNaNA ) {
+        if ( isSigNaNB ) goto returnLargerSignificand;
+        return isNaNB ? uiB : uiA;
+    } else if ( isNaNA ) {
+        if ( isSigNaNB || ! isNaNB ) return uiA;
+ returnLargerSignificand:
+        uiMagA = uiA<<1;
+        uiMagB = uiB<<1;
+        if ( uiMagA < uiMagB ) return uiB;
+        if ( uiMagB < uiMagA ) return uiA;
+        return ( uiA < uiB ) ? uiA : uiB;
+    } else {
+        return uiB;
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/s_propagateNaNF64UI.c b/target-riscv/fpu-custom-riscv/8086/s_propagateNaNF64UI.c
new file mode 100644
index 0000000..0ff6446
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/s_propagateNaNF64UI.c
@@ -0,0 +1,55 @@
+
+/*** UPDATE COMMENTS. ***/
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Takes two double-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+uint_fast64_t
+ softfloat_propagateNaNF64UI( uint_fast64_t uiA, uint_fast64_t uiB )
+{
+    bool isNaNA, isSigNaNA, isNaNB, isSigNaNB;
+    uint_fast64_t uiMagA, uiMagB;
+
+    /*------------------------------------------------------------------------
+    *------------------------------------------------------------------------*/
+    isNaNA = isNaNF64UI( uiA );
+    isSigNaNA = softfloat_isSigNaNF64UI( uiA );
+    isNaNB = isNaNF64UI( uiB );
+    isSigNaNB = softfloat_isSigNaNF64UI( uiB );
+    /*------------------------------------------------------------------------
+    | Make NaNs non-signaling.
+    *------------------------------------------------------------------------*/
+    uiA |= UINT64_C( 0x0008000000000000 );
+    uiB |= UINT64_C( 0x0008000000000000 );
+    /*------------------------------------------------------------------------
+    *------------------------------------------------------------------------*/
+    if ( isSigNaNA | isSigNaNB ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    if ( isSigNaNA ) {
+        if ( isSigNaNB ) goto returnLargerSignificand;
+        return isNaNB ? uiB : uiA;
+    } else if ( isNaNA ) {
+        if ( isSigNaNB || ! isNaNB ) return uiA;
+ returnLargerSignificand:
+        uiMagA = uiA & UINT64_C( 0x7FFFFFFFFFFFFFFF );
+        uiMagB = uiB & UINT64_C( 0x7FFFFFFFFFFFFFFF );
+        if ( uiMagA < uiMagB ) return uiB;
+        if ( uiMagB < uiMagA ) return uiA;
+        return ( uiA < uiB ) ? uiA : uiB;
+    } else {
+        return uiB;
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/softfloat_raiseFlags.c b/target-riscv/fpu-custom-riscv/8086/softfloat_raiseFlags.c
new file mode 100644
index 0000000..c0c0dc8
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/softfloat_raiseFlags.c
@@ -0,0 +1,51 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+#include "platform.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Raises the exceptions specified by `flags'.  Floating-point traps can be
+| defined here if desired.  It is currently not possible for such a trap
+| to substitute a result value.  If traps are not implemented, this routine
+| should be simply `float_exception_flags |= flags;'.
+*----------------------------------------------------------------------------*/
+
+void softfloat_raiseFlags( int_fast8_t flags )
+{
+
+    softfloat_exceptionFlags |= flags;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/8086/softfloat_types.h b/target-riscv/fpu-custom-riscv/8086/softfloat_types.h
new file mode 100644
index 0000000..b5c1828
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/softfloat_types.h
@@ -0,0 +1,16 @@
+
+#ifndef softfloat_types_h
+#define softfloat_types_h
+
+/*** COMMENTS. ***/
+
+#include <stdbool.h>
+#include <stdint.h>
+
+typedef struct { uint32_t v; } float32_t;
+typedef struct { uint64_t v; } float64_t;
+typedef struct { uint64_t v; uint16_t x; } floatx80_t;
+typedef struct { uint64_t v[ 2 ]; } float128_t;
+
+#endif
+
diff --git a/target-riscv/fpu-custom-riscv/8086/specialize.h b/target-riscv/fpu-custom-riscv/8086/specialize.h
new file mode 100644
index 0000000..ca0bb1d
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/8086/specialize.h
@@ -0,0 +1,113 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+#include <stdbool.h>
+#include <stdint.h>
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+#define init_detectTininess softfloat_tininess_afterRounding;
+
+/*----------------------------------------------------------------------------
+| Structure used to transfer NaN representations from one format to another.
+*----------------------------------------------------------------------------*/
+struct commonNaN {
+    bool sign;
+    uint64_t v64, v0;
+};
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated single-precision NaN.
+*----------------------------------------------------------------------------*/
+#define defaultNaNF32UI 0xFFC00000
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the single-precision floating-point value `a' is a signaling
+| NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool softfloat_isSigNaNF32UI( uint_fast32_t ui )
+    { return ( ( ui>>22 & 0x1FF ) == 0x1FE ) && ( ui & 0x003FFFFF ); }
+#else
+bool softfloat_isSigNaNF32UI( uint_fast32_t );
+#endif
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+struct commonNaN softfloat_f32UIToCommonNaN( uint_fast32_t );
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE uint_fast32_t softfloat_commonNaNToF32UI( struct commonNaN a )
+    { return (uint_fast32_t) a.sign<<31 | 0x7FC00000 | a.v64>>41; }
+#else
+uint_fast32_t softfloat_commonNaNToF32UI( struct commonNaN );
+#endif
+
+/*----------------------------------------------------------------------------
+| Takes two single-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+uint_fast32_t softfloat_propagateNaNF32UI( uint_fast32_t, uint_fast32_t );
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated double-precision NaN.
+*----------------------------------------------------------------------------*/
+#define defaultNaNF64UI UINT64_C(0xFFF8000000000000)
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool softfloat_isSigNaNF64UI( uint_fast64_t ui )
+{
+    return
+        ( ( ui>>51 & 0xFFF ) == 0xFFE )
+            && ( ui & UINT64_C( 0x0007FFFFFFFFFFFF ) );
+}
+#else
+bool softfloat_isSigNaNF64UI( uint_fast64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+/*** MIGHT BE INLINE'D. ***/
+struct commonNaN softfloat_f64UIToCommonNaN( uint_fast64_t );
+uint_fast64_t softfloat_commonNaNToF64UI( struct commonNaN );
+
+/*----------------------------------------------------------------------------
+| Takes two double-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+uint_fast64_t softfloat_propagateNaNF64UI( uint_fast64_t, uint_fast64_t );
+
diff --git a/target-riscv/fpu-custom-riscv/f32_add.c b/target-riscv/fpu-custom-riscv/f32_add.c
new file mode 100644
index 0000000..dc53d68
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_add.c
@@ -0,0 +1,29 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t f32_add( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool signA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signB;
+    float32_t ( *magsRoutine )( uint_fast32_t, uint_fast32_t, bool );
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF32UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF32UI( uiB );
+    magsRoutine =
+        ( signA == signB ) ? softfloat_addMagsF32 : softfloat_subMagsF32;
+    return magsRoutine( uiA, uiB, signA );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_classify.c b/target-riscv/fpu-custom-riscv/f32_classify.c
new file mode 100644
index 0000000..d16aa25
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_classify.c
@@ -0,0 +1,33 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+uint_fast16_t f32_classify( float32_t a )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+
+    uA.f = a;
+    uiA = uA.ui;
+
+    uint_fast16_t infOrNaN = expF32UI( uiA ) == 0xFF;
+    uint_fast16_t subnormalOrZero = expF32UI( uiA ) == 0;
+    bool sign = signF32UI( uiA );
+
+    return
+        (  sign && infOrNaN && fracF32UI( uiA ) == 0 )          << 0 |
+        (  sign && !infOrNaN && !subnormalOrZero )              << 1 |
+        (  sign && subnormalOrZero && fracF32UI( uiA ) )        << 2 |
+        (  sign && subnormalOrZero && fracF32UI( uiA ) == 0 )   << 3 |
+        ( !sign && infOrNaN && fracF32UI( uiA ) == 0 )          << 7 |
+        ( !sign && !infOrNaN && !subnormalOrZero )              << 6 |
+        ( !sign && subnormalOrZero && fracF32UI( uiA ) )        << 5 |
+        ( !sign && subnormalOrZero && fracF32UI( uiA ) == 0 )   << 4 |
+        ( isNaNF32UI( uiA ) &&  softfloat_isSigNaNF32UI( uiA )) << 8 |
+        ( isNaNF32UI( uiA ) && !softfloat_isSigNaNF32UI( uiA )) << 9;
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_div.c b/target-riscv/fpu-custom-riscv/f32_div.c
new file mode 100644
index 0000000..958b140
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_div.c
@@ -0,0 +1,96 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t f32_div( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast32_t sigA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signB;
+    int_fast16_t expB;
+    uint_fast32_t sigB;
+    bool signZ;
+    struct exp16_sig32 normExpSig;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ;
+    uint_fast32_t uiZ;
+    union ui32_f32 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF32UI( uiA );
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF32UI( uiB );
+    expB = expF32UI( uiB );
+    sigB = fracF32UI( uiB );
+    signZ = signA ^ signB;
+    if ( expA == 0xFF ) {
+        if ( sigA ) goto propagateNaN;
+        if ( expB == 0xFF ) {
+            if ( sigB ) goto propagateNaN;
+            goto invalid;
+        }
+        goto infinity;
+    }
+    if ( expB == 0xFF ) {
+        if ( sigB ) goto propagateNaN;
+        goto zero;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) {
+            if ( ! ( expA | sigA ) ) goto invalid;
+            softfloat_raiseFlags( softfloat_flag_infinity );
+            goto infinity;
+        }
+        normExpSig = softfloat_normSubnormalF32Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) goto zero;
+        normExpSig = softfloat_normSubnormalF32Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    expZ = expA - expB + 0x7D;
+    sigA = ( sigA | 0x00800000 )<<7;
+    sigB = ( sigB | 0x00800000 )<<8;
+    if ( sigB <= ( sigA + sigA ) ) {
+        ++expZ;
+        sigA >>= 1;
+    }
+    sigZ = ( (uint_fast64_t) sigA<<32 ) / sigB;
+    if ( ! ( sigZ & 0x3F ) ) {
+        sigZ |= ( (uint_fast64_t) sigB * sigZ != (uint_fast64_t) sigA<<32 );
+    }
+    return softfloat_roundPackToF32( signZ, expZ, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF32UI( uiA, uiB );
+    goto uiZ;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    uiZ = defaultNaNF32UI;
+    goto uiZ;
+ infinity:
+    uiZ = packToF32UI( signZ, 0xFF, 0 );
+    goto uiZ;
+ zero:
+    uiZ = packToF32UI( signZ, 0, 0 );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_eq.c b/target-riscv/fpu-custom-riscv/f32_eq.c
new file mode 100644
index 0000000..8f2306b
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_eq.c
@@ -0,0 +1,34 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f32_eq( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF32UI( uiA ) == 0xFF ) && fracF32UI( uiA ) )
+        || ( ( expF32UI( uiB ) == 0xFF ) && fracF32UI( uiB ) )
+    ) {
+        if (
+            softfloat_isSigNaNF32UI( uiA ) || softfloat_isSigNaNF32UI( uiB )
+        ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+        }
+        return false;
+    }
+    return ( uiA == uiB ) || ! (uint32_t) ( ( uiA | uiB )<<1 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_eq_signaling.c b/target-riscv/fpu-custom-riscv/f32_eq_signaling.c
new file mode 100644
index 0000000..bfba48a
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_eq_signaling.c
@@ -0,0 +1,29 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+bool f32_eq_signaling( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF32UI( uiA ) == 0xFF ) && fracF32UI( uiA ) )
+        || ( ( expF32UI( uiB ) == 0xFF ) && fracF32UI( uiB ) )
+    ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return false;
+    }
+    return ( uiA == uiB ) || ! (uint32_t) ( ( uiA | uiB )<<1 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_isSignalingNaN.c b/target-riscv/fpu-custom-riscv/f32_isSignalingNaN.c
new file mode 100644
index 0000000..09aaa82
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_isSignalingNaN.c
@@ -0,0 +1,16 @@
+
+#include <stdbool.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f32_isSignalingNaN( float32_t a )
+{
+    union ui32_f32 uA;
+
+    uA.f = a;
+    return softfloat_isSigNaNF32UI( uA.ui );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_le.c b/target-riscv/fpu-custom-riscv/f32_le.c
new file mode 100644
index 0000000..5f47be5
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_le.c
@@ -0,0 +1,34 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+bool f32_le( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF32UI( uiA ) == 0xFF ) && fracF32UI( uiA ) )
+        || ( ( expF32UI( uiB ) == 0xFF ) && fracF32UI( uiB ) )
+    ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return false;
+    }
+    signA = signF32UI( uiA );
+    signB = signF32UI( uiB );
+    return
+        ( signA != signB ) ? signA || ! (uint32_t) ( ( uiA | uiB )<<1 )
+            : ( uiA == uiB ) || ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_le_quiet.c b/target-riscv/fpu-custom-riscv/f32_le_quiet.c
new file mode 100644
index 0000000..2b541da
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_le_quiet.c
@@ -0,0 +1,39 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f32_le_quiet( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF32UI( uiA ) == 0xFF ) && fracF32UI( uiA ) )
+        || ( ( expF32UI( uiB ) == 0xFF ) && fracF32UI( uiB ) )
+    ) {
+        if (
+            softfloat_isSigNaNF32UI( uiA ) || softfloat_isSigNaNF32UI( uiB )
+        ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+        }
+        return false;
+    }
+    signA = signF32UI( uiA );
+    signB = signF32UI( uiB );
+    return
+        ( signA != signB ) ? signA || ! (uint32_t) ( ( uiA | uiB )<<1 )
+            : ( uiA == uiB ) || ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_lt.c b/target-riscv/fpu-custom-riscv/f32_lt.c
new file mode 100644
index 0000000..753b28a
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_lt.c
@@ -0,0 +1,34 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+bool f32_lt( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF32UI( uiA ) == 0xFF ) && fracF32UI( uiA ) )
+        || ( ( expF32UI( uiB ) == 0xFF ) && fracF32UI( uiB ) )
+    ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return false;
+    }
+    signA = signF32UI( uiA );
+    signB = signF32UI( uiB );
+    return
+        ( signA != signB ) ? signA && ( (uint32_t) ( ( uiA | uiB )<<1 ) != 0 )
+            : ( uiA != uiB ) && ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_lt_quiet.c b/target-riscv/fpu-custom-riscv/f32_lt_quiet.c
new file mode 100644
index 0000000..ecd90bf
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_lt_quiet.c
@@ -0,0 +1,39 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f32_lt_quiet( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF32UI( uiA ) == 0xFF ) && fracF32UI( uiA ) )
+        || ( ( expF32UI( uiB ) == 0xFF ) && fracF32UI( uiB ) )
+    ) {
+        if (
+            softfloat_isSigNaNF32UI( uiA ) || softfloat_isSigNaNF32UI( uiB )
+        ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+        }
+        return false;
+    }
+    signA = signF32UI( uiA );
+    signB = signF32UI( uiB );
+    return
+        ( signA != signB ) ? signA && ( (uint32_t) ( ( uiA | uiB )<<1 ) != 0 )
+            : ( uiA != uiB ) && ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_mul.c b/target-riscv/fpu-custom-riscv/f32_mul.c
new file mode 100644
index 0000000..ea4f7e5
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_mul.c
@@ -0,0 +1,89 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t f32_mul( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast32_t sigA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signB;
+    int_fast16_t expB;
+    uint_fast32_t sigB;
+    bool signZ;
+    uint_fast32_t magBits;
+    struct exp16_sig32 normExpSig;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ, uiZ;
+    union ui32_f32 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF32UI( uiA );
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF32UI( uiB );
+    expB = expF32UI( uiB );
+    sigB = fracF32UI( uiB );
+    signZ = signA ^ signB;
+    if ( expA == 0xFF ) {
+        if ( sigA || ( ( expB == 0xFF ) && sigB ) ) goto propagateNaN;
+        magBits = expB | sigB;
+        goto infArg;
+    }
+    if ( expB == 0xFF ) {
+        if ( sigB ) goto propagateNaN;
+        magBits = expA | sigA;
+        goto infArg;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) goto zero;
+        normExpSig = softfloat_normSubnormalF32Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) goto zero;
+        normExpSig = softfloat_normSubnormalF32Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    expZ = expA + expB - 0x7F;
+    sigA = ( sigA | 0x00800000 )<<7;
+    sigB = ( sigB | 0x00800000 )<<8;
+    sigZ = softfloat_shortShift64RightJam( (uint_fast64_t) sigA * sigB, 32 );
+    if ( sigZ < 0x40000000 ) {
+        --expZ;
+        sigZ <<= 1;
+    }
+    return softfloat_roundPackToF32( signZ, expZ, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF32UI( uiA, uiB );
+    goto uiZ;
+ infArg:
+    if ( ! magBits ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        uiZ = defaultNaNF32UI;
+    } else {
+        uiZ = packToF32UI( signZ, 0xFF, 0 );
+    }
+    goto uiZ;
+ zero:
+    uiZ = packToF32UI( signZ, 0, 0 );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_mulAdd.c b/target-riscv/fpu-custom-riscv/f32_mulAdd.c
new file mode 100644
index 0000000..38ea030
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_mulAdd.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t f32_mulAdd( float32_t a, float32_t b, float32_t c )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    union ui32_f32 uC;
+    uint_fast32_t uiC;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    uC.f = c;
+    uiC = uC.ui;
+    return softfloat_mulAddF32( 0, uiA, uiB, uiC );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_rem.c b/target-riscv/fpu-custom-riscv/f32_rem.c
new file mode 100644
index 0000000..7172da8
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_rem.c
@@ -0,0 +1,124 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t f32_rem( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast32_t sigA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+//    bool signB;
+    int_fast16_t expB;
+    uint_fast32_t sigB;
+    struct exp16_sig32 normExpSig;
+    int_fast16_t expDiff;
+    uint_fast32_t q;
+    uint_fast64_t sigA64, sigB64, q64;
+    uint_fast32_t alternateSigA;
+    uint32_t sigMean;
+    bool signZ;
+    uint_fast32_t uiZ;
+    union ui32_f32 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF32UI( uiA );
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+//    signB = signF32UI( uiB );
+    expB = expF32UI( uiB );
+    sigB = fracF32UI( uiB );
+    if ( expA == 0xFF ) {
+        if ( sigA || ( ( expB == 0xFF ) && sigB ) ) goto propagateNaN;
+        goto invalid;
+    }
+    if ( expB == 0xFF ) {
+        if ( sigB ) goto propagateNaN;
+        return a;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) goto invalid;
+        normExpSig = softfloat_normSubnormalF32Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) return a;
+        normExpSig = softfloat_normSubnormalF32Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    expDiff = expA - expB;
+    sigA |= 0x00800000;
+    sigB |= 0x00800000;
+    if ( expDiff < 32 ) {
+        sigA <<= 8;
+        sigB <<= 8;
+        if ( expDiff < 0 ) {
+            if ( expDiff < -1 ) return a;
+            sigA >>= 1;
+        }
+        q = ( sigB <= sigA );
+        if ( q ) sigA -= sigB;
+        if ( 0 < expDiff ) {
+            q = ( (uint_fast64_t) sigA<<32 ) / sigB;
+            q >>= 32 - expDiff;
+            sigB >>= 2;
+            sigA = ( ( sigA>>1 )<<( expDiff - 1 ) ) - sigB * q;
+        } else {
+            sigA >>= 2;
+            sigB >>= 2;
+        }
+    } else {
+        if ( sigB <= sigA ) sigA -= sigB;
+        sigA64 = (uint_fast64_t) sigA<<40;
+        sigB64 = (uint_fast64_t) sigB<<40;
+        expDiff -= 64;
+        while ( 0 < expDiff ) {
+            q64 = softfloat_estimateDiv128To64( sigA64, 0, sigB64 );
+            q64 = ( 2 < q64 ) ? q64 - 2 : 0;
+            sigA64 = - ( ( sigB * q64 )<<38 );
+            expDiff -= 62;
+        }
+        expDiff += 64;
+        q64 = softfloat_estimateDiv128To64( sigA64, 0, sigB64 );
+        q64 = ( 2 < q64 ) ? q64 - 2 : 0;
+        q = q64>>( 64 - expDiff );
+        sigB <<= 6;
+        sigA = ( ( sigA64>>33 )<<( expDiff - 1 ) ) - sigB * q;
+    }
+    do {
+        alternateSigA = sigA;
+        ++q;
+        sigA -= sigB;
+    } while ( sigA < 0x80000000 );
+    sigMean = sigA + alternateSigA;
+    if ( ( 0x80000000 <= sigMean ) || ( ! sigMean && ( q & 1 ) ) ) {
+        sigA = alternateSigA;
+    }
+    signZ = ( 0x80000000 <= sigA );
+    if ( signZ ) sigA = - sigA;
+    return softfloat_normRoundPackToF32( signA ^ signZ, expB, sigA );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF32UI( uiA, uiB );
+    goto uiZ;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    uiZ = defaultNaNF32UI;
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_roundToInt.c b/target-riscv/fpu-custom-riscv/f32_roundToInt.c
new file mode 100644
index 0000000..f8f9114
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_roundToInt.c
@@ -0,0 +1,78 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t f32_roundToInt( float32_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    int_fast16_t expA;
+    uint_fast32_t uiZ;
+    bool signA;
+    uint_fast32_t lastBitMask, roundBitsMask;
+    union ui32_f32 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    expA = expF32UI( uiA );
+    if ( 0x96 <= expA ) {
+        if ( ( expA == 0xFF ) && fracF32UI( uiA ) ) {
+            uiZ = softfloat_propagateNaNF32UI( uiA, 0 );
+            goto uiZ;
+        }
+        return a;
+    }
+    if ( expA <= 0x7E ) {
+        if ( ! (uint32_t) ( uiA<<1 ) ) return a;
+        if ( exact ) softfloat_exceptionFlags |= softfloat_flag_inexact;
+        signA = signF32UI( uiA );
+        switch ( roundingMode ) {
+         case softfloat_round_nearest_even:
+            if ( ( expA == 0x7E ) && fracF32UI( uiA ) ) {
+                uiZ = packToF32UI( signA, 0x7F, 0 );
+                goto uiZ;
+            }
+            break;
+         case softfloat_round_min:
+            uiZ = signA ? 0xBF800000 : 0;
+            goto uiZ;
+         case softfloat_round_max:
+            uiZ = signA ? 0x80000000 : 0x3F800000;
+            goto uiZ;
+         case softfloat_round_nearest_maxMag:
+            if ( expA == 0x7E ) {
+                uiZ = packToF32UI( signA, 0x7F, 0 );
+                goto uiZ;
+            }
+            break;
+        }
+        uiZ = packToF32UI( signA, 0, 0 );
+        goto uiZ;
+    }
+    lastBitMask = (uint_fast32_t) 1<<( 0x96 - expA );
+    roundBitsMask = lastBitMask - 1;
+    uiZ = uiA;
+    if ( roundingMode == softfloat_round_nearest_maxMag ) {
+        uiZ += lastBitMask>>1;
+    } else if ( roundingMode == softfloat_round_nearest_even ) {
+        uiZ += lastBitMask>>1;
+        if ( ! ( uiZ & roundBitsMask ) ) uiZ &= ~ lastBitMask;
+    } else if ( roundingMode != softfloat_round_minMag ) {
+        if ( signF32UI( uiZ ) ^ ( roundingMode == softfloat_round_max ) ) {
+            uiZ += roundBitsMask;
+        }
+    }
+    uiZ &= ~ roundBitsMask;
+    if ( exact && ( uiZ != uiA ) ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_sqrt.c b/target-riscv/fpu-custom-riscv/f32_sqrt.c
new file mode 100644
index 0000000..82e0886
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_sqrt.c
@@ -0,0 +1,74 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t f32_sqrt( float32_t a )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast32_t sigA, uiZ;
+    struct exp16_sig32 normExpSig;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ;
+    uint_fast64_t term, rem;
+    union ui32_f32 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF32UI( uiA );
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    if ( expA == 0xFF ) {
+        if ( sigA ) {
+            uiZ = softfloat_propagateNaNF32UI( uiA, 0 );
+            goto uiZ;
+        }
+        if ( ! signA ) return a;
+        goto invalid;
+    }
+    if ( signA ) {
+        if ( ! ( expA | sigA ) ) return a;
+        goto invalid;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) return a;
+        normExpSig = softfloat_normSubnormalF32Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    expZ = ( ( expA - 0x7F )>>1 ) + 0x7E;
+    sigA = ( sigA | 0x00800000 )<<8;
+    sigZ = softfloat_estimateSqrt32( expA, sigA ) + 2;
+    if ( ( sigZ & 0x7F ) <= 5 ) {
+        if ( sigZ < 2 ) {
+            sigZ = 0x7FFFFFFF;
+            goto roundPack;
+        }
+        sigA >>= expA & 1;
+        term = (uint_fast64_t) sigZ * sigZ;
+        rem = ( (uint_fast64_t) sigA<<32 ) - term;
+        while ( UINT64_C( 0x8000000000000000 ) <= rem ) {
+            --sigZ;
+            rem += ( (uint_fast64_t) sigZ<<1 ) | 1;
+        }
+        sigZ |= ( rem != 0 );
+    }
+    sigZ = softfloat_shortShift32Right1Jam( sigZ );
+ roundPack:
+    return softfloat_roundPackToF32( 0, expZ, sigZ );
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    uiZ = defaultNaNF32UI;
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_sub.c b/target-riscv/fpu-custom-riscv/f32_sub.c
new file mode 100644
index 0000000..d6761ef
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_sub.c
@@ -0,0 +1,29 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t f32_sub( float32_t a, float32_t b )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool signA;
+    union ui32_f32 uB;
+    uint_fast32_t uiB;
+    bool signB;
+    float32_t ( *magsRoutine )( uint_fast32_t, uint_fast32_t, bool );
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF32UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF32UI( uiB );
+    magsRoutine =
+        ( signA == signB ) ? softfloat_subMagsF32 : softfloat_addMagsF32;
+    return magsRoutine( uiA, uiB ^ 0x80000000, signA );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_f64.c b/target-riscv/fpu-custom-riscv/f32_to_f64.c
new file mode 100644
index 0000000..1c12dad
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_f64.c
@@ -0,0 +1,47 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t f32_to_f64( float32_t a )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    uint_fast64_t uiZ;
+    struct exp16_sig32 normExpSig;
+    union ui64_f64 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF32UI( uiA );
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( exp == 0xFF ) {
+        uiZ =
+            sig ? softfloat_commonNaNToF64UI(
+                      softfloat_f32UIToCommonNaN( uiA ) )
+                : packToF64UI( sign, 0x7FF, 0 );
+        goto uiZ;
+    }
+    if ( ! exp ) {
+        if ( ! sig ) {
+            uiZ = packToF64UI( sign, 0, 0 );
+            goto uiZ;
+        }
+        normExpSig = softfloat_normSubnormalF32Sig( sig );
+        exp = normExpSig.exp - 1;
+        sig = normExpSig.sig;
+    }
+    uiZ = packToF64UI( sign, exp + 0x380, (uint_fast64_t) sig<<29 );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_i32.c b/target-riscv/fpu-custom-riscv/f32_to_i32.c
new file mode 100644
index 0000000..cd11b73
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_i32.c
@@ -0,0 +1,34 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast32_t f32_to_i32( float32_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    uint_fast64_t sig64;
+    int_fast16_t shiftCount;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF32UI( uiA );
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( ( exp == 0xFF ) && sig ) sign = 0;
+    if ( exp ) sig |= 0x00800000;
+    sig64 = (uint_fast64_t) sig<<32;
+    shiftCount = 0xAF - exp;
+    if ( 0 < shiftCount ) {
+        sig64 = softfloat_shift64RightJam( sig64, shiftCount );
+    }
+    return softfloat_roundPackToI32( sign, sig64, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_i32_r_minMag.c b/target-riscv/fpu-custom-riscv/f32_to_i32_r_minMag.c
new file mode 100644
index 0000000..5ca24bb
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_i32_r_minMag.c
@@ -0,0 +1,45 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast32_t f32_to_i32_r_minMag( float32_t a, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    bool sign;
+    int_fast16_t shiftCount;
+    int_fast32_t absZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( exp < 0x7F ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    sign = signF32UI( uiA );
+    shiftCount = 0x9E - exp;
+    if ( shiftCount <= 0 ) {
+        if ( uiA != packToF32UI( 1, 0x9E, 0 ) ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+            if ( ! sign || ( ( exp == 0xFF ) && sig ) ) return 0x7FFFFFFF;
+        }
+        return -0x7FFFFFFF - 1;
+    }
+    sig = ( sig | 0x00800000 )<<8;
+    absZ = sig>>shiftCount;
+    if ( exact && (uint32_t) ( sig<<( ( - shiftCount ) & 31 ) ) ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return sign ? - absZ : absZ;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_i64.c b/target-riscv/fpu-custom-riscv/f32_to_i64.c
new file mode 100644
index 0000000..5d9956e
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_i64.c
@@ -0,0 +1,44 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast64_t f32_to_i64( float32_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    int_fast16_t shiftCount;
+    uint_fast64_t sig64, extra;
+    struct uint64_extra sig64Extra;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF32UI( uiA );
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    shiftCount = 0xBE - exp;
+    if ( shiftCount < 0 ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        if ( ! sign || ( ( exp == 0xFF ) && sig ) ) {
+            return INT64_C( 0x7FFFFFFFFFFFFFFF );
+        }
+        return - INT64_C( 0x7FFFFFFFFFFFFFFF ) - 1;
+    }
+    if ( exp ) sig |= 0x00800000;
+    sig64 = (uint_fast64_t) sig<<40;
+    extra = 0;
+    if ( shiftCount ) {
+        sig64Extra = softfloat_shift64ExtraRightJam( sig64, 0, shiftCount );
+        sig64 = sig64Extra.v;
+        extra = sig64Extra.extra;
+    }
+    return softfloat_roundPackToI64( sign, sig64, extra, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_i64_r_minMag.c b/target-riscv/fpu-custom-riscv/f32_to_i64_r_minMag.c
new file mode 100644
index 0000000..4d7139b
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_i64_r_minMag.c
@@ -0,0 +1,52 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast64_t f32_to_i64_r_minMag( float32_t a, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    bool sign;
+    int_fast16_t shiftCount;
+    uint_fast64_t sig64;
+    int_fast64_t absZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( exp < 0x7F ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    sign = signF32UI( uiA );
+    shiftCount = 0xBE - exp;
+    if ( shiftCount <= 0 ) {
+        if ( uiA != packToF32UI( 1, 0xBE, 0 ) ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+            if ( ! sign || ( ( exp == 0xFF ) && sig ) ) {
+                return INT64_C( 0x7FFFFFFFFFFFFFFF );
+            }
+        }
+        return - INT64_C( 0x7FFFFFFFFFFFFFFF ) - 1;
+    }
+    sig |= 0x00800000;
+    sig64 = (uint_fast64_t) sig<<40;
+    absZ = sig64>>shiftCount;
+    shiftCount = 40 - shiftCount;
+    if (
+        exact && ( shiftCount < 0 ) && (uint32_t) ( sig<<( shiftCount & 31 ) )
+    ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return sign ? - absZ : absZ;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_ui32.c b/target-riscv/fpu-custom-riscv/f32_to_ui32.c
new file mode 100644
index 0000000..fb4822e
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_ui32.c
@@ -0,0 +1,33 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast32_t f32_to_ui32( float32_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    uint_fast64_t sig64;
+    int_fast16_t shiftCount;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF32UI( uiA );
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( exp ) sig |= 0x00800000;
+    sig64 = (uint_fast64_t) sig<<32;
+    shiftCount = 0xAF - exp;
+    if ( 0 < shiftCount ) {
+        sig64 = softfloat_shift64RightJam( sig64, shiftCount );
+    }
+    return softfloat_roundPackToUI32( sign, sig64, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_ui32_r_minMag.c b/target-riscv/fpu-custom-riscv/f32_to_ui32_r_minMag.c
new file mode 100644
index 0000000..c438b55
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_ui32_r_minMag.c
@@ -0,0 +1,41 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast32_t f32_to_ui32_r_minMag( float32_t a, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    int_fast16_t shiftCount;
+    uint_fast32_t z;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( exp < 0x7F ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    if ( signF32UI( uiA ) ) goto invalid;
+    shiftCount = 0x9E - exp;
+    if ( shiftCount < 0 ) goto invalid;
+    sig = ( sig | 0x00800000 )<<8;
+    z = sig>>shiftCount;
+    if ( exact && ( sig & ( ( (uint_fast32_t) 1<<shiftCount ) - 1 ) ) ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return 0xFFFFFFFF;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_ui64.c b/target-riscv/fpu-custom-riscv/f32_to_ui64.c
new file mode 100644
index 0000000..6fac696
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_ui64.c
@@ -0,0 +1,42 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast64_t f32_to_ui64( float32_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    int_fast16_t shiftCount;
+    uint_fast64_t sig64, extra;
+    struct uint64_extra sig64Extra;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF32UI( uiA );
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    shiftCount = 0xBE - exp;
+    if ( shiftCount < 0 ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return UINT64_C( 0xFFFFFFFFFFFFFFFF );
+    }
+    if ( exp ) sig |= 0x00800000;
+    sig64 = (uint_fast64_t) sig<<40;
+    extra = 0;
+    if ( shiftCount ) {
+        sig64Extra = softfloat_shift64ExtraRightJam( sig64, 0, shiftCount );
+        sig64 = sig64Extra.v;
+        extra = sig64Extra.extra;
+    }
+    return
+        softfloat_roundPackToUI64( sign, sig64, extra, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f32_to_ui64_r_minMag.c b/target-riscv/fpu-custom-riscv/f32_to_ui64_r_minMag.c
new file mode 100644
index 0000000..9d39d16
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f32_to_ui64_r_minMag.c
@@ -0,0 +1,45 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast64_t f32_to_ui64_r_minMag( float32_t a, bool exact )
+{
+    union ui32_f32 uA;
+    uint_fast32_t uiA;
+    int_fast16_t exp;
+    uint_fast32_t sig;
+    int_fast16_t shiftCount;
+    uint_fast64_t sig64, z;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF32UI( uiA );
+    sig = fracF32UI( uiA );
+    if ( exp < 0x7F ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    if ( signF32UI( uiA ) ) goto invalid;
+    shiftCount = 0xBE - exp;
+    if ( shiftCount < 0 ) goto invalid;
+    sig |= 0x00800000;
+    sig64 = (uint_fast64_t) sig<<40;
+    z = sig64>>shiftCount;
+    shiftCount = 40 - shiftCount;
+    if (
+        exact && ( shiftCount < 0 ) && (uint32_t) ( sig<<( shiftCount & 31 ) )
+    ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return UINT64_C( 0xFFFFFFFFFFFFFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_add.c b/target-riscv/fpu-custom-riscv/f64_add.c
new file mode 100644
index 0000000..9ec4b5f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_add.c
@@ -0,0 +1,29 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t f64_add( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool signA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signB;
+    float64_t ( *magsRoutine )( uint_fast64_t, uint_fast64_t, bool );
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF64UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF64UI( uiB );
+    magsRoutine =
+        ( signA == signB ) ? softfloat_addMagsF64 : softfloat_subMagsF64;
+    return magsRoutine( uiA, uiB, signA );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_classify.c b/target-riscv/fpu-custom-riscv/f64_classify.c
new file mode 100644
index 0000000..2ec124b
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_classify.c
@@ -0,0 +1,33 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+uint_fast16_t f64_classify( float64_t a )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+
+    uA.f = a;
+    uiA = uA.ui;
+
+    uint_fast16_t infOrNaN = expF64UI( uiA ) == 0x7FF;
+    uint_fast16_t subnormalOrZero = expF64UI( uiA ) == 0;
+    bool sign = signF64UI( uiA );
+
+    return
+        (  sign && infOrNaN && fracF64UI( uiA ) == 0 )          << 0 |
+        (  sign && !infOrNaN && !subnormalOrZero )              << 1 |
+        (  sign && subnormalOrZero && fracF64UI( uiA ) )        << 2 |
+        (  sign && subnormalOrZero && fracF64UI( uiA ) == 0 )   << 3 |
+        ( !sign && infOrNaN && fracF64UI( uiA ) == 0 )          << 7 |
+        ( !sign && !infOrNaN && !subnormalOrZero )              << 6 |
+        ( !sign && subnormalOrZero && fracF64UI( uiA ) )        << 5 |
+        ( !sign && subnormalOrZero && fracF64UI( uiA ) == 0 )   << 4 |
+        ( isNaNF64UI( uiA ) &&  softfloat_isSigNaNF64UI( uiA )) << 8 |
+        ( isNaNF64UI( uiA ) && !softfloat_isSigNaNF64UI( uiA )) << 9;
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_div.c b/target-riscv/fpu-custom-riscv/f64_div.c
new file mode 100644
index 0000000..9bc72b3
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_div.c
@@ -0,0 +1,104 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t f64_div( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast64_t sigA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signB;
+    int_fast16_t expB;
+    uint_fast64_t sigB;
+    bool signZ;
+    struct exp16_sig64 normExpSig;
+    int_fast16_t expZ;
+    uint_fast64_t sigZ;
+    struct uint128 term, rem;
+    uint_fast64_t uiZ;
+    union ui64_f64 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF64UI( uiA );
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF64UI( uiB );
+    expB = expF64UI( uiB );
+    sigB = fracF64UI( uiB );
+    signZ = signA ^ signB;
+    if ( expA == 0x7FF ) {
+        if ( sigA ) goto propagateNaN;
+        if ( expB == 0x7FF ) {
+            if ( sigB ) goto propagateNaN;
+            goto invalid;
+        }
+        goto infinity;
+    }
+    if ( expB == 0x7FF ) {
+        if ( sigB ) goto propagateNaN;
+        goto zero;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) {
+            if ( ! ( expA | sigA ) ) goto invalid;
+            softfloat_raiseFlags( softfloat_flag_infinity );
+            goto infinity;
+        }
+        normExpSig = softfloat_normSubnormalF64Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) goto zero;
+        normExpSig = softfloat_normSubnormalF64Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    expZ = expA - expB + 0x3FD;
+    sigA = ( sigA | UINT64_C( 0x0010000000000000 ) )<<10;
+    sigB = ( sigB | UINT64_C( 0x0010000000000000 ) )<<11;
+    if ( sigB <= ( sigA + sigA ) ) {
+        ++expZ;
+        sigA >>= 1;
+    }
+    sigZ = softfloat_estimateDiv128To64( sigA, 0, sigB );
+    if ( ( sigZ & 0x1FF ) <= 2 ) {
+        term = softfloat_mul64To128( sigB, sigZ );
+        rem = softfloat_sub128( sigA, 0, term.v64, term.v0 );
+        while ( UINT64_C( 0x8000000000000000 ) <= rem.v64 ) {
+            --sigZ;
+            rem = softfloat_add128( rem.v64, rem.v0, 0, sigB );
+        }
+        sigZ |= ( rem.v0 != 0 );
+    }
+    return softfloat_roundPackToF64( signZ, expZ, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF64UI( uiA, uiB );
+    goto uiZ;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    uiZ = defaultNaNF64UI;
+    goto uiZ;
+ infinity:
+    uiZ = packToF64UI( signZ, 0x7FF, 0 );
+    goto uiZ;
+ zero:
+    uiZ = packToF64UI( signZ, 0, 0 );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_eq.c b/target-riscv/fpu-custom-riscv/f64_eq.c
new file mode 100644
index 0000000..925aabc
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_eq.c
@@ -0,0 +1,35 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f64_eq( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF64UI( uiA ) == 0x7FF ) && fracF64UI( uiA ) )
+        || ( ( expF64UI( uiB ) == 0x7FF ) && fracF64UI( uiB ) )
+    ) {
+        if (
+            softfloat_isSigNaNF64UI( uiA ) || softfloat_isSigNaNF64UI( uiB )
+        ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+        }
+        return false;
+    }
+    return
+        ( uiA == uiB ) || ! ( ( uiA | uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_eq_signaling.c b/target-riscv/fpu-custom-riscv/f64_eq_signaling.c
new file mode 100644
index 0000000..7a54dc1
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_eq_signaling.c
@@ -0,0 +1,30 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+bool f64_eq_signaling( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF64UI( uiA ) == 0x7FF ) && fracF64UI( uiA ) )
+        || ( ( expF64UI( uiB ) == 0x7FF ) && fracF64UI( uiB ) )
+    ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return false;
+    }
+    return
+        ( uiA == uiB ) || ! ( ( uiA | uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_isSignalingNaN.c b/target-riscv/fpu-custom-riscv/f64_isSignalingNaN.c
new file mode 100644
index 0000000..d720ac1
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_isSignalingNaN.c
@@ -0,0 +1,16 @@
+
+#include <stdbool.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f64_isSignalingNaN( float64_t a )
+{
+    union ui64_f64 uA;
+
+    uA.f = a;
+    return softfloat_isSigNaNF64UI( uA.ui );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_le.c b/target-riscv/fpu-custom-riscv/f64_le.c
new file mode 100644
index 0000000..e6c5caf
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_le.c
@@ -0,0 +1,35 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+bool f64_le( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF64UI( uiA ) == 0x7FF ) && fracF64UI( uiA ) )
+        || ( ( expF64UI( uiB ) == 0x7FF ) && fracF64UI( uiB ) )
+    ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return false;
+    }
+    signA = signF64UI( uiA );
+    signB = signF64UI( uiB );
+    return
+        ( signA != signB )
+            ? signA || ! ( ( uiA | uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) )
+            : ( uiA == uiB ) || ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_le_quiet.c b/target-riscv/fpu-custom-riscv/f64_le_quiet.c
new file mode 100644
index 0000000..e9b7ede
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_le_quiet.c
@@ -0,0 +1,40 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f64_le_quiet( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF64UI( uiA ) == 0x7FF ) && fracF64UI( uiA ) )
+        || ( ( expF64UI( uiB ) == 0x7FF ) && fracF64UI( uiB ) )
+    ) {
+        if (
+            softfloat_isSigNaNF64UI( uiA ) || softfloat_isSigNaNF64UI( uiB )
+        ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+        }
+        return false;
+    }
+    signA = signF64UI( uiA );
+    signB = signF64UI( uiB );
+    return
+        ( signA != signB )
+            ? signA || ! ( ( uiA | uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) )
+            : ( uiA == uiB ) || ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_lt.c b/target-riscv/fpu-custom-riscv/f64_lt.c
new file mode 100644
index 0000000..1b2f696
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_lt.c
@@ -0,0 +1,35 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+bool f64_lt( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF64UI( uiA ) == 0x7FF ) && fracF64UI( uiA ) )
+        || ( ( expF64UI( uiB ) == 0x7FF ) && fracF64UI( uiB ) )
+    ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return false;
+    }
+    signA = signF64UI( uiA );
+    signB = signF64UI( uiB );
+    return
+        ( signA != signB )
+            ? signA && ( ( uiA | uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) )
+            : ( uiA != uiB ) && ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_lt_quiet.c b/target-riscv/fpu-custom-riscv/f64_lt_quiet.c
new file mode 100644
index 0000000..f27e6da
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_lt_quiet.c
@@ -0,0 +1,40 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+bool f64_lt_quiet( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signA, signB;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    if (
+           ( ( expF64UI( uiA ) == 0x7FF ) && fracF64UI( uiA ) )
+        || ( ( expF64UI( uiB ) == 0x7FF ) && fracF64UI( uiB ) )
+    ) {
+        if (
+            softfloat_isSigNaNF64UI( uiA ) || softfloat_isSigNaNF64UI( uiB )
+        ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+        }
+        return false;
+    }
+    signA = signF64UI( uiA );
+    signB = signF64UI( uiB );
+    return
+        ( signA != signB )
+            ? signA && ( ( uiA | uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) )
+            : ( uiA != uiB ) && ( signA ^ ( uiA < uiB ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_mul.c b/target-riscv/fpu-custom-riscv/f64_mul.c
new file mode 100644
index 0000000..5a7b8a4
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_mul.c
@@ -0,0 +1,91 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t f64_mul( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast64_t sigA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signB;
+    int_fast16_t expB;
+    uint_fast64_t sigB;
+    bool signZ;
+    uint_fast64_t magBits;
+    struct exp16_sig64 normExpSig;
+    int_fast16_t expZ;
+    struct uint128 sigZ128;
+    uint_fast64_t sigZ, uiZ;
+    union ui64_f64 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF64UI( uiA );
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF64UI( uiB );
+    expB = expF64UI( uiB );
+    sigB = fracF64UI( uiB );
+    signZ = signA ^ signB;
+    if ( expA == 0x7FF ) {
+        if ( sigA || ( ( expB == 0x7FF ) && sigB ) ) goto propagateNaN;
+        magBits = expB | sigB;
+        goto infArg;
+    }
+    if ( expB == 0x7FF ) {
+        if ( sigB ) goto propagateNaN;
+        magBits = expA | sigA;
+        goto infArg;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) goto zero;
+        normExpSig = softfloat_normSubnormalF64Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) goto zero;
+        normExpSig = softfloat_normSubnormalF64Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    expZ = expA + expB - 0x3FF;
+    sigA = ( sigA | UINT64_C( 0x0010000000000000 ) )<<10;
+    sigB = ( sigB | UINT64_C( 0x0010000000000000 ) )<<11;
+    sigZ128 = softfloat_mul64To128( sigA, sigB );
+    sigZ = sigZ128.v64 | ( sigZ128.v0 != 0 );
+    if ( sigZ < UINT64_C( 0x4000000000000000 ) ) {
+        --expZ;
+        sigZ <<= 1;
+    }
+    return softfloat_roundPackToF64( signZ, expZ, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF64UI( uiA, uiB );
+    goto uiZ;
+ infArg:
+    if ( ! magBits ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        uiZ = defaultNaNF64UI;
+    } else {
+        uiZ = packToF64UI( signZ, 0x7FF, 0 );
+    }
+    goto uiZ;
+ zero:
+    uiZ = packToF64UI( signZ, 0, 0 );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_mulAdd.c b/target-riscv/fpu-custom-riscv/f64_mulAdd.c
new file mode 100644
index 0000000..a9f5891
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_mulAdd.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t f64_mulAdd( float64_t a, float64_t b, float64_t c )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    union ui64_f64 uC;
+    uint_fast64_t uiC;
+
+    uA.f = a;
+    uiA = uA.ui;
+    uB.f = b;
+    uiB = uB.ui;
+    uC.f = c;
+    uiC = uC.ui;
+    return softfloat_mulAddF64( 0, uiA, uiB, uiC );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_rem.c b/target-riscv/fpu-custom-riscv/f64_rem.c
new file mode 100644
index 0000000..ffb031f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_rem.c
@@ -0,0 +1,113 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t f64_rem( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast64_t sigA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+//    bool signB;
+    int_fast16_t expB;
+    uint_fast64_t sigB;
+    struct exp16_sig64 normExpSig;
+    int_fast16_t expDiff;
+    uint_fast64_t q, alternateSigA;
+    uint64_t sigMean;
+    bool signZ;
+    uint_fast64_t uiZ;
+    union ui64_f64 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF64UI( uiA );
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+//    signB = signF64UI( uiB );
+    expB = expF64UI( uiB );
+    sigB = fracF64UI( uiB );
+    if ( expA == 0x7FF ) {
+        if ( sigA || ( ( expB == 0x7FF ) && sigB ) ) goto propagateNaN;
+        goto invalid;
+    }
+    if ( expB == 0x7FF ) {
+        if ( sigB ) goto propagateNaN;
+        return a;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) goto invalid;
+        normExpSig = softfloat_normSubnormalF64Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) return a;
+        normExpSig = softfloat_normSubnormalF64Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    expDiff = expA - expB;
+    sigA = ( sigA | UINT64_C( 0x0010000000000000 ) )<<11;
+    sigB = ( sigB | UINT64_C( 0x0010000000000000 ) )<<11;
+    if ( expDiff < 0 ) {
+        if ( expDiff < -1 ) return a;
+        sigA >>= 1;
+    }
+    q = ( sigB <= sigA );
+    if ( q ) sigA -= sigB;
+    expDiff -= 64;
+    while ( 0 < expDiff ) {
+        q = softfloat_estimateDiv128To64( sigA, 0, sigB );
+        q = ( 2 < q ) ? q - 2 : 0;
+        sigA = - ( ( sigB>>2 ) * q );
+        expDiff -= 62;
+    }
+    expDiff += 64;
+    if ( 0 < expDiff ) {
+        q = softfloat_estimateDiv128To64( sigA, 0, sigB );
+        q = ( 2 < q ) ? q - 2 : 0;
+        q >>= 64 - expDiff;
+        sigB >>= 2;
+        sigA = ( ( sigA>>1 )<<( expDiff - 1 ) ) - sigB * q;
+    } else {
+        sigA >>= 2;
+        sigB >>= 2;
+    }
+    do {
+        alternateSigA = sigA;
+        ++q;
+        sigA -= sigB;
+    } while ( sigA < UINT64_C( 0x8000000000000000 ) );
+    sigMean = sigA + alternateSigA;
+    if (
+        ( UINT64_C( 0x8000000000000000 ) <= sigMean )
+            || ( ! sigMean && ( q & 1 ) )
+    ) {
+        sigA = alternateSigA;
+    }
+    signZ = ( UINT64_C( 0x8000000000000000 ) <= sigA );
+    if ( signZ ) sigA = - sigA;
+    return softfloat_normRoundPackToF64( signA ^ signZ, expB, sigA );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF64UI( uiA, uiB );
+    goto uiZ;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    uiZ = defaultNaNF64UI;
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_roundToInt.c b/target-riscv/fpu-custom-riscv/f64_roundToInt.c
new file mode 100644
index 0000000..ef16dfa
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_roundToInt.c
@@ -0,0 +1,80 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t f64_roundToInt( float64_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    int_fast16_t expA;
+    uint_fast64_t uiZ;
+    bool signA;
+    uint_fast64_t lastBitMask, roundBitsMask;
+    union ui64_f64 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    expA = expF64UI( uiA );
+    if ( 0x433 <= expA ) {
+        if ( ( expA == 0x7FF ) && fracF64UI( uiA ) ) {
+            uiZ = softfloat_propagateNaNF64UI( uiA, 0 );
+            goto uiZ;
+        }
+        return a;
+    }
+    if ( expA <= 0x3FE ) {
+        if ( ! ( uiA & UINT64_C( 0x7FFFFFFFFFFFFFFF ) ) ) return a;
+        if ( exact ) softfloat_exceptionFlags |= softfloat_flag_inexact;
+        signA = signF64UI( uiA );
+        switch ( roundingMode ) {
+         case softfloat_round_nearest_even:
+            if ( ( expA == 0x3FE ) && fracF64UI( uiA ) ) {
+                uiZ = packToF64UI( signA, 0x3FF, 0 );
+                goto uiZ;
+            }
+            break;
+         case softfloat_round_min:
+            uiZ = signA ? UINT64_C( 0xBFF0000000000000 ) : 0;
+            goto uiZ;
+         case softfloat_round_max:
+            uiZ =
+                signA ? UINT64_C( 0x8000000000000000 )
+                    : UINT64_C( 0x3FF0000000000000 );
+            goto uiZ;
+         case softfloat_round_nearest_maxMag:
+            if ( expA == 0x3FE ) {
+                uiZ = packToF64UI( signA, 0x3FF, 0 );
+                goto uiZ;
+            }
+            break;
+        }
+        uiZ = packToF64UI( signA, 0, 0 );
+        goto uiZ;
+    }
+    lastBitMask = (uint_fast64_t) 1<<( 0x433 - expA );
+    roundBitsMask = lastBitMask - 1;
+    uiZ = uiA;
+    if ( roundingMode == softfloat_round_nearest_maxMag ) {
+        uiZ += lastBitMask>>1;
+    } else if ( roundingMode == softfloat_round_nearest_even ) {
+        uiZ += lastBitMask>>1;
+        if ( ! ( uiZ & roundBitsMask ) ) uiZ &= ~ lastBitMask;
+    } else if ( roundingMode != softfloat_round_minMag ) {
+        if ( signF64UI( uiZ ) ^ ( roundingMode == softfloat_round_max ) ) {
+            uiZ += roundBitsMask;
+        }
+    }
+    uiZ &= ~ roundBitsMask;
+    if ( exact && ( uiZ != uiA ) ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_sqrt.c b/target-riscv/fpu-custom-riscv/f64_sqrt.c
new file mode 100644
index 0000000..e11e672
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_sqrt.c
@@ -0,0 +1,74 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t f64_sqrt( float64_t a )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool signA;
+    int_fast16_t expA;
+    uint_fast64_t sigA, uiZ;
+    struct exp16_sig64 normExpSig;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ32;
+    uint_fast64_t sigZ;
+    struct uint128 term, rem;
+    union ui64_f64 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF64UI( uiA );
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    if ( expA == 0x7FF ) {
+        if ( sigA ) {
+            uiZ = softfloat_propagateNaNF64UI( uiA, 0 );
+            goto uiZ;
+        }
+        if ( ! signA ) return a;
+        goto invalid;
+    }
+    if ( signA ) {
+        if ( ! ( expA | sigA ) ) return a;
+        goto invalid;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) return a;
+        normExpSig = softfloat_normSubnormalF64Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    expZ = ( ( expA - 0x3FF )>>1 ) + 0x3FE;
+    sigA |= UINT64_C( 0x0010000000000000 );
+    sigZ32 = softfloat_estimateSqrt32( expA, sigA>>21 );
+    sigA <<= 9 - ( expA & 1 );
+    sigZ =
+        softfloat_estimateDiv128To64( sigA, 0, (uint_fast64_t) sigZ32<<32 )
+            + ( (uint_fast64_t) sigZ32<<30 );
+    if ( ( sigZ & 0x1FF ) <= 5 ) {
+        term = softfloat_mul64To128( sigZ, sigZ );
+        rem = softfloat_sub128( sigA, 0, term.v64, term.v0 );
+        while ( UINT64_C( 0x8000000000000000 ) <= rem.v64 ) {
+            --sigZ;
+            rem =
+                softfloat_add128(
+                    rem.v64, rem.v0, sigZ>>63, (uint64_t) ( sigZ<<1 ) );
+        }
+        sigZ |= ( ( rem.v64 | rem.v0 ) != 0 );
+    }
+    return softfloat_roundPackToF64( 0, expZ, sigZ );
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    uiZ = defaultNaNF64UI;
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_sub.c b/target-riscv/fpu-custom-riscv/f64_sub.c
new file mode 100644
index 0000000..38bd574
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_sub.c
@@ -0,0 +1,29 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t f64_sub( float64_t a, float64_t b )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool signA;
+    union ui64_f64 uB;
+    uint_fast64_t uiB;
+    bool signB;
+    float64_t ( *magsRoutine )( uint_fast64_t, uint_fast64_t, bool );
+
+    uA.f = a;
+    uiA = uA.ui;
+    signA = signF64UI( uiA );
+    uB.f = b;
+    uiB = uB.ui;
+    signB = signF64UI( uiB );
+    magsRoutine =
+        ( signA == signB ) ? softfloat_subMagsF64 : softfloat_addMagsF64;
+    return magsRoutine( uiA, uiB, signA );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_f32.c b/target-riscv/fpu-custom-riscv/f64_to_f32.c
new file mode 100644
index 0000000..bfa04bf
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_f32.c
@@ -0,0 +1,43 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t f64_to_f32( float64_t a )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    uint_fast32_t uiZ, sig32;
+    union ui32_f32 uZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF64UI( uiA );
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp == 0x7FF ) {
+        uiZ =
+            sig ? softfloat_commonNaNToF32UI(
+                      softfloat_f64UIToCommonNaN( uiA ) )
+                : packToF32UI( sign, 0xFF, 0 );
+        goto uiZ;
+    }
+    sig32 = softfloat_shortShift64RightJam( sig, 22 );
+    if ( ! ( exp | sig32 ) ) {
+        uiZ = packToF32UI( sign, 0, 0 );
+        goto uiZ;
+    }
+    return softfloat_roundPackToF32( sign, exp - 0x381, sig32 | 0x40000000 );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_i32.c b/target-riscv/fpu-custom-riscv/f64_to_i32.c
new file mode 100644
index 0000000..b77f2c4
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_i32.c
@@ -0,0 +1,30 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast32_t f64_to_i32( float64_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF64UI( uiA );
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( ( exp == 0x7FF ) && sig ) sign = 0;
+    if ( exp ) sig |= UINT64_C( 0x0010000000000000 );
+    shiftCount = 0x42C - exp;
+    if ( 0 < shiftCount ) sig = softfloat_shift64RightJam( sig, shiftCount );
+    return softfloat_roundPackToI32( sign, sig, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_i32_r_minMag.c b/target-riscv/fpu-custom-riscv/f64_to_i32_r_minMag.c
new file mode 100644
index 0000000..d221bd6
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_i32_r_minMag.c
@@ -0,0 +1,50 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast32_t f64_to_i32_r_minMag( float64_t a, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    bool sign;
+    int_fast16_t shiftCount;
+    uint_fast32_t absZ;
+    union { uint32_t ui; int32_t i; } uZ;
+    int_fast32_t z;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp < 0x3FF ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    sign = signF64UI( uiA );
+    if ( 0x41E < exp ) {
+        if ( ( exp == 0x7FF ) && sig ) sign = 0;
+        goto invalid;
+    }
+    sig |= UINT64_C( 0x0010000000000000 );
+    shiftCount = 0x433 - exp;
+    absZ = sig>>shiftCount;
+    uZ.ui = sign ? - absZ : absZ;
+    z = uZ.i;
+    if ( ( z < 0 ) != sign ) goto invalid;
+    if ( exact && ( (uint_fast64_t) absZ<<shiftCount != sig ) ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return sign ? -0x7FFFFFFF - 1 : 0x7FFFFFFF;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_i64.c b/target-riscv/fpu-custom-riscv/f64_to_i64.c
new file mode 100644
index 0000000..04b8c9f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_i64.c
@@ -0,0 +1,46 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast64_t f64_to_i64( float64_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+    struct uint64_extra sigExtra;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF64UI( uiA );
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp ) sig |= UINT64_C( 0x0010000000000000 );
+    shiftCount = 0x433 - exp;
+    if ( shiftCount <= 0 ) {
+        if ( 0x43E < exp ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+            return
+                ! sign
+                    || ( ( exp == 0x7FF )
+                             && fracF64UI( uiA ) )
+                    ? INT64_C( 0x7FFFFFFFFFFFFFFF )
+                    : - INT64_C( 0x7FFFFFFFFFFFFFFF ) - 1;
+        }
+        sigExtra.v = sig<<( - shiftCount );
+        sigExtra.extra = 0;
+    } else {
+        sigExtra = softfloat_shift64ExtraRightJam( sig, 0, shiftCount );
+    }
+    return
+        softfloat_roundPackToI64(
+            sign, sigExtra.v, sigExtra.extra, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_i64_r_minMag.c b/target-riscv/fpu-custom-riscv/f64_to_i64_r_minMag.c
new file mode 100644
index 0000000..d3fc2ed
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_i64_r_minMag.c
@@ -0,0 +1,52 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast64_t f64_to_i64_r_minMag( float64_t a, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+    int_fast64_t absZ;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF64UI( uiA );
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    shiftCount = exp - 0x433;
+    if ( 0 <= shiftCount ) {
+        if ( 0x43E <= exp ) {
+            if ( uiA != packToF64UI( 1, 0x43E, 0 ) ) {
+                softfloat_raiseFlags( softfloat_flag_invalid );
+                if ( ! sign || ( ( exp == 0x7FF ) && sig ) ) {
+                    return INT64_C( 0x7FFFFFFFFFFFFFFF );
+                }
+            }
+            return - INT64_C( 0x7FFFFFFFFFFFFFFF ) - 1;
+        }
+        sig |= UINT64_C( 0x0010000000000000 );
+        absZ = sig<<shiftCount;
+    } else {
+        if ( exp < 0x3FF ) {
+            if ( exact && ( exp | sig ) ) {
+                softfloat_exceptionFlags |= softfloat_flag_inexact;
+            }
+            return 0;
+        }
+        sig |= UINT64_C( 0x0010000000000000 );
+        absZ = sig>>( - shiftCount );
+        if ( exact && (uint64_t) ( sig<<( shiftCount & 63 ) ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+    }
+    return sign ? - absZ : absZ;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_ui32.c b/target-riscv/fpu-custom-riscv/f64_to_ui32.c
new file mode 100644
index 0000000..d5a5105
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_ui32.c
@@ -0,0 +1,29 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast32_t f64_to_ui32( float64_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF64UI( uiA );
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp ) sig |= UINT64_C( 0x0010000000000000 );
+    shiftCount = 0x42C - exp;
+    if ( 0 < shiftCount ) sig = softfloat_shift64RightJam( sig, shiftCount );
+    return softfloat_roundPackToUI32( sign, sig, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_ui32_r_minMag.c b/target-riscv/fpu-custom-riscv/f64_to_ui32_r_minMag.c
new file mode 100644
index 0000000..6cf5f32
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_ui32_r_minMag.c
@@ -0,0 +1,40 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast32_t f64_to_ui32_r_minMag( float64_t a, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+    uint_fast32_t z;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp < 0x3FF ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    if ( signF64UI( uiA ) || ( 0x41E < exp ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        return 0xFFFFFFFF;
+    }
+    sig |= UINT64_C( 0x0010000000000000 );
+    shiftCount = 0x433 - exp;
+    z = sig>>shiftCount;
+    if ( exact && ( (uint_fast64_t) z<<shiftCount != sig ) ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_ui64.c b/target-riscv/fpu-custom-riscv/f64_to_ui64.c
new file mode 100644
index 0000000..00e933c
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_ui64.c
@@ -0,0 +1,41 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast64_t f64_to_ui64( float64_t a, int_fast8_t roundingMode, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    bool sign;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+    struct uint64_extra sigExtra;
+
+    uA.f = a;
+    uiA = uA.ui;
+    sign = signF64UI( uiA );
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp ) sig |= UINT64_C( 0x0010000000000000 );
+    shiftCount = 0x433 - exp;
+    if ( shiftCount <= 0 ) {
+        if ( 0x43E < exp ) {
+            softfloat_raiseFlags( softfloat_flag_invalid );
+            return UINT64_C( 0xFFFFFFFFFFFFFFFF );
+        }
+        sigExtra.v = sig<<( - shiftCount );
+        sigExtra.extra = 0;
+    } else {
+        sigExtra = softfloat_shift64ExtraRightJam( sig, 0, shiftCount );
+    }
+    return
+        softfloat_roundPackToUI64(
+            sign, sigExtra.v, sigExtra.extra, roundingMode, exact );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/f64_to_ui64_r_minMag.c b/target-riscv/fpu-custom-riscv/f64_to_ui64_r_minMag.c
new file mode 100644
index 0000000..f943337
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/f64_to_ui64_r_minMag.c
@@ -0,0 +1,45 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast64_t f64_to_ui64_r_minMag( float64_t a, bool exact )
+{
+    union ui64_f64 uA;
+    uint_fast64_t uiA;
+    int_fast16_t exp;
+    uint_fast64_t sig;
+    int_fast16_t shiftCount;
+    uint_fast64_t z;
+
+    uA.f = a;
+    uiA = uA.ui;
+    exp = expF64UI( uiA );
+    sig = fracF64UI( uiA );
+    if ( exp < 0x3FF ) {
+        if ( exact && ( exp | sig ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+        return 0;
+    }
+    if ( signF64UI( uiA ) ) goto invalid;
+    shiftCount = exp - 0x433;
+    if ( 0 <= shiftCount ) {
+        if ( 0x43E < exp ) goto invalid;
+        z = ( sig | UINT64_C( 0x0010000000000000 ) )<<shiftCount;
+    } else {
+        sig |= UINT64_C( 0x0010000000000000 );
+        z = sig>>( - shiftCount );
+        if ( exact && (uint64_t) ( sig<<( shiftCount & 63 ) ) ) {
+            softfloat_exceptionFlags |= softfloat_flag_inexact;
+        }
+    }
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return UINT64_C( 0xFFFFFFFFFFFFFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/i32_to_f32.c b/target-riscv/fpu-custom-riscv/i32_to_f32.c
new file mode 100644
index 0000000..d24d82e
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/i32_to_f32.c
@@ -0,0 +1,21 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t i32_to_f32( int_fast32_t a )
+{
+    bool sign;
+    union ui32_f32 uZ;
+
+    sign = ( a < 0 );
+    if ( ! ( a & 0x7FFFFFFF ) ) {
+        uZ.ui = sign ? packToF32UI( 1, 0x9E, 0 ) : 0;
+        return uZ.f;
+    }
+    return softfloat_normRoundPackToF32( sign, 0x9C, sign ? - a : a );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/i32_to_f64.c b/target-riscv/fpu-custom-riscv/i32_to_f64.c
new file mode 100644
index 0000000..eec8808
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/i32_to_f64.c
@@ -0,0 +1,31 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t i32_to_f64( int_fast32_t a )
+{
+    uint_fast64_t uiZ;
+    bool sign;
+    uint_fast32_t absA;
+    int shiftCount;
+    union ui64_f64 uZ;
+
+    if ( ! a ) {
+        uiZ = 0;
+    } else {
+        sign = ( a < 0 );
+        absA = sign ? - a : a;
+        shiftCount = softfloat_countLeadingZeros32( absA ) + 21;
+        uiZ =
+            packToF64UI(
+                sign, 0x432 - shiftCount, (uint_fast64_t) absA<<shiftCount );
+    }
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/i64_to_f32.c b/target-riscv/fpu-custom-riscv/i64_to_f32.c
new file mode 100644
index 0000000..33e81c9
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/i64_to_f32.c
@@ -0,0 +1,36 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t i64_to_f32( int_fast64_t a )
+{
+    bool sign;
+    uint_fast64_t absA;
+    int shiftCount;
+    union ui32_f32 u;
+    uint_fast32_t sig;
+
+    sign = ( a < 0 );
+    absA = sign ? - (uint_fast64_t) a : a;
+    shiftCount = softfloat_countLeadingZeros64( absA ) - 40;
+    if ( 0 <= shiftCount ) {
+        u.ui =
+            a ? packToF32UI(
+                    sign, 0x95 - shiftCount, (uint_fast32_t) absA<<shiftCount )
+                : 0;
+        return u.f;
+    } else {
+        shiftCount += 7;
+        sig =
+            ( shiftCount < 0 )
+                ? softfloat_shortShift64RightJam( absA, - shiftCount )
+                : (uint_fast32_t) absA<<shiftCount;
+        return softfloat_roundPackToF32( sign, 0x9C - shiftCount, sig );
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/i64_to_f64.c b/target-riscv/fpu-custom-riscv/i64_to_f64.c
new file mode 100644
index 0000000..4d595e0
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/i64_to_f64.c
@@ -0,0 +1,21 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t i64_to_f64( int_fast64_t a )
+{
+    bool sign;
+    union ui64_f64 uZ;
+
+    sign = ( a < 0 );
+    if ( ! ( a & UINT64_C( 0x7FFFFFFFFFFFFFFF ) ) ) {
+        uZ.ui = sign ? packToF64UI( 1, 0x43E, 0 ) : 0;
+        return uZ.f;
+    }
+    return softfloat_normRoundPackToF64( sign, 0x43C, sign ? - a : a );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/internals.h b/target-riscv/fpu-custom-riscv/internals.h
new file mode 100644
index 0000000..fcdc0b3
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/internals.h
@@ -0,0 +1,232 @@
+
+/*** UPDATE COMMENTS. ***/
+
+#include "softfloat_types.h"
+
+union ui32_f32 { uint32_t ui; float32_t f; };
+union ui64_f64 { uint64_t ui; float64_t f; };
+#ifdef LITTLEENDIAN
+union ui128_f128 { uint64_t ui0, ui64; float128_t f; };
+#else
+union ui128_f128 { uint64_t ui64, ui0; float128_t f; };
+#endif
+
+enum {
+    softfloat_mulAdd_subC    = 1,
+    softfloat_mulAdd_subProd = 2
+};
+
+uint_fast32_t
+ softfloat_roundPackToUI32( bool, uint_fast64_t, int_fast8_t, bool );
+uint_fast64_t
+ softfloat_roundPackToUI64(
+     bool, uint_fast64_t, uint_fast64_t, int_fast8_t, bool );
+/*----------------------------------------------------------------------------
+| Takes a 64-bit fixed-point value `absZ' with binary point between bits 6
+| and 7, and returns the properly rounded 32-bit integer corresponding to the
+| input.  If `zSign' is 1, the input is negated before being converted to an
+| integer.  Bit 63 of `absZ' must be zero.  Ordinarily, the fixed-point input
+| is simply rounded to an integer, with the inexact exception raised if the
+| input cannot be represented exactly as an integer.  However, if the fixed-
+| point input is too large, the invalid exception is raised and the largest
+| positive or negative integer is returned.
+*----------------------------------------------------------------------------*/
+int_fast32_t
+ softfloat_roundPackToI32( bool, uint_fast64_t, int_fast8_t, bool );
+/*----------------------------------------------------------------------------
+| Takes the 128-bit fixed-point value formed by concatenating `absZ0' and
+| `absZ1', with binary point between bits 63 and 64 (between the input words),
+| and returns the properly rounded 64-bit integer corresponding to the input.
+| If `zSign' is 1, the input is negated before being converted to an integer.
+| Ordinarily, the fixed-point input is simply rounded to an integer, with
+| the inexact exception raised if the input cannot be represented exactly as
+| an integer.  However, if the fixed-point input is too large, the invalid
+| exception is raised and the largest positive or negative integer is
+| returned.
+*----------------------------------------------------------------------------*/
+int_fast64_t
+ softfloat_roundPackToI64(
+     bool, uint_fast64_t, uint_fast64_t, int_fast8_t, bool );
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the single-precision floating-point value `a' is a NaN;
+| otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+#define isNaNF32UI( ui ) (0xFF000000<(uint32_t)((uint_fast32_t)(ui)<<1))
+/*----------------------------------------------------------------------------
+| Returns the sign bit of the single-precision floating-point value `a'.
+*----------------------------------------------------------------------------*/
+#define signF32UI( a ) ((bool)((uint32_t)(a)>>31))
+/*----------------------------------------------------------------------------
+| Returns the exponent bits of the single-precision floating-point value `a'.
+*----------------------------------------------------------------------------*/
+#define expF32UI( a ) ((int_fast16_t)((a)>>23)&0xFF)
+/*----------------------------------------------------------------------------
+| Returns the fraction bits of the single-precision floating-point value `a'.
+*----------------------------------------------------------------------------*/
+#define fracF32UI( a ) ((a)&0x007FFFFF)
+/*----------------------------------------------------------------------------
+| Packs the sign `zSign', exponent `zExp', and significand `zSig' into a
+| single-precision floating-point value, returning the result.  After being
+| shifted into the proper positions, the three fields are simply added
+| together to form the result.  This means that any integer portion of `zSig'
+| will be added into the exponent.  Since a properly normalized significand
+| will have an integer portion equal to 1, the `zExp' input should be 1 less
+| than the desired result exponent whenever `zSig' is a complete, normalized
+| significand.
+*----------------------------------------------------------------------------*/
+#define packToF32UI( sign, exp, sig ) (((uint32_t)(sign)<<31)+((uint32_t)(exp)<<23)+(sig))
+
+/*----------------------------------------------------------------------------
+| Normalizes the subnormal single-precision floating-point value represented
+| by the denormalized significand `aSig'.  The normalized exponent and
+| significand are stored at the locations pointed to by `zExpPtr' and
+| `zSigPtr', respectively.
+*----------------------------------------------------------------------------*/
+struct exp16_sig32 { int_fast16_t exp; uint_fast32_t sig; };
+struct exp16_sig32 softfloat_normSubnormalF32Sig( uint_fast32_t );
+
+/*----------------------------------------------------------------------------
+| Takes an abstract floating-point value having sign `zSign', exponent `zExp',
+| and significand `zSig', and returns the proper single-precision floating-
+| point value corresponding to the abstract input.  Ordinarily, the abstract
+| value is simply rounded and packed into the single-precision format, with
+| the inexact exception raised if the abstract input cannot be represented
+| exactly.  However, if the abstract value is too large, the overflow and
+| inexact exceptions are raised and an infinity or maximal finite value is
+| returned.  If the abstract value is too small, the input value is rounded to
+| a subnormal number, and the underflow and inexact exceptions are raised if
+| the abstract input cannot be represented exactly as a subnormal single-
+| precision floating-point number.
+|     The input significand `zSig' has its binary point between bits 30
+| and 29, which is 7 bits to the left of the usual location.  This shifted
+| significand must be normalized or smaller.  If `zSig' is not normalized,
+| `zExp' must be 0; in that case, the result returned is a subnormal number,
+| and it must not require rounding.  In the usual case that `zSig' is
+| normalized, `zExp' must be 1 less than the ``true'' floating-point exponent.
+| The handling of underflow and overflow follows the IEC/IEEE Standard for
+| Binary Floating-Point Arithmetic.
+*----------------------------------------------------------------------------*/
+float32_t softfloat_roundPackToF32( bool, int_fast16_t, uint_fast32_t );
+/*----------------------------------------------------------------------------
+| Takes an abstract floating-point value having sign `zSign', exponent `zExp',
+| and significand `zSig', and returns the proper single-precision floating-
+| point value corresponding to the abstract input.  This routine is just like
+| `roundAndPackFloat32' except that `zSig' does not have to be normalized.
+| Bit 31 of `zSig' must be zero, and `zExp' must be 1 less than the ``true''
+| floating-point exponent.
+*----------------------------------------------------------------------------*/
+float32_t softfloat_normRoundPackToF32( bool, int_fast16_t, uint_fast32_t );
+
+/*----------------------------------------------------------------------------
+| Returns the result of adding the absolute values of the single-precision
+| floating-point values `a' and `b'.  If `zSign' is 1, the sum is negated
+| before being returned.  `zSign' is ignored if the result is a NaN.
+| The addition is performed according to the IEC/IEEE Standard for Binary
+| Floating-Point Arithmetic.
+*----------------------------------------------------------------------------*/
+float32_t softfloat_addMagsF32( uint_fast32_t, uint_fast32_t, bool );
+/*----------------------------------------------------------------------------
+| Returns the result of subtracting the absolute values of the single-
+| precision floating-point values `a' and `b'.  If `zSign' is 1, the
+| difference is negated before being returned.  `zSign' is ignored if the
+| result is a NaN.  The subtraction is performed according to the IEC/IEEE
+| Standard for Binary Floating-Point Arithmetic.
+*----------------------------------------------------------------------------*/
+float32_t softfloat_subMagsF32( uint_fast32_t, uint_fast32_t, bool );
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+float32_t
+ softfloat_mulAddF32( int, uint_fast32_t, uint_fast32_t, uint_fast32_t );
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the double-precision floating-point value `a' is a NaN;
+| otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+#define isNaNF64UI( ui ) (UINT64_C(0xFFE0000000000000)<(uint64_t)((uint_fast64_t)(ui)<<1))
+/*----------------------------------------------------------------------------
+| Returns the sign bit of the double-precision floating-point value `a'.
+*----------------------------------------------------------------------------*/
+#define signF64UI( a ) ((bool)((uint64_t)(a)>>63))
+/*----------------------------------------------------------------------------
+| Returns the exponent bits of the double-precision floating-point value `a'.
+*----------------------------------------------------------------------------*/
+#define expF64UI( a ) ((int_fast16_t)((a)>>52)&0x7FF)
+/*----------------------------------------------------------------------------
+| Returns the fraction bits of the double-precision floating-point value `a'.
+*----------------------------------------------------------------------------*/
+#define fracF64UI( a ) ((a)&UINT64_C(0x000FFFFFFFFFFFFF))
+/*----------------------------------------------------------------------------
+| Packs the sign `zSign', exponent `zExp', and significand `zSig' into a
+| double-precision floating-point value, returning the result.  After being
+| shifted into the proper positions, the three fields are simply added
+| together to form the result.  This means that any integer portion of `zSig'
+| will be added into the exponent.  Since a properly normalized significand
+| will have an integer portion equal to 1, the `zExp' input should be 1 less
+| than the desired result exponent whenever `zSig' is a complete, normalized
+| significand.
+*----------------------------------------------------------------------------*/
+#define packToF64UI( sign, exp, sig ) (((uint64_t)(sign)<<63)+((uint64_t)(exp)<<52)+(sig))
+
+/*----------------------------------------------------------------------------
+| Normalizes the subnormal double-precision floating-point value represented
+| by the denormalized significand `aSig'.  The normalized exponent and
+| significand are stored at the locations pointed to by `zExpPtr' and
+| `zSigPtr', respectively.
+*----------------------------------------------------------------------------*/
+struct exp16_sig64 { int_fast16_t exp; uint_fast64_t sig; };
+struct exp16_sig64 softfloat_normSubnormalF64Sig( uint_fast64_t );
+
+/*----------------------------------------------------------------------------
+| Takes an abstract floating-point value having sign `zSign', exponent `zExp',
+| and significand `zSig', and returns the proper double-precision floating-
+| point value corresponding to the abstract input.  Ordinarily, the abstract
+| value is simply rounded and packed into the double-precision format, with
+| the inexact exception raised if the abstract input cannot be represented
+| exactly.  However, if the abstract value is too large, the overflow and
+| inexact exceptions are raised and an infinity or maximal finite value is
+| returned.  If the abstract value is too small, the input value is rounded
+| to a subnormal number, and the underflow and inexact exceptions are raised
+| if the abstract input cannot be represented exactly as a subnormal double-
+| precision floating-point number.
+|     The input significand `zSig' has its binary point between bits 62
+| and 61, which is 10 bits to the left of the usual location.  This shifted
+| significand must be normalized or smaller.  If `zSig' is not normalized,
+| `zExp' must be 0; in that case, the result returned is a subnormal number,
+| and it must not require rounding.  In the usual case that `zSig' is
+| normalized, `zExp' must be 1 less than the ``true'' floating-point exponent.
+| The handling of underflow and overflow follows the IEC/IEEE Standard for
+| Binary Floating-Point Arithmetic.
+*----------------------------------------------------------------------------*/
+float64_t softfloat_roundPackToF64( bool, int_fast16_t, uint_fast64_t );
+/*----------------------------------------------------------------------------
+| Takes an abstract floating-point value having sign `zSign', exponent `zExp',
+| and significand `zSig', and returns the proper double-precision floating-
+| point value corresponding to the abstract input.  This routine is just like
+| `roundAndPackFloat64' except that `zSig' does not have to be normalized.
+| Bit 63 of `zSig' must be zero, and `zExp' must be 1 less than the ``true''
+| floating-point exponent.
+*----------------------------------------------------------------------------*/
+float64_t softfloat_normRoundPackToF64( bool, int_fast16_t, uint_fast64_t );
+
+/*----------------------------------------------------------------------------
+| Returns the result of adding the absolute values of the double-precision
+| floating-point values `a' and `b'.  If `zSign' is 1, the sum is negated
+| before being returned.  `zSign' is ignored if the result is a NaN.
+| The addition is performed according to the IEC/IEEE Standard for Binary
+| Floating-Point Arithmetic.
+*----------------------------------------------------------------------------*/
+float64_t softfloat_addMagsF64( uint_fast64_t, uint_fast64_t, bool );
+/*----------------------------------------------------------------------------
+| Returns the result of subtracting the absolute values of the double-
+| precision floating-point values `a' and `b'.  If `zSign' is 1, the
+| difference is negated before being returned.  `zSign' is ignored if the
+| result is a NaN.  The subtraction is performed according to the IEC/IEEE
+| Standard for Binary Floating-Point Arithmetic.
+*----------------------------------------------------------------------------*/
+float64_t softfloat_subMagsF64( uint_fast64_t, uint_fast64_t, bool );
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+float64_t
+ softfloat_mulAddF64( int, uint_fast64_t, uint_fast64_t, uint_fast64_t );
+
diff --git a/target-riscv/fpu-custom-riscv/platform.h b/target-riscv/fpu-custom-riscv/platform.h
new file mode 100644
index 0000000..6c54313
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/platform.h
@@ -0,0 +1,42 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+#define LITTLEENDIAN
+
+#ifndef UINT64_C
+# define UINT64_C(x) (x ## ULL)
+# define INT64_C(x) (x ## LL)
+#endif
diff --git a/target-riscv/fpu-custom-riscv/primitives.h b/target-riscv/fpu-custom-riscv/primitives.h
new file mode 100644
index 0000000..71038ea
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/primitives.h
@@ -0,0 +1,628 @@
+
+/*============================================================================
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 3.
+
+*** UPDATE
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal notice) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+#include <stdbool.h>
+#include <stdint.h>
+
+/*** CHANGE TO USE `fast' INTEGER TYPES? ***/
+/*** ADD 80-BIT FUNCTIONS? ***/
+
+#ifdef LITTLEENDIAN
+struct uintx80 { uint64_t v0; uint16_t v64; };
+struct uint128 { uint64_t v0, v64; };
+struct uint192 { uint64_t v0, v64, v128; };
+struct uint256 { uint64_t v0, v64, v128, v192; };
+#else
+struct uintx80 { uint16_t v64; uint64_t v0; };
+struct uint128 { uint64_t v64, v0; };
+struct uint192 { uint64_t v128, v64, v0; };
+struct uint256 { uint64_t v256, v128, v64, v0; };
+#endif
+
+struct uint64_extra { uint64_t v, extra; };
+struct uint128_extra { uint64_t v64; uint64_t v0; uint64_t extra; };
+
+
+/*** SHIFT COUNTS CANNOT BE ZERO.  MUST CHECK BEFORE CALLING! ***/
+
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the 128-bit value formed by concatenating `a0' and `a1'
+| is equal to the 128-bit value formed by concatenating `b0' and `b1'.
+| Otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool
+ softfloat_eq128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+    { return ( a64 == b64 ) && ( a0 == b0 ); }
+#else
+bool softfloat_eq128( uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the 128-bit value formed by concatenating `a0' and `a1' is less
+| than or equal to the 128-bit value formed by concatenating `b0' and `b1'.
+| Otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool
+ softfloat_le128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+    { return ( a64 < b64 ) || ( ( a64 == b64 ) && ( a0 <= b0 ) ); }
+#else
+bool softfloat_le128( uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the 128-bit value formed by concatenating `a0' and `a1' is less
+| than the 128-bit value formed by concatenating `b0' and `b1'.  Otherwise,
+| returns 0.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool
+ softfloat_lt128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+    { return ( a64 < b64 ) || ( ( a64 == b64 ) && ( a0 < b0 ) ); }
+#else
+bool softfloat_lt128( uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts the 128-bit value formed by concatenating `a0' and `a1' left by the
+| number of bits given in `count'.  Any bits shifted off are lost.  The value
+| of `count' must be less than 64.  The result is broken into two 64-bit
+| pieces which are stored at the locations pointed to by `z0Ptr' and `z1Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE struct uint128
+ softfloat_shortShift128Left( uint64_t a64, uint64_t a0, unsigned int count )
+{
+    struct uint128 z;
+    z.v64 = a64<<count | a0>>( ( - count ) & 63 );
+    z.v0 = a0<<count;
+    return z;
+}
+#else
+struct uint128 softfloat_shortShift128Left( uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts the 192-bit value formed by concatenating `a0', `a1', and `a2' left
+| by the number of bits given in `count'.  Any bits shifted off are lost.
+| The value of `count' must be less than 64.  The result is broken into three
+| 64-bit pieces which are stored at the locations pointed to by `z0Ptr',
+| `z1Ptr', and `z2Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 3 <= INLINE_LEVEL )
+INLINE struct uint192
+ softfloat_shortShift192Left(
+     uint64_t a128, uint64_t a64, uint64_t a0, unsigned int count )
+{
+    unsigned int negCount = - count;
+    struct uint192 z;
+    z.v128 = a128<<count | a64>>( negCount & 63 );
+    z.v64 = a64<<count | a0>>( negCount & 63 );
+    z.v0 = a0<<count;
+    return z;
+}
+#else
+struct uint192
+ softfloat_shortShift192Left( uint64_t, uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts `a' right by the number of bits given in `count'.  If any nonzero
+| bits are shifted off, they are ``jammed'' into the least significant bit of
+| the result by setting the least significant bit to 1.  The value of `count'
+| can be arbitrarily large; in particular, if `count' is greater than 32, the
+| result will be either 0 or 1, depending on whether `a' is zero or nonzero.
+| The result is stored in the location pointed to by `zPtr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE uint32_t softfloat_shift32RightJam( uint32_t a, unsigned int count )
+{
+    return
+        ( count < 32 )
+            ? a>>count | ( (uint32_t) ( a<<( ( - count ) & 31 ) ) != 0 )
+            : ( a != 0 );
+}
+#else
+uint32_t softfloat_shift32RightJam( uint32_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shift count is less than 32.
+*----------------------------------------------------------------------------*/
+#if defined INLINE
+INLINE uint32_t softfloat_shortShift32Right1Jam( uint32_t a )
+    { return a>>1 | ( a & 1 ); }
+#else
+uint32_t softfloat_shortShift32Right1Jam( uint32_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts `a' right by the number of bits given in `count'.  If any nonzero
+| bits are shifted off, they are ``jammed'' into the least significant bit of
+| the result by setting the least significant bit to 1.  The value of `count'
+| can be arbitrarily large; in particular, if `count' is greater than 64, the
+| result will be either 0 or 1, depending on whether `a' is zero or nonzero.
+| The result is stored in the location pointed to by `zPtr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 3 <= INLINE_LEVEL )
+INLINE uint64_t softfloat_shift64RightJam( uint64_t a, unsigned int count )
+{
+    return
+        ( count < 64 )
+            ? a>>count | ( (uint64_t) ( a<<( ( - count ) & 63 ) ) != 0 )
+            : ( a != 0 );
+}
+#else
+uint64_t softfloat_shift64RightJam( uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shift count is less than 64.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE uint64_t
+ softfloat_shortShift64RightJam( uint64_t a, unsigned int count )
+    { return a>>count | ( ( a & ( ( (uint64_t) 1<<count ) - 1 ) ) != 0 ); }
+#else
+uint64_t softfloat_shortShift64RightJam( uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts the 128-bit value formed by concatenating `a0' and `a1' right by 64
+| _plus_ the number of bits given in `count'.  The shifted result is at most
+| 64 nonzero bits; this is stored at the location pointed to by `z0Ptr'.  The
+| bits shifted off form a second 64-bit result as follows:  The _last_ bit
+| shifted off is the most-significant bit of the extra result, and the other
+| 63 bits of the extra result are all zero if and only if _all_but_the_last_
+| bits shifted off were all zero.  This extra result is stored in the location
+| pointed to by `z1Ptr'.  The value of `count' can be arbitrarily large.
+|     (This routine makes more sense if `a0' and `a1' are considered to form
+| a fixed-point value with binary point between `a0' and `a1'.  This fixed-
+| point value is shifted right by the number of bits given in `count', and
+| the integer part of the result is returned at the location pointed to by
+| `z0Ptr'.  The fractional part of the result may be slightly corrupted as
+| described above, and is returned at the location pointed to by `z1Ptr'.)
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 3 <= INLINE_LEVEL )
+INLINE struct uint64_extra
+ softfloat_shift64ExtraRightJam(
+     uint64_t a, uint64_t extra, unsigned int count )
+{
+    struct uint64_extra z;
+    if ( count < 64 ) {
+        z.v = a>>count;
+        z.extra = a<<( ( - count ) & 63 );
+    } else {
+        z.v = 0;
+        z.extra = ( count == 64 ) ? a : ( a != 0 );
+    }
+    z.extra |= ( extra != 0 );
+    return z;
+}
+#else
+struct uint64_extra
+ softfloat_shift64ExtraRightJam( uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shift count is less than 64.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE struct uint64_extra
+ softfloat_shortShift64ExtraRightJam(
+     uint64_t a, uint64_t extra, unsigned int count )
+{
+    struct uint64_extra z;
+    z.v = a>>count;
+    z.extra = a<<( ( - count ) & 63 ) | ( extra != 0 );
+    return z;
+}
+#else
+struct uint64_extra
+ softfloat_shortShift64ExtraRightJam( uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts the 128-bit value formed by concatenating `a0' and `a1' right by the
+| number of bits given in `count'.  Any bits shifted off are lost.  The value
+| of `count' can be arbitrarily large; in particular, if `count' is greater
+| than 128, the result will be 0.  The result is broken into two 64-bit pieces
+| which are stored at the locations pointed to by `z0Ptr' and `z1Ptr'.
+*----------------------------------------------------------------------------*/
+/*----------------------------------------------------------------------------
+| Shift count is less than 64.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE struct uint128
+ softfloat_shortShift128Right( uint64_t a64, uint64_t a0, unsigned int count )
+{
+    struct uint128 z;
+    z.v64 = a64>>count;
+    z.v0 = a64<<( ( - count ) & 63 ) | a0>>count;
+    return z;
+}
+#else
+struct uint128
+ softfloat_shortShift128Right( uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts the 128-bit value formed by concatenating `a0' and `a1' right by the
+| number of bits given in `count'.  If any nonzero bits are shifted off, they
+| are ``jammed'' into the least significant bit of the result by setting the
+| least significant bit to 1.  The value of `count' can be arbitrarily large;
+| in particular, if `count' is greater than 128, the result will be either
+| 0 or 1, depending on whether the concatenation of `a0' and `a1' is zero or
+| nonzero.  The result is broken into two 64-bit pieces which are stored at
+| the locations pointed to by `z0Ptr' and `z1Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 4 <= INLINE_LEVEL )
+INLINE struct uint128
+ softfloat_shift128RightJam( uint64_t a64, uint64_t a0, unsigned int count )
+{
+    unsigned int negCount;
+    struct uint128 z;
+    if ( count < 64 ) {
+        negCount = - count;
+        z.v64 = a64>>( count & 63 );
+        z.v0 =
+            a64<<( negCount & 63 ) | a0>>count
+                | ( (uint64_t) ( a0<<( negCount & 63 ) ) != 0 );
+    } else {
+        z.v64 = 0;
+        z.v0 =
+            ( count < 128 )
+                ? a64>>( count & 63 )
+                      | ( ( ( a64 & ( ( (uint64_t) 1<<( count & 63 ) ) - 1 ) )
+                                | a0 )
+                              != 0 )
+                : ( ( a64 | a0 ) != 0 );
+    }
+    return z;
+}
+#else
+struct uint128
+ softfloat_shift128RightJam( uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shifts the 192-bit value formed by concatenating `a0', `a1', and `a2' right
+| by 64 _plus_ the number of bits given in `count'.  The shifted result is
+| at most 128 nonzero bits; these are broken into two 64-bit pieces which are
+| stored at the locations pointed to by `z0Ptr' and `z1Ptr'.  The bits shifted
+| off form a third 64-bit result as follows:  The _last_ bit shifted off is
+| the most-significant bit of the extra result, and the other 63 bits of the
+| extra result are all zero if and only if _all_but_the_last_ bits shifted off
+| were all zero.  This extra result is stored in the location pointed to by
+| `z2Ptr'.  The value of `count' can be arbitrarily large.
+|     (This routine makes more sense if `a0', `a1', and `a2' are considered
+| to form a fixed-point value with binary point between `a1' and `a2'.  This
+| fixed-point value is shifted right by the number of bits given in `count',
+| and the integer part of the result is returned at the locations pointed to
+| by `z0Ptr' and `z1Ptr'.  The fractional part of the result may be slightly
+| corrupted as described above, and is returned at the location pointed to by
+| `z2Ptr'.)
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 5 <= INLINE_LEVEL )
+INLINE struct uint128_extra
+ softfloat_shift128ExtraRightJam(
+     uint64_t a64, uint64_t a0, uint64_t extra, unsigned int count )
+{
+    unsigned int negCount = - count;
+    struct uint128_extra z;
+    if ( count < 64 ) {
+        z.v64 = a64>>count;
+        z.v0 = a64<<( negCount & 63 ) | a0>>count;
+        z.extra = a0<<( negCount & 63 );
+    } else {
+        z.v64 = 0;
+        if ( count == 64 ) {
+            z.v0 = a64;
+            z.extra = a0;
+        } else {
+            extra |= a0;
+            if ( count < 128 ) {
+                z.v0 = a64>>( count & 63 );
+                z.extra = a64<<( negCount & 63 );
+            } else {
+                z.v0 = 0;
+                z.extra = ( count == 128 ) ? a64 : ( a64 != 0 );
+            }
+        }
+    }
+    z.extra |= ( extra != 0 );
+    return z;
+}
+#else
+struct uint128_extra
+ softfloat_shift128ExtraRightJam( uint64_t, uint64_t, uint64_t, unsigned int );
+#endif
+
+/*----------------------------------------------------------------------------
+| Shift count is less than 64.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 3 <= INLINE_LEVEL )
+INLINE struct uint128_extra
+ softfloat_shortShift128ExtraRightJam(
+     uint64_t a64, uint64_t a0, uint64_t extra, unsigned int count )
+{
+    unsigned int negCount = - count;
+    struct uint128_extra z;
+    z.v64 = a64>>count;
+    z.v0 = a64<<( negCount & 63 ) | a0>>count;
+    z.extra = a0<<( negCount & 63 ) | ( extra != 0 );
+    return z;
+}
+#else
+struct uint128_extra
+ softfloat_shortShift128ExtraRightJam(
+     uint64_t, uint64_t, uint64_t, unsigned int );
+#endif
+
+extern const uint8_t softfloat_countLeadingZeros8[ 256 ];
+
+/*----------------------------------------------------------------------------
+| Returns the number of leading 0 bits before the most-significant 1 bit of
+| `a'.  If `a' is zero, 32 is returned.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE int softfloat_countLeadingZeros32( uint32_t a )
+{
+    int count = 0;
+    if ( a < 0x10000 ) {
+        count = 16;
+        a <<= 16;
+    }
+    if ( a < 0x1000000 ) {
+        count += 8;
+        a <<= 8;
+    }
+    count += softfloat_countLeadingZeros8[ a>>24 ];
+    return count;
+}
+#else
+int softfloat_countLeadingZeros32( uint32_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Returns the number of leading 0 bits before the most-significant 1 bit of
+| `a'.  If `a' is zero, 64 is returned.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 4 <= INLINE_LEVEL )
+INLINE int softfloat_countLeadingZeros64( uint64_t a )
+{
+    int count = 32;
+    uint32_t a32 = a;
+    if ( UINT64_C( 0x100000000 ) <= a ) {
+        count = 0;
+        a32 = a>>32;
+    }
+    /*------------------------------------------------------------------------
+    | From here, result is current count + count leading zeros of `a32'.
+    *------------------------------------------------------------------------*/
+    if ( a32 < 0x10000 ) {
+        count += 16;
+        a32 <<= 16;
+    }
+    if ( a32 < 0x1000000 ) {
+        count += 8;
+        a32 <<= 8;
+    }
+    count += softfloat_countLeadingZeros8[ a32>>24 ];
+    return count;
+}
+#else
+int softfloat_countLeadingZeros64( uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Adds the 128-bit value formed by concatenating `a0' and `a1' to the 128-bit
+| value formed by concatenating `b0' and `b1'.  Addition is modulo 2^128, so
+| any carry out is lost.  The result is broken into two 64-bit pieces which
+| are stored at the locations pointed to by `z0Ptr' and `z1Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE struct uint128
+ softfloat_add128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+    struct uint128 z;
+    z.v0 = a0 + b0;
+    z.v64 = a64 + b64;
+    z.v64 += ( z.v0 < a0 );
+    return z;
+}
+#else
+struct uint128 softfloat_add128( uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Adds the 192-bit value formed by concatenating `a0', `a1', and `a2' to the
+| 192-bit value formed by concatenating `b0', `b1', and `b2'.  Addition is
+| modulo 2^192, so any carry out is lost.  The result is broken into three
+| 64-bit pieces which are stored at the locations pointed to by `z0Ptr',
+| `z1Ptr', and `z2Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 3 <= INLINE_LEVEL )
+INLINE struct uint192
+ softfloat_add192(
+     uint64_t a128,
+     uint64_t a64,
+     uint64_t a0,
+     uint64_t b128,
+     uint64_t b64,
+     uint64_t b0
+ )
+{
+    struct uint192 z;
+    unsigned int carry64, carry128;
+    z.v0 = a0 + b0;
+    carry64 = ( z.v0 < a0 );
+    z.v64 = a64 + b64;
+    carry128 = ( z.v64 < a64 );
+    z.v128 = a128 + b128;
+    z.v64 += carry64;
+    carry128 += ( z.v64 < carry64 );
+    z.v128 += carry128;
+    return z;
+}
+#else
+struct uint192
+ softfloat_add192(
+     uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Subtracts the 128-bit value formed by concatenating `b0' and `b1' from the
+| 128-bit value formed by concatenating `a0' and `a1'.  Subtraction is modulo
+| 2^128, so any borrow out (carry out) is lost.  The result is broken into two
+| 64-bit pieces which are stored at the locations pointed to by `z0Ptr' and
+| `z1Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 2 <= INLINE_LEVEL )
+INLINE struct uint128
+ softfloat_sub128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+    struct uint128 z;
+    z.v0 = a0 - b0;
+    z.v64 = a64 - b64;
+    z.v64 -= ( a0 < b0 );
+    return z;
+}
+#else
+struct uint128 softfloat_sub128( uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Subtracts the 192-bit value formed by concatenating `b0', `b1', and `b2'
+| from the 192-bit value formed by concatenating `a0', `a1', and `a2'.
+| Subtraction is modulo 2^192, so any borrow out (carry out) is lost.  The
+| result is broken into three 64-bit pieces which are stored at the locations
+| pointed to by `z0Ptr', `z1Ptr', and `z2Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 3 <= INLINE_LEVEL )
+INLINE struct uint192
+ softfloat_sub192(
+     uint64_t a128,
+     uint64_t a64,
+     uint64_t a0,
+     uint64_t b128,
+     uint64_t b64,
+     uint64_t b0
+ )
+{
+    struct uint192 z;
+    unsigned int borrow64, borrow128;
+    z.v0 = a0 - b0;
+    borrow64 = ( a0 < b0 );
+    z.v64 = a64 - b64;
+    borrow128 = ( a64 < b64 );
+    z.v128 = a128 - b128;
+    borrow128 += ( z.v64 < borrow64 );
+    z.v64 -= borrow64;
+    z.v128 -= borrow128;
+    return z;
+}
+#else
+struct uint192
+ softfloat_sub192(
+     uint64_t, uint64_t, uint64_t, uint64_t, uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Multiplies `a' by `b' to obtain a 128-bit product.  The product is broken
+| into two 64-bit pieces which are stored at the locations pointed to by
+| `z0Ptr' and `z1Ptr'.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 4 <= INLINE_LEVEL )
+INLINE struct uint128 softfloat_mul64To128( uint64_t a, uint64_t b )
+{
+    uint32_t a32 = a>>32;
+    uint32_t a0 = a;
+    uint32_t b32 = b>>32;
+    uint32_t b0 = b;
+    struct uint128 z;
+    uint64_t mid1, mid2, mid;
+    z.v0 = (uint64_t) a0 * b0;
+    mid1 = (uint64_t) a32 * b0;
+    mid2 = (uint64_t) a0 * b32;
+    z.v64 = (uint64_t) a32 * b32;
+    mid = mid1 + mid2;
+    z.v64 += ( (uint64_t) ( mid < mid1 ) )<<32 | mid>>32;
+    mid <<= 32;
+    z.v0 += mid;
+    z.v64 += ( z.v0 < mid );
+    return z;
+}
+#else
+struct uint128 softfloat_mul64To128( uint64_t, uint64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+| Multiplies the 128-bit value formed by concatenating `a0' and `a1' by
+| `b' to obtain a 192-bit product.  The product is broken into three 64-bit
+| pieces which are stored at the locations pointed to by `z0Ptr', `z1Ptr', and
+| `z2Ptr'.
+*----------------------------------------------------------------------------*/
+struct uint192 softfloat_mul128By64To192( uint64_t, uint64_t, uint64_t );
+/*----------------------------------------------------------------------------
+| Multiplies the 128-bit value formed by concatenating `a0' and `a1' to the
+| 128-bit value formed by concatenating `b0' and `b1' to obtain a 256-bit
+| product.  The product is broken into four 64-bit pieces which are stored at
+| the locations pointed to by `z0Ptr', `z1Ptr', `z2Ptr', and `z3Ptr'.
+*----------------------------------------------------------------------------*/
+struct uint256 softfloat_mul128To256( uint64_t, uint64_t, uint64_t, uint64_t );
+
+/*----------------------------------------------------------------------------
+| Returns an approximation to the 64-bit integer quotient obtained by dividing
+| `b' into the 128-bit value formed by concatenating `a0' and `a1'.  The
+| divisor `b' must be at least 2^63.  If q is the exact quotient truncated
+| toward zero, the approximation returned lies between q and q + 2 inclusive.
+| If the exact quotient q is larger than 64 bits, the maximum positive 64-bit
+| unsigned integer is returned.
+*----------------------------------------------------------------------------*/
+uint64_t softfloat_estimateDiv128To64( uint64_t, uint64_t, uint64_t );
+
+/*----------------------------------------------------------------------------
+| Returns an approximation to the square root of the 32-bit significand given
+| by `a'.  Considered as an integer, `a' must be at least 2^31.  If bit 0 of
+| `aExp' (the least significant bit) is 1, the integer returned approximates
+| 2^31*sqrt(`a'/2^31), where `a' is considered an integer.  If bit 0 of `aExp'
+| is 0, the integer returned approximates 2^31*sqrt(`a'/2^30).  In either
+| case, the approximation returned lies strictly within +/-2 of the exact
+| value.
+*----------------------------------------------------------------------------*/
+uint32_t softfloat_estimateSqrt32( unsigned int, uint32_t );
+
diff --git a/target-riscv/fpu-custom-riscv/s_add128.c b/target-riscv/fpu-custom-riscv/s_add128.c
new file mode 100644
index 0000000..aa5ba5e
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_add128.c
@@ -0,0 +1,17 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128
+ softfloat_add128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+    struct uint128 z;
+
+    z.v0 = a0 + b0;
+    z.v64 = a64 + b64;
+    z.v64 += ( z.v0 < a0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_add192.c b/target-riscv/fpu-custom-riscv/s_add192.c
new file mode 100644
index 0000000..8ef414a
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_add192.c
@@ -0,0 +1,30 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint192
+ softfloat_add192(
+     uint64_t a128,
+     uint64_t a64,
+     uint64_t a0,
+     uint64_t b128,
+     uint64_t b64,
+     uint64_t b0
+ )
+{
+    struct uint192 z;
+    unsigned int carry64, carry128;
+
+    z.v0 = a0 + b0;
+    carry64 = ( z.v0 < a0 );
+    z.v64 = a64 + b64;
+    carry128 = ( z.v64 < a64 );
+    z.v128 = a128 + b128;
+    z.v64 += carry64;
+    carry128 += ( z.v64 < carry64 );
+    z.v128 += carry128;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_addMagsF32.c b/target-riscv/fpu-custom-riscv/s_addMagsF32.c
new file mode 100644
index 0000000..696f216
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_addMagsF32.c
@@ -0,0 +1,75 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+
+float32_t
+ softfloat_addMagsF32( uint_fast32_t uiA, uint_fast32_t uiB, bool signZ )
+{
+    int_fast16_t expA;
+    uint_fast32_t sigA;
+    int_fast16_t expB;
+    uint_fast32_t sigB;
+    int_fast16_t expDiff;
+    uint_fast32_t uiZ;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ;
+    union ui32_f32 uZ;
+
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    expB = expF32UI( uiB );
+    sigB = fracF32UI( uiB );
+    expDiff = expA - expB;
+    sigA <<= 6;
+    sigB <<= 6;
+    if ( ! expDiff ) {
+        if ( expA == 0xFF ) {
+            if ( sigA | sigB ) goto propagateNaN;
+            uiZ = uiA;
+            goto uiZ;
+        }
+        if ( ! expA ) {
+            uiZ = packToF32UI( signZ, 0, ( uiA + uiB ) & 0x7FFFFFFF );
+            goto uiZ;
+        }
+        expZ = expA;
+        sigZ = 0x40000000 + sigA + sigB;
+    } else {
+        if ( expDiff < 0 ) {
+            if ( expB == 0xFF ) {
+                if ( sigB ) goto propagateNaN;
+                uiZ = packToF32UI( signZ, 0xFF, 0 );
+                goto uiZ;
+            }
+            expZ = expB;
+            sigA += expA ? 0x20000000 : sigA;
+            sigA = softfloat_shift32RightJam( sigA, - expDiff );
+        } else {
+            if ( expA == 0xFF ) {
+                if ( sigA ) goto propagateNaN;
+                uiZ = uiA;
+                goto uiZ;
+            }
+            expZ = expA;
+            sigB += expB ? 0x20000000 : sigB;
+            sigB = softfloat_shift32RightJam( sigB, expDiff );
+        }
+        sigZ = 0x20000000 + sigA + sigB;
+        if ( sigZ < 0x40000000 ) {
+            --expZ;
+            sigZ <<= 1;
+        }
+    }
+    return softfloat_roundPackToF32( signZ, expZ, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF32UI( uiA, uiB );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_addMagsF64.c b/target-riscv/fpu-custom-riscv/s_addMagsF64.c
new file mode 100644
index 0000000..a81c3e4
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_addMagsF64.c
@@ -0,0 +1,77 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+
+float64_t
+ softfloat_addMagsF64( uint_fast64_t uiA, uint_fast64_t uiB, bool signZ )
+{
+    int_fast16_t expA;
+    uint_fast64_t sigA;
+    int_fast16_t expB;
+    uint_fast64_t sigB;
+    int_fast16_t expDiff;
+    uint_fast64_t uiZ;
+    int_fast16_t expZ;
+    uint_fast64_t sigZ;
+    union ui64_f64 uZ;
+
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    expB = expF64UI( uiB );
+    sigB = fracF64UI( uiB );
+    expDiff = expA - expB;
+    sigA <<= 9;
+    sigB <<= 9;
+    if ( ! expDiff ) {
+        if ( expA == 0x7FF ) {
+            if ( sigA | sigB ) goto propagateNaN;
+            uiZ = uiA;
+            goto uiZ;
+        }
+        if ( ! expA ) {
+            uiZ =
+                packToF64UI(
+                    signZ, 0, ( uiA + uiB ) & UINT64_C( 0x7FFFFFFFFFFFFFFF ) );
+            goto uiZ;
+        }
+        expZ = expA;
+        sigZ = UINT64_C( 0x4000000000000000 ) + sigA + sigB;
+    } else {
+        if ( expDiff < 0 ) {
+            if ( expB == 0x7FF ) {
+                if ( sigB ) goto propagateNaN;
+                uiZ = packToF64UI( signZ, 0x7FF, 0 );
+                goto uiZ;
+            }
+            expZ = expB;
+            sigA += expA ? UINT64_C( 0x2000000000000000 ) : sigA;
+            sigA = softfloat_shift64RightJam( sigA, - expDiff );
+        } else {
+            if ( expA == 0x7FF ) {
+                if ( sigA ) goto propagateNaN;
+                uiZ = uiA;
+                goto uiZ;
+            }
+            expZ = expA;
+            sigB += expB ? UINT64_C( 0x2000000000000000 ) : sigB;
+            sigB = softfloat_shift64RightJam( sigB, expDiff );
+        }
+        sigZ = UINT64_C( 0x2000000000000000 ) + sigA + sigB;
+        if ( sigZ < UINT64_C( 0x4000000000000000 ) ) {
+            --expZ;
+            sigZ <<= 1;
+        }
+    }
+    return softfloat_roundPackToF64( signZ, expZ, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF64UI( uiA, uiB );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_commonNaNToF32UI.c b/target-riscv/fpu-custom-riscv/s_commonNaNToF32UI.c
new file mode 100644
index 0000000..140a8e0
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_commonNaNToF32UI.c
@@ -0,0 +1,17 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the single-
+| precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+uint_fast32_t softfloat_commonNaNToF32UI( struct commonNaN a )
+{
+
+    return (uint_fast32_t) a.sign<<31 | 0x7FFFFFFF;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_commonNaNToF64UI.c b/target-riscv/fpu-custom-riscv/s_commonNaNToF64UI.c
new file mode 100644
index 0000000..da36c04
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_commonNaNToF64UI.c
@@ -0,0 +1,18 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the canonical NaN `a' to the double-
+| precision floating-point format.
+*----------------------------------------------------------------------------*/
+
+uint_fast64_t softfloat_commonNaNToF64UI( struct commonNaN a )
+{
+
+    return
+        (uint_fast64_t) a.sign<<63 | UINT64_C( 0x7FFFFFFFFFFFFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_countLeadingZeros32.c b/target-riscv/fpu-custom-riscv/s_countLeadingZeros32.c
new file mode 100644
index 0000000..0bd17e1
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_countLeadingZeros32.c
@@ -0,0 +1,22 @@
+
+#include <stdint.h>
+#include "primitives.h"
+
+int softfloat_countLeadingZeros32( uint32_t a )
+{
+    int count;
+
+    count = 0;
+    if ( a < 0x10000 ) {
+        count = 16;
+        a <<= 16;
+    }
+    if ( a < 0x1000000 ) {
+        count += 8;
+        a <<= 8;
+    }
+    count += softfloat_countLeadingZeros8[ a>>24 ];
+    return count;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_countLeadingZeros64.c b/target-riscv/fpu-custom-riscv/s_countLeadingZeros64.c
new file mode 100644
index 0000000..79f4280
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_countLeadingZeros64.c
@@ -0,0 +1,32 @@
+
+#include <stdint.h>
+#include "primitives.h"
+#include "platform.h"
+
+int softfloat_countLeadingZeros64( uint64_t a )
+{
+    int count;
+    uint32_t a32;
+
+    count = 32;
+    a32 = a;
+    if ( UINT64_C( 0x100000000 ) <= a ) {
+        count = 0;
+        a32 = a>>32;
+    }
+    /*------------------------------------------------------------------------
+    | From here, result is current count + count leading zeros of `a32'.
+    *------------------------------------------------------------------------*/
+    if ( a32 < 0x10000 ) {
+        count += 16;
+        a32 <<= 16;
+    }
+    if ( a32 < 0x1000000 ) {
+        count += 8;
+        a32 <<= 8;
+    }
+    count += softfloat_countLeadingZeros8[ a32>>24 ];
+    return count;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_countLeadingZeros8.c b/target-riscv/fpu-custom-riscv/s_countLeadingZeros8.c
new file mode 100644
index 0000000..4eca7e9
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_countLeadingZeros8.c
@@ -0,0 +1,24 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+const uint8_t softfloat_countLeadingZeros8[ 256 ] = {
+    8, 7, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,
+    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
+    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
+    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
+};
+
diff --git a/target-riscv/fpu-custom-riscv/s_eq128.c b/target-riscv/fpu-custom-riscv/s_eq128.c
new file mode 100644
index 0000000..7261dc4
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_eq128.c
@@ -0,0 +1,13 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+bool softfloat_eq128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+
+    return ( a64 == b64 ) && ( a0 == b0 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_estimateDiv128To64.c b/target-riscv/fpu-custom-riscv/s_estimateDiv128To64.c
new file mode 100644
index 0000000..f8610a2
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_estimateDiv128To64.c
@@ -0,0 +1,28 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+uint64_t softfloat_estimateDiv128To64( uint64_t a64, uint64_t a0, uint64_t b )
+{
+    uint32_t b32;
+    uint64_t z;
+    struct uint128 term, rem;
+    uint64_t rem32;
+
+    if ( b <= a64 ) return UINT64_C( 0xFFFFFFFFFFFFFFFF );
+    b32 = b>>32;
+    z = ( (uint64_t) b32<<32 <= a64 ) ? UINT64_C( 0xFFFFFFFF00000000 )
+            : ( a64 / b32 )<<32;
+    term = softfloat_mul64To128( b, z );
+    rem = softfloat_sub128( a64, a0, term.v64, term.v0 );
+    while ( UINT64_C( 0x8000000000000000 ) <= rem.v64 ) {
+        z -= UINT64_C( 0x100000000 );
+        rem = softfloat_add128( rem.v64, rem.v0, b32, (uint64_t) ( b<<32 ) );
+    }
+    rem32 = ( rem.v64<<32 ) | ( rem.v0>>32 );
+    z |= ( (uint64_t) b32<<32 <= rem32 ) ? 0xFFFFFFFF : rem32 / b32;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_estimateSqrt32.c b/target-riscv/fpu-custom-riscv/s_estimateSqrt32.c
new file mode 100644
index 0000000..e22a9dc
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_estimateSqrt32.c
@@ -0,0 +1,37 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+uint32_t softfloat_estimateSqrt32( unsigned int expA, uint32_t a )
+{
+    static const uint16_t sqrtOddAdjustments[] = {
+        0x0004, 0x0022, 0x005D, 0x00B1, 0x011D, 0x019F, 0x0236, 0x02E0,
+        0x039C, 0x0468, 0x0545, 0x0631, 0x072B, 0x0832, 0x0946, 0x0A67
+    };
+    static const uint16_t sqrtEvenAdjustments[] = {
+        0x0A2D, 0x08AF, 0x075A, 0x0629, 0x051A, 0x0429, 0x0356, 0x029E,
+        0x0200, 0x0179, 0x0109, 0x00AF, 0x0068, 0x0034, 0x0012, 0x0002
+    };
+    int index;
+    uint32_t z;
+    union { uint32_t ui; int32_t i; } u32;
+
+    index = ( a>>27 ) & 15;
+    if ( expA & 1 ) {
+        z = 0x4000 + ( a>>17 ) - sqrtOddAdjustments[ index ];
+        z = ( ( a / z )<<14 ) + ( z<<15 );
+        a >>= 1;
+    } else {
+        z = 0x8000 + ( a>>17 ) - sqrtEvenAdjustments[ index ];
+        z = a / z + z;
+        z = ( 0x20000 <= z ) ? 0xFFFF8000 : z<<15;
+        if ( z <= a ) {
+            u32.ui = a;
+            return u32.i>>1;
+        }
+    }
+    return (uint32_t) ( ( (uint64_t) a<<31 ) / z ) + ( z>>1 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_f32UIToCommonNaN.c b/target-riscv/fpu-custom-riscv/s_f32UIToCommonNaN.c
new file mode 100644
index 0000000..bde1a19
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_f32UIToCommonNaN.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the single-precision floating-point NaN
+| `a' to the canonical NaN format.  If `a' is a signaling NaN, the invalid
+| exception is raised.
+*----------------------------------------------------------------------------*/
+struct commonNaN softfloat_f32UIToCommonNaN( uint_fast32_t uiA )
+{
+    struct commonNaN z;
+
+    if ( softfloat_isSigNaNF32UI( uiA ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    z.sign = uiA>>31;
+    z.v64 = (uint_fast64_t) 0x7FFFF <<41;
+    z.v0 = 0;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_f64UIToCommonNaN.c b/target-riscv/fpu-custom-riscv/s_f64UIToCommonNaN.c
new file mode 100644
index 0000000..4062dfd
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_f64UIToCommonNaN.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Returns the result of converting the double-precision floating-point NaN
+| `a' to the canonical NaN format.  If `a' is a signaling NaN, the invalid
+| exception is raised.
+*----------------------------------------------------------------------------*/
+struct commonNaN softfloat_f64UIToCommonNaN( uint_fast64_t uiA )
+{
+    struct commonNaN z;
+
+    if ( softfloat_isSigNaNF64UI( uiA ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    z.sign = uiA>>63;
+    z.v64 = (uint_fast64_t) 0xFFFFFFFFFFFFF <<12;
+    z.v0 = 0;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_isSigNaNF32UI.c b/target-riscv/fpu-custom-riscv/s_isSigNaNF32UI.c
new file mode 100644
index 0000000..0a9c33f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_isSigNaNF32UI.c
@@ -0,0 +1,13 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+bool softfloat_isSigNaNF32UI( uint_fast32_t ui )
+{
+
+    return ( ( ui>>22 & 0x1FF ) == 0x1FE ) && ( ui & 0x003FFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_isSigNaNF64UI.c b/target-riscv/fpu-custom-riscv/s_isSigNaNF64UI.c
new file mode 100644
index 0000000..d255213
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_isSigNaNF64UI.c
@@ -0,0 +1,15 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "specialize.h"
+
+bool softfloat_isSigNaNF64UI( uint_fast64_t ui )
+{
+
+    return
+        ( ( ui>>51 & 0xFFF ) == 0xFFE )
+            && ( ui & UINT64_C( 0x0007FFFFFFFFFFFF ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_le128.c b/target-riscv/fpu-custom-riscv/s_le128.c
new file mode 100644
index 0000000..f57de60
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_le128.c
@@ -0,0 +1,13 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+bool softfloat_le128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+
+    return ( a64 < b64 ) || ( ( a64 == b64 ) && ( a0 <= b0 ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_lt128.c b/target-riscv/fpu-custom-riscv/s_lt128.c
new file mode 100644
index 0000000..552d5ad
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_lt128.c
@@ -0,0 +1,13 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+bool softfloat_lt128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+
+    return ( a64 < b64 ) || ( ( a64 == b64 ) && ( a0 < b0 ) );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_mul128By64To192.c b/target-riscv/fpu-custom-riscv/s_mul128By64To192.c
new file mode 100644
index 0000000..07692d3
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_mul128By64To192.c
@@ -0,0 +1,20 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint192
+ softfloat_mul128By64To192( uint64_t a64, uint64_t a0, uint64_t b )
+{
+    struct uint128 p0, p64;
+    struct uint192 z;
+
+    p0 = softfloat_mul64To128( a0, b );
+    z.v0 = p0.v0;
+    p64 = softfloat_mul64To128( a64, b );
+    z.v64 = p64.v0 + p0.v64;
+    z.v128 = p64.v64 + ( z.v64 < p64.v0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_mul128To256.c b/target-riscv/fpu-custom-riscv/s_mul128To256.c
new file mode 100644
index 0000000..2e18e70
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_mul128To256.c
@@ -0,0 +1,28 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint256
+ softfloat_mul128To256( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+    struct uint128 p0, p64, p128;
+    struct uint256 z;
+
+    p0 = softfloat_mul64To128( a0, b0 );
+    z.v0 = p0.v0;
+    p64 = softfloat_mul64To128( a64, b0 );
+    z.v64 = p64.v0 + p0.v64;
+    z.v128 = p64.v64 + ( z.v64 < p64.v0 );
+    p128 = softfloat_mul64To128( a64, b64 );
+    z.v128 += p128.v0;
+    z.v192 = p128.v64 + ( z.v128 < p128.v0 );
+    p64 = softfloat_mul64To128( a0, b64 );
+    z.v64 += p64.v0;
+    p64.v64 += ( z.v64 < p64.v0 );
+    z.v128 += p64.v64;
+    z.v192 += ( z.v128 < p64.v64 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_mul64To128.c b/target-riscv/fpu-custom-riscv/s_mul64To128.c
new file mode 100644
index 0000000..c0224de
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_mul64To128.c
@@ -0,0 +1,28 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128 softfloat_mul64To128( uint64_t a, uint64_t b )
+{
+    uint32_t a32, a0, b32, b0;
+    struct uint128 z;
+    uint64_t mid1, mid2, mid;
+
+    a32 = a>>32;
+    a0 = a;
+    b32 = b>>32;
+    b0 = b;
+    z.v0 = (uint64_t) a0 * b0;
+    mid1 = (uint64_t) a32 * b0;
+    mid2 = (uint64_t) a0 * b32;
+    z.v64 = (uint64_t) a32 * b32;
+    mid = mid1 + mid2;
+    z.v64 += ( (uint64_t) ( mid < mid1 ) )<<32 | mid>>32;
+    mid <<= 32;
+    z.v0 += mid;
+    z.v64 += ( z.v0 < mid );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_mulAddF32.c b/target-riscv/fpu-custom-riscv/s_mulAddF32.c
new file mode 100644
index 0000000..444000d
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_mulAddF32.c
@@ -0,0 +1,173 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t
+ softfloat_mulAddF32(
+     int op, uint_fast32_t uiA, uint_fast32_t uiB, uint_fast32_t uiC )
+{
+    bool signA;
+    int_fast16_t expA;
+    uint_fast32_t sigA;
+    bool signB;
+    int_fast16_t expB;
+    uint_fast32_t sigB;
+    bool signC;
+    int_fast16_t expC;
+    uint_fast32_t sigC;
+    bool signProd;
+    uint_fast32_t magBits, uiZ;
+    struct exp16_sig32 normExpSig;
+    int_fast16_t expProd;
+    uint_fast64_t sigProd;
+    bool signZ;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ;
+    int_fast16_t expDiff;
+    uint_fast64_t sigZ64, sigC64;
+    int shiftCount;
+    union ui32_f32 uZ;
+
+    uiZ = 0; // stop gcc complaining
+
+    signA = signF32UI( uiA );
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    signB = signF32UI( uiB );
+    expB = expF32UI( uiB );
+    sigB = fracF32UI( uiB );
+    signC = signF32UI( uiC ) ^ ( op == softfloat_mulAdd_subC );
+    expC = expF32UI( uiC );
+    sigC = fracF32UI( uiC );
+    signProd = signA ^ signB ^ ( op == softfloat_mulAdd_subProd );
+    if ( expA == 0xFF ) {
+        if ( sigA || ( ( expB == 0xFF ) && sigB ) ) goto propagateNaN_ABC;
+        magBits = expB | sigB;
+        goto infProdArg;
+    }
+    if ( expB == 0xFF ) {
+        if ( sigB ) goto propagateNaN_ABC;
+        magBits = expA | sigA;
+        goto infProdArg;
+    }
+    if ( expC == 0xFF ) {
+        if ( sigC ) {
+            uiZ = 0;
+            goto propagateNaN_ZC;
+        }
+        uiZ = uiC;
+        goto uiZ;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) goto zeroProd;
+        normExpSig = softfloat_normSubnormalF32Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) goto zeroProd;
+        normExpSig = softfloat_normSubnormalF32Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    expProd = expA + expB - 0x7E;
+    sigA = ( sigA | 0x00800000 )<<7;
+    sigB = ( sigB | 0x00800000 )<<7;
+    sigProd = (uint_fast64_t) sigA * sigB;
+    if ( sigProd < UINT64_C( 0x2000000000000000 ) ) {
+        --expProd;
+        sigProd <<= 1;
+    }
+    signZ = signProd;
+    if ( ! expC ) {
+        if ( ! sigC ) {
+            expZ = expProd - 1;
+            sigZ = softfloat_shortShift64RightJam( sigProd, 31 );
+            goto roundPack;
+        }
+        normExpSig = softfloat_normSubnormalF32Sig( sigC );
+        expC = normExpSig.exp;
+        sigC = normExpSig.sig;
+    }
+    sigC = ( sigC | 0x00800000 )<<6;
+    expDiff = expProd - expC;
+    if ( signProd == signC ) {
+        if ( expDiff <= 0 ) {
+            expZ = expC;
+            sigZ = sigC + softfloat_shift64RightJam( sigProd, 32 - expDiff );
+        } else {
+            expZ = expProd;
+            sigZ64 =
+                sigProd
+                    + softfloat_shift64RightJam(
+                          (uint_fast64_t) sigC<<32, expDiff );
+            sigZ = softfloat_shortShift64RightJam( sigZ64, 32 );
+        }
+        if ( sigZ < 0x40000000 ) {
+            --expZ;
+            sigZ <<= 1;
+        }
+    } else {
+/*** OPTIMIZE BETTER? ***/
+        sigC64 = (uint_fast64_t) sigC<<32;
+        if ( expDiff < 0 ) {
+            signZ = signC;
+            expZ = expC;
+            sigZ64 = sigC64 - softfloat_shift64RightJam( sigProd, - expDiff );
+        } else if ( ! expDiff ) {
+            expZ = expProd;
+            sigZ64 = sigProd - sigC64;
+            if ( ! sigZ64 ) goto completeCancellation;
+            if ( sigZ64 & UINT64_C( 0x8000000000000000 ) ) {
+                signZ ^= 1;
+                sigZ64 = - sigZ64;
+            }
+        } else {
+            expZ = expProd;
+            sigZ64 = sigProd - softfloat_shift64RightJam( sigC64, expDiff );
+        }
+        shiftCount = softfloat_countLeadingZeros64( sigZ64 ) - 1;
+        expZ -= shiftCount;
+        shiftCount -= 32;
+        if ( shiftCount < 0 ) {
+            sigZ = softfloat_shortShift64RightJam( sigZ64, - shiftCount );
+        } else {
+            sigZ = (uint_fast32_t) sigZ64<<shiftCount;
+        }
+    }
+ roundPack:
+    return softfloat_roundPackToF32( signZ, expZ, sigZ );
+ propagateNaN_ABC:
+    uiZ = softfloat_propagateNaNF32UI( uiA, uiB );
+    goto propagateNaN_ZC;
+ infProdArg:
+    if ( magBits ) {
+        uiZ = packToF32UI( signProd, 0xFF, 0 );
+        if ( expC != 0xFF ) goto uiZ;
+        if ( sigC ) goto propagateNaN_ZC;
+        if ( signProd == signC ) goto uiZ;
+    }
+// invalid:
+//    softfloat_raiseFlags( softfloat_flag_invalid );
+//    uiZ = defaultNaNF32UI;
+ propagateNaN_ZC:
+    uiZ = softfloat_propagateNaNF32UI( uiZ, uiC );
+    goto uiZ;
+ zeroProd:
+    uiZ = uiC;
+    if ( ! ( expC | sigC ) && ( signProd != signC ) ) {
+ completeCancellation:
+        uiZ =
+            packToF32UI( softfloat_roundingMode == softfloat_round_min, 0, 0 );
+    }
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_mulAddF64.c b/target-riscv/fpu-custom-riscv/s_mulAddF64.c
new file mode 100644
index 0000000..965b907
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_mulAddF64.c
@@ -0,0 +1,190 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t
+ softfloat_mulAddF64(
+     int op, uint_fast64_t uiA, uint_fast64_t uiB, uint_fast64_t uiC )
+{
+    bool signA;
+    int_fast16_t expA;
+    uint_fast64_t sigA;
+    bool signB;
+    int_fast16_t expB;
+    uint_fast64_t sigB;
+    bool signC;
+    int_fast16_t expC;
+    uint_fast64_t sigC;
+    bool signProd;
+    uint_fast64_t magBits, uiZ;
+    struct exp16_sig64 normExpSig;
+    int_fast16_t expProd;
+    struct uint128 sigProd;
+    bool signZ;
+    int_fast16_t expZ;
+    uint_fast64_t sigZ;
+    int_fast16_t expDiff;
+    struct uint128 sigC128, sigZ128;
+    int shiftCount;
+    union ui64_f64 uZ;
+
+    uiZ = 0; // stop gcc complaining
+
+    signA = signF64UI( uiA );
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    signB = signF64UI( uiB );
+    expB = expF64UI( uiB );
+    sigB = fracF64UI( uiB );
+    signC = signF64UI( uiC ) ^ ( op == softfloat_mulAdd_subC );
+    expC = expF64UI( uiC );
+    sigC = fracF64UI( uiC );
+    signProd = signA ^ signB ^ ( op == softfloat_mulAdd_subProd );
+    if ( expA == 0x7FF ) {
+        if ( sigA || ( ( expB == 0x7FF ) && sigB ) ) goto propagateNaN_ABC;
+        magBits = expB | sigB;
+        goto infProdArg;
+    }
+    if ( expB == 0x7FF ) {
+        if ( sigB ) goto propagateNaN_ABC;
+        magBits = expA | sigA;
+        goto infProdArg;
+    }
+    if ( expC == 0x7FF ) {
+        if ( sigC ) {
+            uiZ = 0;
+            goto propagateNaN_ZC;
+        }
+        uiZ = uiC;
+        goto uiZ;
+    }
+    if ( ! expA ) {
+        if ( ! sigA ) goto zeroProd;
+        normExpSig = softfloat_normSubnormalF64Sig( sigA );
+        expA = normExpSig.exp;
+        sigA = normExpSig.sig;
+    }
+    if ( ! expB ) {
+        if ( ! sigB ) goto zeroProd;
+        normExpSig = softfloat_normSubnormalF64Sig( sigB );
+        expB = normExpSig.exp;
+        sigB = normExpSig.sig;
+    }
+    expProd = expA + expB - 0x3FE;
+    sigA = ( sigA | UINT64_C( 0x0010000000000000 ) )<<10;
+    sigB = ( sigB | UINT64_C( 0x0010000000000000 ) )<<10;
+    sigProd = softfloat_mul64To128( sigA, sigB );
+    if ( sigProd.v64 < UINT64_C( 0x2000000000000000 ) ) {
+        --expProd;
+        sigProd = softfloat_shortShift128Left( sigProd.v64, sigProd.v0, 1 );
+    }
+    signZ = signProd;
+    if ( ! expC ) {
+        if ( ! sigC ) {
+            expZ = expProd - 1;
+            sigZ = sigProd.v64<<1 | ( sigProd.v0 != 0 );
+            goto roundPack;
+        }
+        normExpSig = softfloat_normSubnormalF64Sig( sigC );
+        expC = normExpSig.exp;
+        sigC = normExpSig.sig;
+    }
+    sigC = ( sigC | UINT64_C( 0x0010000000000000 ) )<<9;
+    expDiff = expProd - expC;
+    if ( signProd == signC ) {
+        if ( expDiff <= 0 ) {
+            expZ = expC;
+            if ( expDiff ) {
+                sigProd.v64 =
+                    softfloat_shift64RightJam( sigProd.v64, - expDiff );
+            }
+            sigZ = ( sigC + sigProd.v64 ) | ( sigProd.v0 != 0 );
+        } else {
+            expZ = expProd;
+            sigC128 = softfloat_shift128RightJam( sigC, 0, expDiff );
+            sigZ128 =
+                softfloat_add128(
+                    sigProd.v64, sigProd.v0, sigC128.v64, sigC128.v0 );
+            sigZ = sigZ128.v64 | ( sigZ128.v0 != 0 );
+        }
+        if ( sigZ < UINT64_C( 0x4000000000000000 ) ) {
+            --expZ;
+            sigZ <<= 1;
+        }
+    } else {
+/*** OPTIMIZE BETTER? ***/
+        if ( expDiff < 0 ) {
+            signZ = signC;
+            expZ = expC;
+            sigProd =
+                softfloat_shift128RightJam(
+                    sigProd.v64, sigProd.v0, - expDiff );
+            sigZ128 = softfloat_sub128( sigC, 0, sigProd.v64, sigProd.v0 );
+        } else if ( ! expDiff ) {
+            expZ = expProd;
+            sigZ128 = softfloat_sub128( sigProd.v64, sigProd.v0, sigC, 0 );
+            if ( ! ( sigZ128.v64 | sigZ128.v0 ) ) goto completeCancellation;
+            if ( sigZ128.v64 & UINT64_C( 0x8000000000000000 ) ) {
+                signZ ^= 1;
+                sigZ128 = softfloat_sub128( 0, 0, sigZ128.v64, sigZ128.v0 );
+            }
+        } else {
+            expZ = expProd;
+            sigC128 = softfloat_shift128RightJam( sigC, 0, expDiff );
+            sigZ128 =
+                softfloat_sub128(
+                    sigProd.v64, sigProd.v0, sigC128.v64, sigC128.v0 );
+        }
+        if ( ! sigZ128.v64 ) {
+            expZ -= 64;
+            sigZ128.v64 = sigZ128.v0;
+            sigZ128.v0 = 0;
+        }
+        shiftCount = softfloat_countLeadingZeros64( sigZ128.v64 ) - 1;
+        expZ -= shiftCount;
+        if ( shiftCount < 0 ) {
+            sigZ = softfloat_shortShift64RightJam( sigZ128.v64, - shiftCount );
+        } else {
+            sigZ128 =
+                softfloat_shortShift128Left(
+                    sigZ128.v64, sigZ128.v0, shiftCount );
+            sigZ = sigZ128.v64;
+        }
+        sigZ |= ( sigZ128.v0 != 0 );
+    }
+ roundPack:
+    return softfloat_roundPackToF64( signZ, expZ, sigZ );
+ propagateNaN_ABC:
+    uiZ = softfloat_propagateNaNF64UI( uiA, uiB );
+    goto propagateNaN_ZC;
+ infProdArg:
+    if ( magBits ) {
+        uiZ = packToF64UI( signProd, 0x7FF, 0 );
+        if ( expC != 0x7FF ) goto uiZ;
+        if ( sigC ) goto propagateNaN_ZC;
+        if ( signProd == signC ) goto uiZ;
+    }
+// invalid:
+//    softfloat_raiseFlags( softfloat_flag_invalid );
+//    uiZ = defaultNaNF64UI;
+ propagateNaN_ZC:
+    uiZ = softfloat_propagateNaNF64UI( uiZ, uiC );
+    goto uiZ;
+ zeroProd:
+    uiZ = uiC;
+    if ( ! ( expC | sigC ) && ( signProd != signC ) ) {
+ completeCancellation:
+        uiZ =
+            packToF64UI( softfloat_roundingMode == softfloat_round_min, 0, 0 );
+    }
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_normRoundPackToF32.c b/target-riscv/fpu-custom-riscv/s_normRoundPackToF32.c
new file mode 100644
index 0000000..be31f3c
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_normRoundPackToF32.c
@@ -0,0 +1,24 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+
+float32_t
+ softfloat_normRoundPackToF32( bool sign, int_fast16_t exp, uint_fast32_t sig )
+{
+    int shiftCount;
+    union ui32_f32 uZ;
+
+    shiftCount = softfloat_countLeadingZeros32( sig ) - 1;
+    exp -= shiftCount;
+    if ( ( 7 <= shiftCount ) && ( (uint16_t) exp < 0xFD ) ) {
+        uZ.ui = packToF32UI( sign, sig ? exp : 0, sig<<( shiftCount - 7 ) );
+        return uZ.f;
+    } else {
+        return softfloat_roundPackToF32( sign, exp, sig<<shiftCount );
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_normRoundPackToF64.c b/target-riscv/fpu-custom-riscv/s_normRoundPackToF64.c
new file mode 100644
index 0000000..0e45af6
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_normRoundPackToF64.c
@@ -0,0 +1,24 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+
+float64_t
+ softfloat_normRoundPackToF64( bool sign, int_fast16_t exp, uint_fast64_t sig )
+{
+    int shiftCount;
+    union ui64_f64 uZ;
+
+    shiftCount = softfloat_countLeadingZeros64( sig ) - 1;
+    exp -= shiftCount;
+    if ( ( 10 <= shiftCount ) && ( (uint16_t) exp < 0x7FD ) ) {
+        uZ.ui = packToF64UI( sign, sig ? exp : 0, sig<<( shiftCount - 10 ) );
+        return uZ.f;
+    } else {
+        return softfloat_roundPackToF64( sign, exp, sig<<shiftCount );
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_normSubnormalF32Sig.c b/target-riscv/fpu-custom-riscv/s_normSubnormalF32Sig.c
new file mode 100644
index 0000000..2ad53ef
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_normSubnormalF32Sig.c
@@ -0,0 +1,18 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+
+struct exp16_sig32 softfloat_normSubnormalF32Sig( uint_fast32_t sig )
+{
+    int shiftCount;
+    struct exp16_sig32 z;
+
+    shiftCount = softfloat_countLeadingZeros32( sig ) - 8;
+    z.exp = 1 - shiftCount;
+    z.sig = sig<<shiftCount;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_normSubnormalF64Sig.c b/target-riscv/fpu-custom-riscv/s_normSubnormalF64Sig.c
new file mode 100644
index 0000000..39c2ffa
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_normSubnormalF64Sig.c
@@ -0,0 +1,18 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+
+struct exp16_sig64 softfloat_normSubnormalF64Sig( uint_fast64_t sig )
+{
+    int shiftCount;
+    struct exp16_sig64 z;
+
+    shiftCount = softfloat_countLeadingZeros64( sig ) - 11;
+    z.exp = 1 - shiftCount;
+    z.sig = sig<<shiftCount;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_propagateNaNF32UI.c b/target-riscv/fpu-custom-riscv/s_propagateNaNF32UI.c
new file mode 100644
index 0000000..e2ce34a
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_propagateNaNF32UI.c
@@ -0,0 +1,25 @@
+
+/*** UPDATE COMMENTS. ***/
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Takes two single-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+uint_fast32_t
+ softfloat_propagateNaNF32UI( uint_fast32_t uiA, uint_fast32_t uiB )
+{
+    if ( softfloat_isSigNaNF32UI( uiA ) | softfloat_isSigNaNF32UI( uiB ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    return defaultNaNF32UI;
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_propagateNaNF64UI.c b/target-riscv/fpu-custom-riscv/s_propagateNaNF64UI.c
new file mode 100644
index 0000000..871989d
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_propagateNaNF64UI.c
@@ -0,0 +1,25 @@
+
+/*** UPDATE COMMENTS. ***/
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Takes two double-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+
+uint_fast64_t
+ softfloat_propagateNaNF64UI( uint_fast64_t uiA, uint_fast64_t uiB )
+{
+    if ( softfloat_isSigNaNF64UI( uiA ) | softfloat_isSigNaNF64UI( uiB ) ) {
+        softfloat_raiseFlags( softfloat_flag_invalid );
+    }
+    return defaultNaNF64UI;
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_roundPackToF32.c b/target-riscv/fpu-custom-riscv/s_roundPackToF32.c
new file mode 100644
index 0000000..11764f1
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_roundPackToF32.c
@@ -0,0 +1,65 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t
+ softfloat_roundPackToF32( bool sign, int_fast16_t exp, uint_fast32_t sig )
+{
+    int roundingMode;
+    bool roundNearestEven;
+    int roundIncrement, roundBits;
+    bool isTiny;
+    uint_fast32_t uiZ;
+    union ui32_f32 uZ;
+
+    roundingMode = softfloat_roundingMode;
+    roundNearestEven = ( roundingMode == softfloat_round_nearest_even );
+    roundIncrement = 0x40;
+    if (
+           ! roundNearestEven
+        && ( roundingMode != softfloat_round_nearest_maxMag )
+    ) {
+        roundIncrement =
+               ( roundingMode == softfloat_round_minMag )
+            || ( roundingMode
+                     == ( sign ? softfloat_round_max : softfloat_round_min ) )
+                ? 0
+                : 0x7F;
+    }
+    roundBits = sig & 0x7F;
+    if ( 0xFD <= (uint16_t) exp ) {
+        if ( exp < 0 ) {
+            isTiny =
+                   ( softfloat_detectTininess
+                         == softfloat_tininess_beforeRounding )
+                || ( exp < -1 )
+                || ( sig + roundIncrement < 0x80000000 );
+            sig = softfloat_shift32RightJam( sig, - exp );
+            exp = 0;
+            roundBits = sig & 0x7F;
+            if ( isTiny && roundBits ) {
+                softfloat_raiseFlags( softfloat_flag_underflow );
+            }
+        } else if (
+            ( 0xFD < exp ) || ( 0x80000000 <= sig + roundIncrement )
+        ) {
+            softfloat_raiseFlags(
+                softfloat_flag_overflow | softfloat_flag_inexact );
+            uiZ = packToF32UI( sign, 0xFF, 0 ) - ! roundIncrement;
+            goto uiZ;
+        }
+    }
+    if ( roundBits ) softfloat_exceptionFlags |= softfloat_flag_inexact;
+    sig = ( sig + roundIncrement )>>7;
+    sig &= ~ ( ! ( roundBits ^ 0x40 ) & roundNearestEven );
+    uiZ = packToF32UI( sign, sig ? exp : 0, sig );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_roundPackToF64.c b/target-riscv/fpu-custom-riscv/s_roundPackToF64.c
new file mode 100644
index 0000000..fb0ef1d
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_roundPackToF64.c
@@ -0,0 +1,66 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t
+ softfloat_roundPackToF64( bool sign, int_fast16_t exp, uint_fast64_t sig )
+{
+    int roundingMode;
+    bool roundNearestEven;
+    int roundIncrement, roundBits;
+    bool isTiny;
+    uint_fast64_t uiZ;
+    union ui64_f64 uZ;
+
+    roundingMode = softfloat_roundingMode;
+    roundNearestEven = ( roundingMode == softfloat_round_nearest_even );
+    roundIncrement = 0x200;
+    if (
+           ! roundNearestEven
+        && ( roundingMode != softfloat_round_nearest_maxMag )
+    ) {
+        roundIncrement =
+               ( roundingMode == softfloat_round_minMag )
+            || ( roundingMode
+                     == ( sign ? softfloat_round_max : softfloat_round_min ) )
+                ? 0
+                : 0x3FF;
+    }
+    roundBits = sig & 0x3FF;
+    if ( 0x7FD <= (uint16_t) exp ) {
+        if ( exp < 0 ) {
+            isTiny =
+                   ( softfloat_detectTininess
+                         == softfloat_tininess_beforeRounding )
+                || ( exp < -1 )
+                || ( sig + roundIncrement < UINT64_C( 0x8000000000000000 ) );
+            sig = softfloat_shift64RightJam( sig, - exp );
+            exp = 0;
+            roundBits = sig & 0x3FF;
+            if ( isTiny && roundBits ) {
+                softfloat_raiseFlags( softfloat_flag_underflow );
+            }
+        } else if (
+            ( 0x7FD < exp )
+                || ( UINT64_C( 0x8000000000000000 ) <= sig + roundIncrement )
+        ) {
+            softfloat_raiseFlags(
+                softfloat_flag_overflow | softfloat_flag_inexact );
+            uiZ = packToF64UI( sign, 0x7FF, 0 ) - ! roundIncrement;
+            goto uiZ;
+        }
+    }
+    if ( roundBits ) softfloat_exceptionFlags |= softfloat_flag_inexact;
+    sig = ( sig + roundIncrement )>>10;
+    sig &= ~ ( ! ( roundBits ^ 0x200 ) & roundNearestEven );
+    uiZ = packToF64UI( sign, sig ? exp : 0, sig );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_roundPackToI32.c b/target-riscv/fpu-custom-riscv/s_roundPackToI32.c
new file mode 100644
index 0000000..1c91497
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_roundPackToI32.c
@@ -0,0 +1,48 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast32_t
+ softfloat_roundPackToI32(
+     bool sign, uint_fast64_t sig, int_fast8_t roundingMode, bool exact )
+{
+    bool roundNearestEven;
+    int roundIncrement, roundBits;
+    uint_fast32_t sig32;
+    union { uint32_t ui; int32_t i; } uZ;
+    int_fast32_t z;
+
+    roundNearestEven = ( roundingMode == softfloat_round_nearest_even );
+    roundIncrement = 0x40;
+    if (
+           ! roundNearestEven
+        && ( roundingMode != softfloat_round_nearest_maxMag )
+    ) {
+        roundIncrement =
+               ( roundingMode == softfloat_round_minMag )
+            || ( roundingMode
+                     == ( sign ? softfloat_round_max : softfloat_round_min ) )
+                ? 0
+                : 0x7F;
+    }
+    roundBits = sig & 0x7F;
+    sig += roundIncrement;
+    if ( sig & UINT64_C( 0xFFFFFF8000000000 ) ) goto invalid;
+    sig32 = sig>>7;
+    sig32 &= ~ ( ! ( roundBits ^ 0x40 ) & roundNearestEven );
+    uZ.ui = sign ? - sig32 : sig32;
+    z = uZ.i;
+    if ( z && ( ( z < 0 ) ^ sign ) ) goto invalid;
+    if ( exact && roundBits ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return sign ? -0x7FFFFFFF - 1 : 0x7FFFFFFF;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_roundPackToI64.c b/target-riscv/fpu-custom-riscv/s_roundPackToI64.c
new file mode 100644
index 0000000..b2f5d63
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_roundPackToI64.c
@@ -0,0 +1,52 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+int_fast64_t
+ softfloat_roundPackToI64(
+     bool sign,
+     uint_fast64_t sig64,
+     uint_fast64_t sig0,
+     int_fast8_t roundingMode,
+     bool exact
+ )
+{
+    bool roundNearestEven, increment;
+    union { uint64_t ui; int64_t i; } uZ;
+    int_fast64_t z;
+
+    roundNearestEven = ( roundingMode == softfloat_round_nearest_even );
+    increment = ( UINT64_C( 0x8000000000000000 ) <= sig0 );
+    if (
+           ! roundNearestEven
+        && ( roundingMode != softfloat_round_nearest_maxMag )
+    ) {
+        increment =
+               ( roundingMode != softfloat_round_minMag )
+            && ( roundingMode
+                     == ( sign ? softfloat_round_min : softfloat_round_max ) )
+            && sig0;
+    }
+    if ( increment ) {
+        ++sig64;
+        if ( ! sig64 ) goto invalid;
+        sig64 &=
+            ~ ( ! ( sig0 & UINT64_C( 0x7FFFFFFFFFFFFFFF ) )
+                    & roundNearestEven );
+    }
+    uZ.ui = sign ? - sig64 : sig64;
+    z = uZ.i;
+    if ( z && ( ( z < 0 ) ^ sign ) ) goto invalid;
+    if ( exact && sig0 ) softfloat_exceptionFlags |= softfloat_flag_inexact;
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return
+        sign ? - INT64_C( 0x7FFFFFFFFFFFFFFF ) - 1
+            : INT64_C( 0x7FFFFFFFFFFFFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_roundPackToUI32.c b/target-riscv/fpu-custom-riscv/s_roundPackToUI32.c
new file mode 100644
index 0000000..ab44ec7
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_roundPackToUI32.c
@@ -0,0 +1,44 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast32_t
+ softfloat_roundPackToUI32(
+     bool sign, uint_fast64_t sig, int_fast8_t roundingMode, bool exact )
+{
+    bool roundNearestEven;
+    int roundIncrement, roundBits;
+    uint_fast32_t z;
+
+    roundNearestEven = ( roundingMode == softfloat_round_nearest_even );
+    roundIncrement = 0x40;
+    if (
+           ! roundNearestEven
+        && ( roundingMode != softfloat_round_nearest_maxMag )
+    ) {
+        roundIncrement =
+               ( roundingMode == softfloat_round_minMag )
+            || ( roundingMode
+                     == ( sign ? softfloat_round_max : softfloat_round_min ) )
+                ? 0
+                : 0x7F;
+    }
+    roundBits = sig & 0x7F;
+    sig += roundIncrement;
+    if ( sig & UINT64_C( 0xFFFFFF8000000000 ) ) goto invalid;
+    z = sig>>7;
+    z &= ~ ( ! ( roundBits ^ 0x40 ) & roundNearestEven );
+    if ( sign && z ) goto invalid;
+    if ( exact && roundBits ) {
+        softfloat_exceptionFlags |= softfloat_flag_inexact;
+    }
+    return z;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return 0xFFFFFFFF;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_roundPackToUI64.c b/target-riscv/fpu-custom-riscv/s_roundPackToUI64.c
new file mode 100644
index 0000000..d42266f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_roundPackToUI64.c
@@ -0,0 +1,46 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "softfloat.h"
+
+uint_fast64_t
+ softfloat_roundPackToUI64(
+     bool sign,
+     uint_fast64_t sig64,
+     uint_fast64_t sig0,
+     int_fast8_t roundingMode,
+     bool exact
+ )
+{
+    bool roundNearestEven, increment;
+
+    roundNearestEven = ( roundingMode == softfloat_round_nearest_even );
+    increment = ( UINT64_C( 0x8000000000000000 ) <= sig0 );
+    if (
+           ! roundNearestEven
+        && ( roundingMode != softfloat_round_nearest_maxMag )
+    ) {
+        increment =
+               ( roundingMode != softfloat_round_minMag )
+            && ( roundingMode
+                     == ( sign ? softfloat_round_min : softfloat_round_max ) )
+            && sig0;
+    }
+    if ( increment ) {
+        ++sig64;
+        if ( ! sig64 ) goto invalid;
+        sig64 &=
+            ~ ( ! ( sig0 & UINT64_C( 0x7FFFFFFFFFFFFFFF ) )
+                    & roundNearestEven );
+    }
+    if ( sign && sig64 ) goto invalid;
+    if ( exact && sig0 ) softfloat_exceptionFlags |= softfloat_flag_inexact;
+    return sig64;
+ invalid:
+    softfloat_raiseFlags( softfloat_flag_invalid );
+    return UINT64_C( 0xFFFFFFFFFFFFFFFF );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shift128ExtraRightJam.c b/target-riscv/fpu-custom-riscv/s_shift128ExtraRightJam.c
new file mode 100644
index 0000000..a4a2df0
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shift128ExtraRightJam.c
@@ -0,0 +1,38 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128_extra
+ softfloat_shift128ExtraRightJam(
+     uint64_t a64, uint64_t a0, uint64_t extra, unsigned int count )
+{
+    unsigned int negCount;
+    struct uint128_extra z;
+
+    negCount = - count;
+    if ( count < 64 ) {
+        z.v64 = a64>>count;
+        z.v0 = a64<<( negCount & 63 ) | a0>>count;
+        z.extra = a0<<( negCount & 63 );
+    } else {
+        z.v64 = 0;
+        if ( count == 64 ) {
+            z.v0 = a64;
+            z.extra = a0;
+        } else {
+            extra |= a0;
+            if ( count < 128 ) {
+                z.v0 = a64>>( count & 63 );
+                z.extra = a64<<( negCount & 63 );
+            } else {
+                z.v0 = 0;
+                z.extra = ( count == 128 ) ? a64 : ( a64 != 0 );
+            }
+        }
+    }
+    z.extra |= ( extra != 0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shift128RightJam.c b/target-riscv/fpu-custom-riscv/s_shift128RightJam.c
new file mode 100644
index 0000000..5a4e188
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shift128RightJam.c
@@ -0,0 +1,31 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128
+ softfloat_shift128RightJam( uint64_t a64, uint64_t a0, unsigned int count )
+{
+    unsigned int negCount;
+    struct uint128 z;
+
+    if ( count < 64 ) {
+        negCount = - count;
+        z.v64 = a64>>( count & 63 );
+        z.v0 =
+            a64<<( negCount & 63 ) | a0>>count
+                | ( (uint64_t) ( a0<<( negCount & 63 ) ) != 0 );
+    } else {
+        z.v64 = 0;
+        z.v0 =
+            ( count < 128 )
+                ? a64>>( count & 63 )
+                      | ( ( ( a64 & ( ( (uint64_t) 1<<( count & 63 ) ) - 1 ) )
+                                | a0 )
+                              != 0 )
+                : ( ( a64 | a0 ) != 0 );
+    }
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shift32RightJam.c b/target-riscv/fpu-custom-riscv/s_shift32RightJam.c
new file mode 100644
index 0000000..b697a34
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shift32RightJam.c
@@ -0,0 +1,15 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+uint32_t softfloat_shift32RightJam( uint32_t a, unsigned int count )
+{
+
+    return
+        ( count < 32 )
+            ? a>>count | ( (uint32_t) ( a<<( ( - count ) & 31 ) ) != 0 )
+            : ( a != 0 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shift64ExtraRightJam.c b/target-riscv/fpu-custom-riscv/s_shift64ExtraRightJam.c
new file mode 100644
index 0000000..d930feb
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shift64ExtraRightJam.c
@@ -0,0 +1,23 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint64_extra
+ softfloat_shift64ExtraRightJam(
+     uint64_t a, uint64_t extra, unsigned int count )
+{
+    struct uint64_extra z;
+
+    if ( count < 64 ) {
+        z.v = a>>count;
+        z.extra = a<<( ( - count ) & 63 );
+    } else {
+        z.v = 0;
+        z.extra = ( count == 64 ) ? a : ( a != 0 );
+    }
+    z.extra |= ( extra != 0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shift64RightJam.c b/target-riscv/fpu-custom-riscv/s_shift64RightJam.c
new file mode 100644
index 0000000..ebebb61
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shift64RightJam.c
@@ -0,0 +1,15 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+uint64_t softfloat_shift64RightJam( uint64_t a, unsigned int count )
+{
+
+    return
+        ( count < 64 )
+            ? a>>count | ( (uint64_t) ( a<<( ( - count ) & 63 ) ) != 0 )
+            : ( a != 0 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift128ExtraRightJam.c b/target-riscv/fpu-custom-riscv/s_shortShift128ExtraRightJam.c
new file mode 100644
index 0000000..fa69021
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift128ExtraRightJam.c
@@ -0,0 +1,20 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128_extra
+ softfloat_shortShift128ExtraRightJam(
+     uint64_t a64, uint64_t a0, uint64_t extra, unsigned int count )
+{
+    unsigned int negCount;
+    struct uint128_extra z;
+
+    negCount = - count;
+    z.v64 = a64>>count;
+    z.v0 = a64<<( negCount & 63 ) | a0>>count;
+    z.extra = a0<<( negCount & 63 ) | ( extra != 0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift128Left.c b/target-riscv/fpu-custom-riscv/s_shortShift128Left.c
new file mode 100644
index 0000000..acd40cc
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift128Left.c
@@ -0,0 +1,16 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128
+ softfloat_shortShift128Left( uint64_t a64, uint64_t a0, unsigned int count )
+{
+    struct uint128 z;
+
+    z.v64 = a64<<count | a0>>( ( - count ) & 63 );
+    z.v0 = a0<<count;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift128Right.c b/target-riscv/fpu-custom-riscv/s_shortShift128Right.c
new file mode 100644
index 0000000..86b30ce
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift128Right.c
@@ -0,0 +1,16 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128
+ softfloat_shortShift128Right( uint64_t a64, uint64_t a0, unsigned int count )
+{
+    struct uint128 z;
+
+    z.v64 = a64>>count;
+    z.v0 = a64<<( ( - count ) & 63 ) | a0>>count;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift192Left.c b/target-riscv/fpu-custom-riscv/s_shortShift192Left.c
new file mode 100644
index 0000000..d4fc666
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift192Left.c
@@ -0,0 +1,20 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint192
+ softfloat_shortShift192Left(
+     uint64_t a128, uint64_t a64, uint64_t a0, unsigned int count )
+{
+    unsigned int negCount;
+    struct uint192 z;
+
+    negCount = - count;
+    z.v128 = a128<<count | a64>>( negCount & 63 );
+    z.v64 = a64<<count | a0>>( negCount & 63 );
+    z.v0 = a0<<count;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift32Right1Jam.c b/target-riscv/fpu-custom-riscv/s_shortShift32Right1Jam.c
new file mode 100644
index 0000000..db4c304
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift32Right1Jam.c
@@ -0,0 +1,12 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+uint32_t softfloat_shortShift32Right1Jam( uint32_t a )
+{
+
+    return a>>1 | ( a & 1 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift64ExtraRightJam.c b/target-riscv/fpu-custom-riscv/s_shortShift64ExtraRightJam.c
new file mode 100644
index 0000000..de85314
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift64ExtraRightJam.c
@@ -0,0 +1,17 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint64_extra
+ softfloat_shortShift64ExtraRightJam(
+     uint64_t a, uint64_t extra, unsigned int count )
+{
+    struct uint64_extra z;
+
+    z.v = a>>count;
+    z.extra = a<<( ( - count ) & 63 ) | ( extra != 0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_shortShift64RightJam.c b/target-riscv/fpu-custom-riscv/s_shortShift64RightJam.c
new file mode 100644
index 0000000..0da6c93
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_shortShift64RightJam.c
@@ -0,0 +1,12 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+uint64_t softfloat_shortShift64RightJam( uint64_t a, unsigned int count )
+{
+
+    return a>>count | ( ( a & ( ( (uint64_t) 1<<count ) - 1 ) ) != 0 );
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_sub128.c b/target-riscv/fpu-custom-riscv/s_sub128.c
new file mode 100644
index 0000000..153dc5f
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_sub128.c
@@ -0,0 +1,17 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint128
+ softfloat_sub128( uint64_t a64, uint64_t a0, uint64_t b64, uint64_t b0 )
+{
+    struct uint128 z;
+
+    z.v0 = a0 - b0;
+    z.v64 = a64 - b64;
+    z.v64 -= ( a0 < b0 );
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_sub192.c b/target-riscv/fpu-custom-riscv/s_sub192.c
new file mode 100644
index 0000000..d3976b6
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_sub192.c
@@ -0,0 +1,30 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+
+struct uint192
+ softfloat_sub192(
+     uint64_t a128,
+     uint64_t a64,
+     uint64_t a0,
+     uint64_t b128,
+     uint64_t b64,
+     uint64_t b0
+ )
+{
+    struct uint192 z;
+    unsigned int borrow64, borrow128;
+
+    z.v0 = a0 - b0;
+    borrow64 = ( a0 < b0 );
+    z.v64 = a64 - b64;
+    borrow128 = ( a64 < b64 );
+    z.v128 = a128 - b128;
+    borrow128 += ( z.v64 < borrow64 );
+    z.v64 -= borrow64;
+    z.v128 -= borrow128;
+    return z;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_subMagsF32.c b/target-riscv/fpu-custom-riscv/s_subMagsF32.c
new file mode 100644
index 0000000..0c83b02
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_subMagsF32.c
@@ -0,0 +1,81 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float32_t
+ softfloat_subMagsF32( uint_fast32_t uiA, uint_fast32_t uiB, bool signZ )
+{
+    int_fast16_t expA;
+    uint_fast32_t sigA;
+    int_fast16_t expB;
+    uint_fast32_t sigB;
+    int_fast16_t expDiff;
+    uint_fast32_t uiZ;
+    int_fast16_t expZ;
+    uint_fast32_t sigZ;
+    union ui32_f32 uZ;
+
+    expA = expF32UI( uiA );
+    sigA = fracF32UI( uiA );
+    expB = expF32UI( uiB );
+    sigB = fracF32UI( uiB );
+    expDiff = expA - expB;
+    sigA <<= 7;
+    sigB <<= 7;
+    if ( 0 < expDiff ) goto expABigger;
+    if ( expDiff < 0 ) goto expBBigger;
+    if ( expA == 0xFF ) {
+        if ( sigA | sigB ) goto propagateNaN;
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        uiZ = defaultNaNF32UI;
+        goto uiZ;
+    }
+    if ( ! expA ) {
+        expA = 1;
+        expB = 1;
+    }
+    if ( sigB < sigA ) goto aBigger;
+    if ( sigA < sigB ) goto bBigger;
+    uiZ = packToF32UI( softfloat_roundingMode == softfloat_round_min, 0, 0 );
+    goto uiZ;
+ expBBigger:
+    if ( expB == 0xFF ) {
+        if ( sigB ) goto propagateNaN;
+        uiZ = packToF32UI( signZ ^ 1, 0xFF, 0 );
+        goto uiZ;
+    }
+    sigA += expA ? 0x40000000 : sigA;
+    sigA = softfloat_shift32RightJam( sigA, - expDiff );
+    sigB |= 0x40000000;
+ bBigger:
+    signZ ^= 1;
+    expZ = expB;
+    sigZ = sigB - sigA;
+    goto normRoundPack;
+ expABigger:
+    if ( expA == 0xFF ) {
+        if ( sigA ) goto propagateNaN;
+        uiZ = uiA;
+        goto uiZ;
+    }
+    sigB += expB ? 0x40000000 : sigB;
+    sigB = softfloat_shift32RightJam( sigB, expDiff );
+    sigA |= 0x40000000;
+ aBigger:
+    expZ = expA;
+    sigZ = sigA - sigB;
+ normRoundPack:
+    return softfloat_normRoundPackToF32( signZ, expZ - 1, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF32UI( uiA, uiB );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/s_subMagsF64.c b/target-riscv/fpu-custom-riscv/s_subMagsF64.c
new file mode 100644
index 0000000..45b81ba
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/s_subMagsF64.c
@@ -0,0 +1,81 @@
+
+#include <stdbool.h>
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+float64_t
+ softfloat_subMagsF64( uint_fast64_t uiA, uint_fast64_t uiB, bool signZ )
+{
+    int_fast16_t expA;
+    uint_fast64_t sigA;
+    int_fast16_t expB;
+    uint_fast64_t sigB;
+    int_fast16_t expDiff;
+    uint_fast64_t uiZ;
+    int_fast16_t expZ;
+    uint_fast64_t sigZ;
+    union ui64_f64 uZ;
+
+    expA = expF64UI( uiA );
+    sigA = fracF64UI( uiA );
+    expB = expF64UI( uiB );
+    sigB = fracF64UI( uiB );
+    expDiff = expA - expB;
+    sigA <<= 10;
+    sigB <<= 10;
+    if ( 0 < expDiff ) goto expABigger;
+    if ( expDiff < 0 ) goto expBBigger;
+    if ( expA == 0x7FF ) {
+        if ( sigA | sigB ) goto propagateNaN;
+        softfloat_raiseFlags( softfloat_flag_invalid );
+        uiZ = defaultNaNF64UI;
+        goto uiZ;
+    }
+    if ( ! expA ) {
+        expA = 1;
+        expB = 1;
+    }
+    if ( sigB < sigA ) goto aBigger;
+    if ( sigA < sigB ) goto bBigger;
+    uiZ = packToF64UI( softfloat_roundingMode == softfloat_round_min, 0, 0 );
+    goto uiZ;
+ expBBigger:
+    if ( expB == 0x7FF ) {
+        if ( sigB ) goto propagateNaN;
+        uiZ = packToF64UI( signZ ^ 1, 0x7FF, 0 );
+        goto uiZ;
+    }
+    sigA += expA ? UINT64_C( 0x4000000000000000 ) : sigA;
+    sigA = softfloat_shift64RightJam( sigA, - expDiff );
+    sigB |= UINT64_C( 0x4000000000000000 );
+ bBigger:
+    signZ ^= 1;
+    expZ = expB;
+    sigZ = sigB - sigA;
+    goto normRoundPack;
+ expABigger:
+    if ( expA == 0x7FF ) {
+        if ( sigA ) goto propagateNaN;
+        uiZ = uiA;
+        goto uiZ;
+    }
+    sigB += expB ? UINT64_C( 0x4000000000000000 ) : sigB;
+    sigB = softfloat_shift64RightJam( sigB, expDiff );
+    sigA |= UINT64_C( 0x4000000000000000 );
+ aBigger:
+    expZ = expA;
+    sigZ = sigA - sigB;
+ normRoundPack:
+    return softfloat_normRoundPackToF64( signZ, expZ - 1, sigZ );
+ propagateNaN:
+    uiZ = softfloat_propagateNaNF64UI( uiA, uiB );
+ uiZ:
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/softfloat.h b/target-riscv/fpu-custom-riscv/softfloat.h
new file mode 100644
index 0000000..81859e8
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/softfloat.h
@@ -0,0 +1,235 @@
+
+#ifndef softfloat_h
+#define softfloat_h
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*** UPDATE COMMENTS. ***/
+
+/*============================================================================
+
+This C header file is part of the SoftFloat IEEE Floating-point Arithmetic
+Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+#include "softfloat_types.h"
+
+/*----------------------------------------------------------------------------
+| Software floating-point underflow tininess-detection mode.
+*----------------------------------------------------------------------------*/
+extern int_fast8_t softfloat_detectTininess;
+enum {
+    softfloat_tininess_beforeRounding = 0,
+    softfloat_tininess_afterRounding  = 1
+};
+
+/*----------------------------------------------------------------------------
+| Software floating-point rounding mode.
+*----------------------------------------------------------------------------*/
+extern int_fast8_t softfloat_roundingMode;
+enum {
+    softfloat_round_nearest_even   = 0,
+    softfloat_round_minMag         = 1,
+    softfloat_round_min            = 2,
+    softfloat_round_max            = 3,
+    softfloat_round_nearest_maxMag = 4
+};
+
+/*----------------------------------------------------------------------------
+| Software floating-point exception flags.
+*----------------------------------------------------------------------------*/
+extern int_fast8_t softfloat_exceptionFlags;
+enum {
+    softfloat_flag_inexact   =  1,
+    softfloat_flag_underflow =  2,
+    softfloat_flag_overflow  =  4,
+    softfloat_flag_infinity  =  8,
+    softfloat_flag_invalid   = 16
+};
+
+/*----------------------------------------------------------------------------
+| Routine to raise any or all of the software floating-point exception flags.
+*----------------------------------------------------------------------------*/
+void softfloat_raiseFlags( int_fast8_t );
+
+/*----------------------------------------------------------------------------
+| Integer-to-floating-point conversion routines.
+*----------------------------------------------------------------------------*/
+float32_t ui32_to_f32( uint_fast32_t );
+float64_t ui32_to_f64( uint_fast32_t );
+floatx80_t ui32_to_fx80( uint_fast32_t );
+float128_t ui32_to_f128( uint_fast32_t );
+float32_t ui64_to_f32( uint_fast64_t );
+float64_t ui64_to_f64( uint_fast64_t );
+floatx80_t ui64_to_fx80( uint_fast64_t );
+float128_t ui64_to_f128( uint_fast64_t );
+float32_t i32_to_f32( int_fast32_t );
+float64_t i32_to_f64( int_fast32_t );
+floatx80_t i32_to_fx80( int_fast32_t );
+float128_t i32_to_f128( int_fast32_t );
+float32_t i64_to_f32( int_fast64_t );
+float64_t i64_to_f64( int_fast64_t );
+floatx80_t i64_to_fx80( int_fast64_t );
+float128_t i64_to_f128( int_fast64_t );
+
+/*----------------------------------------------------------------------------
+| 32-bit (single-precision) floating-point operations.
+*----------------------------------------------------------------------------*/
+uint_fast32_t f32_to_ui32( float32_t, int_fast8_t, bool );
+uint_fast64_t f32_to_ui64( float32_t, int_fast8_t, bool );
+int_fast32_t f32_to_i32( float32_t, int_fast8_t, bool );
+int_fast64_t f32_to_i64( float32_t, int_fast8_t, bool );
+uint_fast32_t f32_to_ui32_r_minMag( float32_t, bool );
+uint_fast64_t f32_to_ui64_r_minMag( float32_t, bool );
+int_fast32_t f32_to_i32_r_minMag( float32_t, bool );
+int_fast64_t f32_to_i64_r_minMag( float32_t, bool );
+float64_t f32_to_f64( float32_t );
+floatx80_t f32_to_fx80( float32_t );
+float128_t f32_to_f128( float32_t );
+float32_t f32_roundToInt( float32_t, int_fast8_t, bool );
+float32_t f32_add( float32_t, float32_t );
+float32_t f32_sub( float32_t, float32_t );
+float32_t f32_mul( float32_t, float32_t );
+float32_t f32_mulAdd( float32_t, float32_t, float32_t );
+float32_t f32_div( float32_t, float32_t );
+float32_t f32_rem( float32_t, float32_t );
+float32_t f32_sqrt( float32_t );
+bool f32_eq( float32_t, float32_t );
+bool f32_le( float32_t, float32_t );
+bool f32_lt( float32_t, float32_t );
+bool f32_eq_signaling( float32_t, float32_t );
+bool f32_le_quiet( float32_t, float32_t );
+bool f32_lt_quiet( float32_t, float32_t );
+bool f32_isSignalingNaN( float32_t );
+uint_fast16_t f32_classify( float32_t );
+
+/*----------------------------------------------------------------------------
+| 64-bit (double-precision) floating-point operations.
+*----------------------------------------------------------------------------*/
+uint_fast32_t f64_to_ui32( float64_t, int_fast8_t, bool );
+uint_fast64_t f64_to_ui64( float64_t, int_fast8_t, bool );
+int_fast32_t f64_to_i32( float64_t, int_fast8_t, bool );
+int_fast64_t f64_to_i64( float64_t, int_fast8_t, bool );
+uint_fast32_t f64_to_ui32_r_minMag( float64_t, bool );
+uint_fast64_t f64_to_ui64_r_minMag( float64_t, bool );
+int_fast32_t f64_to_i32_r_minMag( float64_t, bool );
+int_fast64_t f64_to_i64_r_minMag( float64_t, bool );
+float32_t f64_to_f32( float64_t );
+floatx80_t f64_to_fx80( float64_t );
+float128_t f64_to_f128( float64_t );
+float64_t f64_roundToInt( float64_t, int_fast8_t, bool );
+float64_t f64_add( float64_t, float64_t );
+float64_t f64_sub( float64_t, float64_t );
+float64_t f64_mul( float64_t, float64_t );
+float64_t f64_mulAdd( float64_t, float64_t, float64_t );
+float64_t f64_div( float64_t, float64_t );
+float64_t f64_rem( float64_t, float64_t );
+float64_t f64_sqrt( float64_t );
+bool f64_eq( float64_t, float64_t );
+bool f64_le( float64_t, float64_t );
+bool f64_lt( float64_t, float64_t );
+bool f64_eq_signaling( float64_t, float64_t );
+bool f64_le_quiet( float64_t, float64_t );
+bool f64_lt_quiet( float64_t, float64_t );
+bool f64_isSignalingNaN( float64_t );
+uint_fast16_t f64_classify( float64_t );
+
+/*----------------------------------------------------------------------------
+| Extended double-precision rounding precision.  Valid values are 32, 64, and
+| 80.
+*----------------------------------------------------------------------------*/
+extern int_fast8_t floatx80_roundingPrecision;
+
+/*----------------------------------------------------------------------------
+| Extended double-precision floating-point operations.
+*----------------------------------------------------------------------------*/
+uint_fast32_t fx80_to_ui32( floatx80_t, int_fast8_t, bool );
+uint_fast64_t fx80_to_ui64( floatx80_t, int_fast8_t, bool );
+int_fast32_t fx80_to_i32( floatx80_t, int_fast8_t, bool );
+int_fast64_t fx80_to_i64( floatx80_t, int_fast8_t, bool );
+uint_fast32_t fx80_to_ui32_r_minMag( floatx80_t, bool );
+uint_fast64_t fx80_to_ui64_r_minMag( floatx80_t, bool );
+int_fast32_t fx80_to_i32_r_minMag( floatx80_t, bool );
+int_fast64_t fx80_to_i64_r_minMag( floatx80_t, bool );
+float32_t fx80_to_f32( floatx80_t );
+float64_t fx80_to_f64( floatx80_t );
+float128_t fx80_to_f128( floatx80_t );
+floatx80_t fx80_roundToInt( floatx80_t, int_fast8_t, bool );
+floatx80_t fx80_add( floatx80_t, floatx80_t );
+floatx80_t fx80_sub( floatx80_t, floatx80_t );
+floatx80_t fx80_mul( floatx80_t, floatx80_t );
+floatx80_t fx80_mulAdd( floatx80_t, floatx80_t, floatx80_t );
+floatx80_t fx80_div( floatx80_t, floatx80_t );
+floatx80_t fx80_rem( floatx80_t, floatx80_t );
+floatx80_t fx80_sqrt( floatx80_t );
+bool fx80_eq( floatx80_t, floatx80_t );
+bool fx80_le( floatx80_t, floatx80_t );
+bool fx80_lt( floatx80_t, floatx80_t );
+bool fx80_eq_signaling( floatx80_t, floatx80_t );
+bool fx80_le_quiet( floatx80_t, floatx80_t );
+bool fx80_lt_quiet( floatx80_t, floatx80_t );
+bool fx80_isSignalingNaN( floatx80_t );
+
+/*----------------------------------------------------------------------------
+| 128-bit (quadruple-precision) floating-point operations.
+*----------------------------------------------------------------------------*/
+uint_fast32_t f128_to_ui32( float128_t, int_fast8_t, bool );
+uint_fast64_t f128_to_ui64( float128_t, int_fast8_t, bool );
+int_fast32_t f128_to_i32( float128_t, int_fast8_t, bool );
+int_fast64_t f128_to_i64( float128_t, int_fast8_t, bool );
+uint_fast32_t f128_to_ui32_r_minMag( float128_t, bool );
+uint_fast64_t f128_to_ui64_r_minMag( float128_t, bool );
+int_fast32_t f128_to_i32_r_minMag( float128_t, bool );
+int_fast64_t f128_to_i64_r_minMag( float128_t, bool );
+float32_t f128_to_f32( float128_t );
+float64_t f128_to_f64( float128_t );
+floatx80_t f128_to_fx80( float128_t );
+float128_t f128_roundToInt( float128_t, int_fast8_t, bool );
+float128_t f128_add( float128_t, float128_t );
+float128_t f128_sub( float128_t, float128_t );
+float128_t f128_mul( float128_t, float128_t );
+float128_t f128_mulAdd( float128_t, float128_t, float128_t );
+float128_t f128_div( float128_t, float128_t );
+float128_t f128_rem( float128_t, float128_t );
+float128_t f128_sqrt( float128_t );
+bool f128_eq( float128_t, float128_t );
+bool f128_le( float128_t, float128_t );
+bool f128_lt( float128_t, float128_t );
+bool f128_eq_signaling( float128_t, float128_t );
+bool f128_le_quiet( float128_t, float128_t );
+bool f128_lt_quiet( float128_t, float128_t );
+bool f128_isSignalingNaN( float128_t );
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
+
diff --git a/target-riscv/fpu-custom-riscv/softfloat.mk.in b/target-riscv/fpu-custom-riscv/softfloat.mk.in
new file mode 100644
index 0000000..7f70053
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/softfloat.mk.in
@@ -0,0 +1,126 @@
+softfloat_subproject_deps = \
+
+softfloat_hdrs = \
+	internals.h \
+	primitives.h \
+	softfloat.h \
+	softfloat_types.h \
+	platform.h \
+	specialize.h \
+
+softfloat_c_srcs = \
+	f32_add.c                      \
+	f32_div.c                      \
+	f32_eq.c                       \
+	f32_eq_signaling.c             \
+	f32_isSignalingNaN.c           \
+	f32_le.c                       \
+	f32_le_quiet.c                 \
+	f32_lt.c                       \
+	f32_lt_quiet.c                 \
+	f32_mulAdd.c                   \
+	f32_mul.c                      \
+	f32_rem.c                      \
+	f32_roundToInt.c               \
+	f32_sqrt.c                     \
+	f32_sub.c                      \
+	f32_to_f64.c                   \
+	f32_to_i32.c                   \
+	f32_to_i32_r_minMag.c          \
+	f32_to_i64.c                   \
+	f32_to_i64_r_minMag.c          \
+	f32_to_ui32.c                  \
+	f32_to_ui32_r_minMag.c         \
+	f32_to_ui64.c                  \
+	f32_to_ui64_r_minMag.c         \
+	f32_classify.c                 \
+	f64_add.c                      \
+	f64_div.c                      \
+	f64_eq.c                       \
+	f64_eq_signaling.c             \
+	f64_isSignalingNaN.c           \
+	f64_le.c                       \
+	f64_le_quiet.c                 \
+	f64_lt.c                       \
+	f64_lt_quiet.c                 \
+	f64_mulAdd.c                   \
+	f64_mul.c                      \
+	f64_rem.c                      \
+	f64_roundToInt.c               \
+	f64_sqrt.c                     \
+	f64_sub.c                      \
+	f64_to_f32.c                   \
+	f64_to_i32.c                   \
+	f64_to_i32_r_minMag.c          \
+	f64_to_i64.c                   \
+	f64_to_i64_r_minMag.c          \
+	f64_to_ui32.c                  \
+	f64_to_ui32_r_minMag.c         \
+	f64_to_ui64.c                  \
+	f64_to_ui64_r_minMag.c         \
+	f64_classify.c                 \
+	i32_to_f32.c                   \
+	i32_to_f64.c                   \
+	i64_to_f32.c                   \
+	i64_to_f64.c                   \
+	s_add128.c                     \
+	s_add192.c                     \
+	s_addMagsF32.c                 \
+	s_addMagsF64.c                 \
+	s_countLeadingZeros32.c        \
+	s_countLeadingZeros64.c        \
+	s_countLeadingZeros8.c         \
+	s_eq128.c                      \
+	s_estimateDiv128To64.c         \
+	s_estimateSqrt32.c             \
+	s_le128.c                      \
+	s_lt128.c                      \
+	s_mul128By64To192.c            \
+	s_mul128To256.c                \
+	s_mul64To128.c                 \
+	s_mulAddF32.c                  \
+	s_mulAddF64.c                  \
+	s_normRoundPackToF32.c         \
+	s_normRoundPackToF64.c         \
+	s_normSubnormalF32Sig.c        \
+	s_normSubnormalF64Sig.c        \
+	softfloat_state.c              \
+	s_roundPackToF32.c             \
+	s_roundPackToF64.c             \
+	s_roundPackToI32.c             \
+	s_roundPackToI64.c             \
+	s_roundPackToUI32.c            \
+	s_roundPackToUI64.c            \
+	s_shift128ExtraRightJam.c      \
+	s_shift128RightJam.c           \
+	s_shift32RightJam.c            \
+	s_shift64ExtraRightJam.c       \
+	s_shift64RightJam.c            \
+	s_shortShift128ExtraRightJam.c \
+	s_shortShift128Left.c          \
+	s_shortShift128Right.c         \
+	s_shortShift192Left.c          \
+	s_shortShift32Right1Jam.c      \
+	s_shortShift64ExtraRightJam.c  \
+	s_shortShift64RightJam.c       \
+	s_sub128.c                     \
+	s_sub192.c                     \
+	s_subMagsF32.c                 \
+	s_subMagsF64.c                 \
+	ui32_to_f32.c                  \
+	ui32_to_f64.c                  \
+	ui64_to_f32.c                  \
+	ui64_to_f64.c                  \
+	softfloat_raiseFlags.c         \
+	s_commonNaNToF32UI.c           \
+	s_commonNaNToF64UI.c           \
+	s_f32UIToCommonNaN.c           \
+	s_f64UIToCommonNaN.c           \
+	s_isSigNaNF32UI.c              \
+	s_isSigNaNF64UI.c              \
+	s_propagateNaNF32UI.c          \
+	s_propagateNaNF64UI.c          \
+
+softfloat_test_srcs =
+
+softfloat_install_prog_srcs =
diff --git a/target-riscv/fpu-custom-riscv/softfloat_raiseFlags.c b/target-riscv/fpu-custom-riscv/softfloat_raiseFlags.c
new file mode 100644
index 0000000..c0c0dc8
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/softfloat_raiseFlags.c
@@ -0,0 +1,51 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+#include "platform.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Raises the exceptions specified by `flags'.  Floating-point traps can be
+| defined here if desired.  It is currently not possible for such a trap
+| to substitute a result value.  If traps are not implemented, this routine
+| should be simply `float_exception_flags |= flags;'.
+*----------------------------------------------------------------------------*/
+
+void softfloat_raiseFlags( int_fast8_t flags )
+{
+
+    softfloat_exceptionFlags |= flags;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/softfloat_state.c b/target-riscv/fpu-custom-riscv/softfloat_state.c
new file mode 100644
index 0000000..cd9b04d
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/softfloat_state.c
@@ -0,0 +1,19 @@
+
+/*** COMMENTS. ***/
+
+#include <stdint.h>
+#include "platform.h"
+#include "internals.h"
+#include "specialize.h"
+#include "softfloat.h"
+
+/*----------------------------------------------------------------------------
+| Floating-point rounding mode, extended double-precision rounding precision,
+| and exception flags.
+*----------------------------------------------------------------------------*/
+int_fast8_t softfloat_roundingMode = softfloat_round_nearest_even;
+int_fast8_t softfloat_detectTininess = init_detectTininess;
+int_fast8_t softfloat_exceptionFlags = 0;
+
+int_fast8_t floatx80_roundingPrecision = 80;
+
diff --git a/target-riscv/fpu-custom-riscv/softfloat_types.h b/target-riscv/fpu-custom-riscv/softfloat_types.h
new file mode 100644
index 0000000..9fada89
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/softfloat_types.h
@@ -0,0 +1,16 @@
+
+#ifndef softfloat_types_h
+#define softfloat_types_h
+
+/*** COMMENTS. ***/
+
+#include <stdbool.h>
+#include <stdint.h>
+
+typedef uint32_t float32_t;
+typedef uint64_t float64_t;
+typedef struct { uint64_t v; uint16_t x; } floatx80_t;
+typedef struct { uint64_t v[ 2 ]; } float128_t;
+
+#endif
+
diff --git a/target-riscv/fpu-custom-riscv/specialize.h b/target-riscv/fpu-custom-riscv/specialize.h
new file mode 100644
index 0000000..2efc132
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/specialize.h
@@ -0,0 +1,113 @@
+
+/*============================================================================
+
+*** FIX.
+
+This C source fragment is part of the SoftFloat IEC/IEEE Floating-point
+Arithmetic Package, Release 2b.
+
+Written by John R. Hauser.  This work was made possible in part by the
+International Computer Science Institute, located at Suite 600, 1947 Center
+Street, Berkeley, California 94704.  Funding was partially provided by the
+National Science Foundation under grant MIP-9311980.  The original version
+of this code was written as part of a project to build a fixed-point vector
+processor in collaboration with the University of California at Berkeley,
+overseen by Profs. Nelson Morgan and John Wawrzynek.  More information
+is available through the Web page `http://www.cs.berkeley.edu/~jhauser/
+arithmetic/SoftFloat.html'.
+
+THIS SOFTWARE IS DISTRIBUTED AS IS, FOR FREE.  Although reasonable effort has
+been made to avoid it, THIS SOFTWARE MAY CONTAIN FAULTS THAT WILL AT TIMES
+RESULT IN INCORRECT BEHAVIOR.  USE OF THIS SOFTWARE IS RESTRICTED TO PERSONS
+AND ORGANIZATIONS WHO CAN AND WILL TAKE FULL RESPONSIBILITY FOR ALL LOSSES,
+COSTS, OR OTHER PROBLEMS THEY INCUR DUE TO THE SOFTWARE, AND WHO FURTHERMORE
+EFFECTIVELY INDEMNIFY JOHN HAUSER AND THE INTERNATIONAL COMPUTER SCIENCE
+INSTITUTE (possibly via similar legal warning) AGAINST ALL LOSSES, COSTS, OR
+OTHER PROBLEMS INCURRED BY THEIR CUSTOMERS AND CLIENTS DUE TO THE SOFTWARE.
+
+Derivative works are acceptable, even for commercial purposes, so long as
+(1) the source code for the derivative work includes prominent notice that
+the work is derivative, and (2) the source code includes prominent notice with
+these four paragraphs for those parts of this code that are retained.
+
+=============================================================================*/
+
+#include <stdbool.h>
+#include <stdint.h>
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+#define init_detectTininess softfloat_tininess_beforeRounding;
+
+/*----------------------------------------------------------------------------
+| Structure used to transfer NaN representations from one format to another.
+*----------------------------------------------------------------------------*/
+struct commonNaN {
+    bool sign;
+    uint64_t v64, v0;
+};
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated single-precision NaN.
+*----------------------------------------------------------------------------*/
+#define defaultNaNF32UI 0xFFFFFFFF
+
+/*----------------------------------------------------------------------------
+| Returns 1 if the single-precision floating-point value `a' is a signaling
+| NaN; otherwise, returns 0.
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool softfloat_isSigNaNF32UI( uint_fast32_t ui )
+    { return ( ( ui>>22 & 0x1FF ) == 0x1FE ) && ( ui & 0x003FFFFF ); }
+#else
+bool softfloat_isSigNaNF32UI( uint_fast32_t );
+#endif
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+struct commonNaN softfloat_f32UIToCommonNaN( uint_fast32_t );
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE uint_fast32_t softfloat_commonNaNToF32UI( struct commonNaN a )
+    { return (uint_fast32_t) a.sign<<31 | 0x7FFFFFFF; }
+#else
+uint_fast32_t softfloat_commonNaNToF32UI( struct commonNaN );
+#endif
+
+/*----------------------------------------------------------------------------
+| Takes two single-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+uint_fast32_t softfloat_propagateNaNF32UI( uint_fast32_t, uint_fast32_t );
+
+/*----------------------------------------------------------------------------
+| The pattern for a default generated double-precision NaN.
+*----------------------------------------------------------------------------*/
+#define defaultNaNF64UI UINT64_C(0xFFFFFFFFFFFFFFFF)
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+#if defined INLINE_LEVEL && ( 1 <= INLINE_LEVEL )
+INLINE bool softfloat_isSigNaNF64UI( uint_fast64_t ui )
+{
+    return
+        ( ( ui>>51 & 0xFFF ) == 0xFFE )
+            && ( ui & UINT64_C( 0x0007FFFFFFFFFFFF ) );
+}
+#else
+bool softfloat_isSigNaNF64UI( uint_fast64_t );
+#endif
+
+/*----------------------------------------------------------------------------
+*----------------------------------------------------------------------------*/
+/*** MIGHT BE INLINE'D. ***/
+struct commonNaN softfloat_f64UIToCommonNaN( uint_fast64_t );
+uint_fast64_t softfloat_commonNaNToF64UI( struct commonNaN );
+
+/*----------------------------------------------------------------------------
+| Takes two double-precision floating-point values `a' and `b', one of which
+| is a NaN, and returns the appropriate NaN result.  If either `a' or `b' is a
+| signaling NaN, the invalid exception is raised.
+*----------------------------------------------------------------------------*/
+uint_fast64_t softfloat_propagateNaNF64UI( uint_fast64_t, uint_fast64_t );
+
diff --git a/target-riscv/fpu-custom-riscv/ui32_to_f32.c b/target-riscv/fpu-custom-riscv/ui32_to_f32.c
new file mode 100644
index 0000000..d79730c
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/ui32_to_f32.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t ui32_to_f32( uint_fast32_t a )
+{
+    union ui32_f32 uZ;
+
+    if ( ! a ) {
+        uZ.ui = 0;
+        return uZ.f;
+    }
+    if ( a & 0x80000000 ) {
+        return
+            softfloat_roundPackToF32(
+                0, 0x9D, softfloat_shortShift32Right1Jam( a ) );
+    } else {
+        return softfloat_normRoundPackToF32( 0, 0x9C, a );
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/ui32_to_f64.c b/target-riscv/fpu-custom-riscv/ui32_to_f64.c
new file mode 100644
index 0000000..9e28eb4
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/ui32_to_f64.c
@@ -0,0 +1,26 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t ui32_to_f64( uint_fast32_t a )
+{
+    uint_fast64_t uiZ;
+    int shiftCount;
+    union ui64_f64 uZ;
+
+    if ( ! a ) {
+        uiZ = 0;
+    } else {
+        shiftCount = softfloat_countLeadingZeros32( a ) + 21;
+        uiZ =
+            packToF64UI(
+                0, 0x432 - shiftCount, (uint_fast64_t) a<<shiftCount );
+    }
+    uZ.ui = uiZ;
+    return uZ.f;
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/ui64_to_f32.c b/target-riscv/fpu-custom-riscv/ui64_to_f32.c
new file mode 100644
index 0000000..b349aad
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/ui64_to_f32.c
@@ -0,0 +1,31 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float32_t ui64_to_f32( uint_fast64_t a )
+{
+    int shiftCount;
+    union ui32_f32 u;
+    uint_fast32_t sig;
+
+    shiftCount = softfloat_countLeadingZeros64( a ) - 40;
+    if ( 0 <= shiftCount ) {
+        u.ui =
+            a ? packToF32UI(
+                    0, 0x95 - shiftCount, (uint_fast32_t) a<<shiftCount )
+                : 0;
+        return u.f;
+    } else {
+        shiftCount += 7;
+        sig =
+            ( shiftCount < 0 )
+                ? softfloat_shortShift64RightJam( a, - shiftCount )
+                : (uint_fast32_t) a<<shiftCount;
+        return softfloat_roundPackToF32( 0, 0x9C - shiftCount, sig );
+    }
+
+}
+
diff --git a/target-riscv/fpu-custom-riscv/ui64_to_f64.c b/target-riscv/fpu-custom-riscv/ui64_to_f64.c
new file mode 100644
index 0000000..8b03b35
--- /dev/null
+++ b/target-riscv/fpu-custom-riscv/ui64_to_f64.c
@@ -0,0 +1,25 @@
+
+#include <stdint.h>
+#include "platform.h"
+#include "primitives.h"
+#include "internals.h"
+#include "softfloat.h"
+
+float64_t ui64_to_f64( uint_fast64_t a )
+{
+    union ui64_f64 uZ;
+
+    if ( ! a ) {
+        uZ.ui = 0;
+        return uZ.f;
+    }
+    if ( a & UINT64_C( 0x8000000000000000 ) ) {
+        return
+            softfloat_roundPackToF64(
+                0, 0x43D, softfloat_shortShift64RightJam( a, 1 ) );
+    } else {
+        return softfloat_normRoundPackToF64( 0, 0x43C, a );
+    }
+
+}
+
diff --git a/target-riscv/gdbstub.c b/target-riscv/gdbstub.c
new file mode 100644
index 0000000..a1d641e
--- /dev/null
+++ b/target-riscv/gdbstub.c
@@ -0,0 +1,59 @@
+/*
+ *  RISC-V gdb server stub
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include "config.h"
+#include "qemu-common.h"
+#include "exec/gdbstub.h"
+
+int riscv_cpu_gdb_read_register(CPUState *cs, uint8_t *mem_buf, int n)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+
+    if (n < 32) {
+        return gdb_get_regl(mem_buf, env->active_tc.gpr[n]);
+    } else if (n < 64) {
+        return gdb_get_regl(mem_buf, env->helper_csr[n-32]);
+    } else if (n == 64) {
+        return gdb_get_regl(mem_buf, env->active_tc.PC);
+    }
+    return 0;
+}
+
+int riscv_cpu_gdb_write_register(CPUState *cs, uint8_t *mem_buf, int n)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+    target_ulong tmp;
+
+    tmp = ldtul_p(mem_buf);
+
+    if (n < 32) {
+        env->active_tc.gpr[n] = tmp;
+        return sizeof(target_ulong);
+    } else if (n < 64) {
+        env->helper_csr[n-32] = tmp;
+        return sizeof(target_ulong);
+    } else if (n == 64) {
+        env->active_tc.PC = tmp;
+        return sizeof(target_ulong);
+    }
+    return sizeof(target_ulong);
+}
diff --git a/target-riscv/helper.c b/target-riscv/helper.c
new file mode 100644
index 0000000..974010d
--- /dev/null
+++ b/target-riscv/helper.c
@@ -0,0 +1,313 @@
+/*
+ *  RISC-V emulation helpers for qemu.
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see <http://www.gnu.org/licenses/>.
+ */
+#include <stdarg.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <inttypes.h>
+#include <signal.h>
+
+#include "cpu.h"
+
+// allow the optimized permissions checking if certain values in 
+// include/exec/cpu-all.h match what we expect
+#if (PAGE_READ == 0x1 || PAGE_WRITE == 0x2 || PAGE_EXEC == 0x4)
+    #define OPTIMIZED_PERMISSIONS_CHECK
+#endif
+
+enum {
+    TLBRET_NOMATCH = -1,
+    TLBRET_MATCH = 0
+};
+
+#if !defined(CONFIG_USER_ONLY)
+
+static int get_physical_address (CPURISCVState *env, hwaddr *physical,
+                                int *prot, target_ulong address,
+                                int rw, int access_type)
+{
+    /* NOTE: the env->active_tc.PC value visible here will not be
+     * correct, but the value visible to the exception handler 
+     * (riscv_cpu_do_interrupt) is correct */
+
+    // first, check if VM is on:
+    if(unlikely(!(env->helper_csr[CSR_STATUS] & SR_VM))) {
+        *physical = address;
+        *prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;
+    } else {
+        // handle translation
+        if ((address >= 0x3f8) && (address < 0x400)) {
+            // TODO: fix linux so that this is not necessary
+            *physical = address;
+            *prot = PAGE_READ | PAGE_WRITE;
+            return TLBRET_MATCH;
+        }
+        if ((address >= 0x400) && (address <= 0x410)) {
+            // hacky memory hole to watch HTIF registers
+            *physical = address;
+            *prot = PAGE_READ | PAGE_WRITE;
+            return TLBRET_MATCH;
+        }
+
+        CPUState *cs = CPU(riscv_env_get_cpu(env));
+        uint64_t pte = 0; 
+        uint64_t base = env->helper_csr[CSR_PTBR];
+        uint64_t ptd;
+        int ptshift = 20;
+        int64_t i = 0;
+#ifdef OPTIMIZED_PERMISSIONS_CHECK
+        uint8_t protcheck;
+#endif
+        for (i = 0; i < 3; i++, ptshift -= 10) {
+            uint64_t idx = (address >> (13+ptshift)) & ((1 << 10)-1);
+            uint64_t pte_addr = base + (idx << 3);
+
+            ptd = ldq_phys(cs->as, pte_addr);
+
+            if (!(ptd & PTE_V)) { 
+                return TLBRET_NOMATCH;
+            } else if (ptd & PTE_T) { 
+                base = (ptd >> 13) << 13;
+            } else {
+                uint64_t vpn = address >> 13;
+                ptd |= (vpn & ((1 <<(ptshift))-1)) << 13;
+       
+                // TODO: fault if physical addr is out of range
+                pte = ptd;
+                break;
+            }
+        }
+
+#ifdef OPTIMIZED_PERMISSIONS_CHECK
+        // Optimized permissions checking:
+        // We explicitly check for some defines at compile time to enable this,
+        // since it basically violates an abstraction barrier.
+        // If some values we rely on are changed, we fall back to the 
+        // unoptimized version.
+        *prot = (pte >> ((env->helper_csr[CSR_STATUS] & SR_S)*3+3)) & 0x7;
+        protcheck = ((rw >> 1) << 2) | ((rw & 0x1) ? (0x2) : (0x1));
+
+        if (unlikely((*prot & protcheck) != protcheck)) {
+            return TLBRET_NOMATCH;
+        }
+#else
+        // unoptimized version. used as a fallback
+        *prot = 0;
+        if (env->helper_csr[CSR_STATUS] & SR_S) {
+            // check supervisor
+            if ((rw & 0x2) & !(pte & PTE_SX)) {
+                return TLBRET_NOMATCH;
+            } else if ((rw == 0x1) & !(pte & PTE_SW)) {
+                return TLBRET_NOMATCH;
+            } else if (!(pte & PTE_SR)) {
+                return TLBRET_NOMATCH;
+            }
+            if (pte & PTE_SX) {
+                *prot |= PAGE_EXEC;
+            }
+            if (pte & PTE_SW) {
+                *prot |= PAGE_WRITE;
+            }
+            if (pte & PTE_SR) {
+                *prot |= PAGE_READ;
+            }
+        } else {
+            // check user
+            if ((rw & 0x2) & !(pte & PTE_UX)) {
+                return TLBRET_NOMATCH;
+            } else if ((rw == 0x1) & !(pte & PTE_UW)) {
+                return TLBRET_NOMATCH;
+            } else if (!(pte & PTE_UR)) {
+                return TLBRET_NOMATCH;
+            }
+            if (pte & PTE_UX) {
+                *prot |= PAGE_EXEC;
+            }
+            if (pte & PTE_UW) {
+                *prot |= PAGE_WRITE;
+            }
+            if (pte & PTE_UR) {
+                *prot |= PAGE_READ;
+            }
+        }
+#endif
+        *physical = ((pte >> 13) << 13) | (address & 0x1FFF);
+    }
+    return TLBRET_MATCH;
+}
+#endif
+
+static void raise_mmu_exception(CPURISCVState *env, target_ulong address,
+                                int rw, int tlb_error)
+{
+    CPUState *cs = CPU(riscv_env_get_cpu(env));
+    int exception = 0;
+
+    switch (tlb_error) {
+    case TLBRET_NOMATCH:
+        /* No TLB match for a mapped address */
+        if (rw & 0x2) { // inst access
+            exception = RISCV_EXCP_INST_ACCESS_FAULT;
+        } else if (rw == 0x1) { // store access
+            exception = RISCV_EXCP_STORE_ACCESS_FAULT;
+            env->helper_csr[CSR_BADVADDR] = address;
+        } else { // load access
+            exception = RISCV_EXCP_LOAD_ACCESS_FAULT;
+            env->helper_csr[CSR_BADVADDR] = address;
+        }
+        break;
+    default:
+        // currently unhandled for RISCV
+        printf("encountered unknown mmu fault in helper.c:raise_mmu_exception\n");
+        exit(0);
+        break;
+    }
+    cs->exception_index = exception;
+}
+
+#if !defined(CONFIG_USER_ONLY)
+hwaddr riscv_cpu_get_phys_page_debug(CPUState *cs, vaddr addr)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    hwaddr phys_addr;
+    int prot;
+
+    if (get_physical_address(&cpu->env, &phys_addr, &prot, addr, 0,
+                             ACCESS_INT) != 0) {
+        return -1;
+    }
+    return phys_addr;
+}
+#endif
+
+// NOTE: this gets called a lot
+int riscv_cpu_handle_mmu_fault(CPUState *cs, vaddr address, int rw,
+                              int mmu_idx)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+#if !defined(CONFIG_USER_ONLY)
+    hwaddr physical;
+    int prot;
+    int access_type;
+#endif
+    int ret = 0;
+
+    qemu_log("%s pc " TARGET_FMT_lx " ad %" VADDR_PRIx " rw %d mmu_idx %d\n",
+              __func__, env->active_tc.PC, address, rw, mmu_idx);
+
+#if !defined(CONFIG_USER_ONLY)
+    access_type = ACCESS_INT; // TODO: huh? this was here from mips
+    ret = get_physical_address(env, &physical, &prot,
+                               address, rw, access_type);
+    qemu_log("%s address=%" VADDR_PRIx " ret %d physical " TARGET_FMT_plx
+             " prot %d\n",
+             __func__, address, ret, physical, prot);
+    if (ret == TLBRET_MATCH) {
+        tlb_set_page(cs, address & TARGET_PAGE_MASK,
+                     physical & TARGET_PAGE_MASK, prot | PAGE_EXEC,
+                     mmu_idx, TARGET_PAGE_SIZE);
+        ret = 0;
+    } else if (ret < 0)
+#endif
+    {
+        raise_mmu_exception(env, address, rw, ret);
+        ret = 1;
+    }
+    return ret;
+}
+
+static const char * const riscv_excp_names[13] = {
+    "instruction_address_misaligned",
+    "instruction_access_fault",
+    "illegal_instruction",
+    "privileged_instruction",
+    "fp_disabled",
+    "UNUSED",
+    "syscall",
+    "breakpoint",
+    "load_address_misaligned",
+    "store_address_misaligned",
+    "load_access_fault",
+    "store_access_fault",
+    "accelerator_disabled",
+};
+
+void riscv_cpu_do_interrupt(CPUState *cs)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+
+#ifdef RISCV_DEBUG_INTERRUPT
+    if (!(cs->exception_index & (0x1 << 31))) {
+        if ((cs->exception_index == RISCV_EXCP_ILLEGAL_INST) 
+            || (cs->exception_index == RISCV_EXCP_FP_DISABLED)) {
+            printf("core   0: exception trap_%s, epc 0x%016lx\n", 
+                    riscv_excp_names[cs->exception_index], env->active_tc.PC);
+        }
+    }
+#endif
+
+    // Store Cause in CSR_CAUSE. this comes from cs->exception_index
+    if (cs->exception_index & (0x1 << 31)) {
+        // hacky for now. the MSB (bit 63) indicates interrupt but cs->exception 
+        // index is only 32 bits wide
+        if (cs->exception_index == RISCV_EXCP_SERIAL_INTERRUPT) {
+            // help out the slow serial device so the driver doesn't get confused
+            // turn off it's irq line
+            env->helper_csr[CSR_STATUS] &= ~(0x1 << 28);
+        }
+        env->helper_csr[CSR_CAUSE] = cs->exception_index & 0x1F;
+        env->helper_csr[CSR_CAUSE] |= (1L << 63);
+    } else {
+        env->helper_csr[CSR_CAUSE] = cs->exception_index;
+    }
+
+    // Manage the PS/S Stack: CSR_STATUS[SR_PS] = CSR_STATUS[SR_S], 
+    // CSR_STATUS[SR_S] = 1 // enable supervisor
+    if (env->helper_csr[CSR_STATUS] & SR_S) {
+        env->helper_csr[CSR_STATUS] |= SR_PS;
+    } else {
+        env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_PS);
+    }
+    env->helper_csr[CSR_STATUS] |= SR_S; // turn on supervisor;
+
+    // Manage the EI/PEI Stack: CSR_STATUS[SR_PEI] = CSR_STATUS[SR_EI]
+    // CSR_STATUS[SR_EI] = 0 // disable interrupts
+    if (env->helper_csr[CSR_STATUS] & SR_EI) {
+        env->helper_csr[CSR_STATUS] |= SR_PEI;
+    } else {
+        env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_PEI);
+    }
+    env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_EI); // turn off interrupts
+
+    // NOTE: CSR_BADVADDR should be set from the handler that raises the exception
+
+    // Store original PC to epc reg
+    // This is correct because the env->active_tc.PC value visible here is 
+    // actually the correct value, unlike other places where env->active_tc.PC
+    // may be used.
+    env->helper_csr[CSR_EPC] = env->active_tc.PC;
+
+    // FINALLY, set PC to value in evec register and return
+    env->active_tc.PC = env->helper_csr[CSR_EVEC];
+
+    cs->exception_index = EXCP_NONE; // mark handled to qemu
+}
diff --git a/target-riscv/helper.h b/target-riscv/helper.h
new file mode 100644
index 0000000..a767ca6
--- /dev/null
+++ b/target-riscv/helper.h
@@ -0,0 +1,76 @@
+// Exceptions
+DEF_HELPER_2(raise_exception, noreturn, env, i32)
+
+// MULHSU helper
+DEF_HELPER_3(mulhsu, tl, env, tl, tl)
+
+// Floating Point - fused
+DEF_HELPER_5(fmadd_s, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fmadd_d, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fmsub_s, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fmsub_d, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fnmsub_s, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fnmsub_d, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fnmadd_s, tl, env, tl, tl, tl, tl)
+DEF_HELPER_5(fnmadd_d, tl, env, tl, tl, tl, tl)
+
+// Floating Point - Single Precision
+DEF_HELPER_4(fadd_s, tl, env, tl, tl, tl)
+DEF_HELPER_4(fsub_s, tl, env, tl, tl, tl)
+DEF_HELPER_4(fmul_s, tl, env, tl, tl, tl)
+DEF_HELPER_4(fdiv_s, tl, env, tl, tl, tl)
+DEF_HELPER_3(fsgnj_s, tl, env, tl, tl)
+DEF_HELPER_3(fsgnjn_s, tl, env, tl, tl)
+DEF_HELPER_3(fsgnjx_s, tl, env, tl, tl)
+DEF_HELPER_3(fmin_s, tl, env, tl, tl)
+DEF_HELPER_3(fmax_s, tl, env, tl, tl)
+DEF_HELPER_3(fsqrt_s, tl, env, tl, tl)
+DEF_HELPER_3(fle_s, tl, env, tl, tl)
+DEF_HELPER_3(flt_s, tl, env, tl, tl)
+DEF_HELPER_3(feq_s, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_w_s, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_wu_s, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_l_s, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_lu_s, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_s_w, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_s_wu, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_s_l, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_s_lu, tl, env, tl, tl)
+DEF_HELPER_2(fclass_s, tl, env, tl)
+
+// Floating Point - Double Precision
+DEF_HELPER_4(fadd_d, tl, env, tl, tl, tl)
+DEF_HELPER_4(fsub_d, tl, env, tl, tl, tl)
+DEF_HELPER_4(fmul_d, tl, env, tl, tl, tl)
+DEF_HELPER_4(fdiv_d, tl, env, tl, tl, tl)
+DEF_HELPER_3(fsgnj_d, tl, env, tl, tl)
+DEF_HELPER_3(fsgnjn_d, tl, env, tl, tl)
+DEF_HELPER_3(fsgnjx_d, tl, env, tl, tl)
+DEF_HELPER_3(fmin_d, tl, env, tl, tl)
+DEF_HELPER_3(fmax_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_s_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_d_s, tl, env, tl, tl)
+DEF_HELPER_3(fsqrt_d, tl, env, tl, tl)
+DEF_HELPER_3(fle_d, tl, env, tl, tl)
+DEF_HELPER_3(flt_d, tl, env, tl, tl)
+DEF_HELPER_3(feq_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_w_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_wu_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_l_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_lu_d, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_d_w, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_d_wu, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_d_l, tl, env, tl, tl)
+DEF_HELPER_3(fcvt_d_lu, tl, env, tl, tl)
+DEF_HELPER_2(fclass_d, tl, env, tl)
+
+/* Special functions */
+#ifndef CONFIG_USER_ONLY
+DEF_HELPER_3(csrrw, tl, env, tl, tl)
+DEF_HELPER_3(csrrs, tl, env, tl, tl)
+DEF_HELPER_3(csrrc, tl, env, tl, tl)
+DEF_HELPER_1(sret, tl, env)
+DEF_HELPER_2(scall, tl, env, tl)
+DEF_HELPER_1(tlb_flush, void, env)
+#endif /* !CONFIG_USER_ONLY */
+//DEF_HELPER_1(wait, void, env)
diff --git a/target-riscv/machine.c b/target-riscv/machine.c
new file mode 100644
index 0000000..27275be
--- /dev/null
+++ b/target-riscv/machine.c
@@ -0,0 +1,89 @@
+/*
+ *  RISC-V CPU machine state helpers
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include "hw/hw.h"
+#include "hw/boards.h"
+
+#include "cpu.h"
+
+static void save_tc(QEMUFile *f, TCState *tc)
+{
+    int i;
+    /* Save active TC */
+    for (i = 0; i < 32; i++) {
+        qemu_put_betls(f, &tc->gpr[i]);
+    }
+    for (i = 0; i < 32; i++) {
+        qemu_put_betls(f, &tc->fpr[i]);
+    }
+    qemu_put_betls(f, &tc->PC);
+}
+
+void cpu_save(QEMUFile *f, void *opaque)
+{
+    CPURISCVState *env = opaque;
+    int i;
+
+    /* Save active TC */
+    save_tc(f, &env->active_tc);
+
+    /* Save CPU metastate */
+    qemu_put_be32s(f, &env->current_tc);
+
+    for (i = 0; i < 32; i++) {
+        qemu_put_betls(f, &env->helper_csr[i]);
+    }
+}
+
+static void load_tc(QEMUFile *f, TCState *tc)
+{
+    int i;
+    /* Save active TC */
+    for(i = 0; i < 32; i++) {
+        qemu_get_betls(f, &tc->gpr[i]);
+    }
+    for(i = 0; i < 32; i++) {
+        qemu_get_betls(f, &tc->fpr[i]);
+    }
+    qemu_get_betls(f, &tc->PC);
+}
+
+int cpu_load(QEMUFile *f, void *opaque, int version_id)
+{
+    CPURISCVState *env = opaque;
+    RISCVCPU *cpu = riscv_env_get_cpu(env);
+    int i;
+
+    if (version_id != 3)
+        return -EINVAL;
+
+    /* Load active TC */
+    load_tc(f, &env->active_tc);
+
+    /* Load CPU metastate */
+    qemu_get_be32s(f, &env->current_tc);
+
+    for (i = 0; i < 32; i++) {
+        qemu_get_betls(f, &env->helper_csr[i]);
+    }
+
+    tlb_flush(CPU(cpu), 1);
+    return 0;
+}
diff --git a/target-riscv/op_helper.c b/target-riscv/op_helper.c
new file mode 100644
index 0000000..8b233ac
--- /dev/null
+++ b/target-riscv/op_helper.c
@@ -0,0 +1,655 @@
+/*
+ *  RISC-V emulation helpers for qemu.
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see <http://www.gnu.org/licenses/>.
+ */
+#include <stdlib.h>
+#include "cpu.h"
+#include "qemu/host-utils.h"
+#include "exec/helper-proto.h"
+#include "exec/cpu_ldst.h"
+
+// custom floating point includes. use this instead of qemu's included 
+// fpu/softmmu since we know it already works exactly as desired for riscv 
+#include "fpu-custom-riscv/softfloat.h"
+#include "fpu-custom-riscv/platform.h"
+#include "fpu-custom-riscv/internals.h"
+
+#ifndef CONFIG_USER_ONLY
+static inline void cpu_riscv_tlb_flush (CPURISCVState *env, int flush_global);
+void csr_write_helper(CPURISCVState *env, target_ulong val_to_write, target_ulong csrno);
+target_ulong csr_read_helper(CPURISCVState *env, target_ulong csrno);
+
+#endif
+
+#define RISCV_RM ({ if(rm == 7) rm = env->helper_csr[CSR_FRM]; \
+                    /* TODO: throw trap for rm > 4 */ \
+                    rm; })
+                    
+#define set_fp_exceptions ({ env->helper_csr[CSR_FFLAGS] |= softfloat_exceptionFlags;\
+                             softfloat_exceptionFlags = 0; })
+
+/* Exceptions processing helpers */
+static inline void QEMU_NORETURN do_raise_exception_err(CPURISCVState *env,
+                                                        uint32_t exception,
+                                                        uintptr_t pc)
+{
+    CPUState *cs = CPU(riscv_env_get_cpu(env));
+    qemu_log("%s: %d\n", __func__, exception);
+    cs->exception_index = exception;
+    if (pc) {
+        /* now we have a real cpu fault */
+        cpu_restore_state(cs, pc);
+    }
+    cpu_loop_exit(cs);
+}
+
+void helper_raise_exception(CPURISCVState *env, uint32_t exception)
+{
+    do_raise_exception_err(env, exception, 0);
+}
+
+/* floating point */
+uint64_t helper_fmadd_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1, frs2, frs3);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmadd_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1, frs2, frs3);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmsub_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1, frs2, frs3 ^ (uint32_t)INT32_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmsub_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1, frs2, frs3 ^ (uint64_t)INT64_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fnmsub_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1 ^ (uint32_t)INT32_MIN, frs2, frs3);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fnmsub_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1 ^ (uint64_t)INT64_MIN, frs2, frs3);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fnmadd_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1 ^ (uint32_t)INT32_MIN, frs2, frs3 ^ (uint32_t)INT32_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fnmadd_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t frs3, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1 ^ (uint64_t)INT64_MIN, frs2, frs3 ^ (uint64_t)INT64_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fadd_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1, 0x3f800000, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fsub_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1, 0x3f800000, frs2 ^ (uint32_t)INT32_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmul_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_mulAdd(frs1, frs2, (frs1 ^ frs2) & (uint32_t)INT32_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fdiv_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_div(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fsgnj_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = (frs1 &~ (uint32_t)INT32_MIN) | (frs2 & (uint32_t)INT32_MIN);
+    return frs1;
+}
+
+uint64_t helper_fsgnjn_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = (frs1 &~ (uint32_t)INT32_MIN) | ((~frs2) & (uint32_t)INT32_MIN);
+    return frs1;
+}
+
+uint64_t helper_fsgnjx_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = frs1 ^ (frs2 & (uint32_t)INT32_MIN);
+    return frs1;
+}
+
+uint64_t helper_fmin_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = isNaNF32UI(frs2) || f32_lt_quiet(frs1, frs2) ? frs1 : frs2;
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmax_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = isNaNF32UI(frs2) || f32_lt_quiet(frs2, frs1) ? frs1 : frs2;
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fsqrt_s(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_sqrt(frs1);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fle_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = f32_le(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_flt_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = f32_lt(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_feq_s(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = f32_eq(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_w_s(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = (int64_t)((int32_t)f32_to_i32(frs1, RISCV_RM, true));
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_wu_s(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = (int64_t)((int32_t)f32_to_ui32(frs1, RISCV_RM, true));
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_l_s(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_to_i64(frs1, RISCV_RM, true);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_lu_s(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f32_to_ui64(frs1, RISCV_RM, true);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_s_w(CPURISCVState *env, uint64_t rs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    rs1 = i32_to_f32((int32_t)rs1);
+    set_fp_exceptions;
+    return rs1;
+}
+
+uint64_t helper_fcvt_s_wu(CPURISCVState *env, uint64_t rs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    rs1 = ui32_to_f32((uint32_t)rs1);
+    set_fp_exceptions;
+    return rs1;
+}
+
+uint64_t helper_fcvt_s_l(CPURISCVState *env, uint64_t rs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    rs1 = i64_to_f32(rs1);
+    set_fp_exceptions;
+    return rs1;
+}
+
+uint64_t helper_fcvt_s_lu(CPURISCVState *env, uint64_t rs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    rs1 = ui64_to_f32(rs1);
+    set_fp_exceptions;
+    return rs1;
+}
+
+uint64_t helper_fclass_s(CPURISCVState *env, uint64_t frs1)
+{
+    frs1 = f32_classify(frs1);
+    return frs1;
+}
+
+uint64_t helper_fadd_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1, 0x3ff0000000000000ULL, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fsub_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1, 0x3ff0000000000000ULL, frs2 ^ (uint64_t)INT64_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmul_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_mulAdd(frs1, frs2, (frs1 ^ frs2) & (uint64_t)INT64_MIN);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fdiv_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_div(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fsgnj_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = (frs1 &~ INT64_MIN) | (frs2 & INT64_MIN);
+    return frs1;
+}
+
+uint64_t helper_fsgnjn_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = (frs1 &~ INT64_MIN) | ((~frs2) & INT64_MIN);
+    return frs1;
+}
+
+uint64_t helper_fsgnjx_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = frs1 ^ (frs2 & INT64_MIN);
+    return frs1;
+}
+
+uint64_t helper_fmin_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = isNaNF64UI(frs2) || f64_lt_quiet(frs1, frs2) ? frs1 : frs2;
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fmax_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = isNaNF64UI(frs2) || f64_lt_quiet(frs2, frs1) ? frs1 : frs2;
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_s_d(CPURISCVState *env, uint64_t rs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    rs1 = f64_to_f32(rs1);
+    set_fp_exceptions;
+    return rs1;
+}
+
+uint64_t helper_fcvt_d_s(CPURISCVState *env, uint64_t rs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    rs1 = f32_to_f64(rs1);
+    set_fp_exceptions;
+    return rs1;
+}
+
+uint64_t helper_fsqrt_d(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_sqrt(frs1);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fle_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = f64_le(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_flt_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = f64_lt(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_feq_d(CPURISCVState *env, uint64_t frs1, uint64_t frs2)
+{
+    frs1 = f64_eq(frs1, frs2);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_w_d(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = (int64_t)((int32_t)f64_to_i32(frs1, RISCV_RM, true));
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_wu_d(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = (int64_t)((int32_t)f64_to_ui32(frs1, RISCV_RM, true));
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_l_d(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_to_i64(frs1, RISCV_RM, true);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_lu_d(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = f64_to_ui64(frs1, RISCV_RM, true);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_d_w(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = i32_to_f64((int32_t)frs1);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_d_wu(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = ui32_to_f64((uint32_t)frs1);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_d_l(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = i64_to_f64(frs1);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fcvt_d_lu(CPURISCVState *env, uint64_t frs1, uint64_t rm)
+{
+    softfloat_roundingMode = RISCV_RM;
+    frs1 = ui64_to_f64(frs1);
+    set_fp_exceptions;
+    return frs1;
+}
+
+uint64_t helper_fclass_d(CPURISCVState *env, uint64_t frs1)
+{
+    frs1 = f64_classify(frs1);
+    return frs1;
+}
+
+target_ulong helper_mulhsu(CPURISCVState *env, target_ulong arg1,
+                          target_ulong arg2)
+{
+    int64_t a = arg1;
+    uint64_t b = arg2;
+    return (int64_t)((__int128_t)a*b >> 64);
+}
+
+inline void csr_write_helper(CPURISCVState *env, target_ulong val_to_write, target_ulong csrno)
+{
+
+    switch (csrno) {
+        case CSR_COUNT:
+            cpu_riscv_store_count(env, (uint32_t)val_to_write);
+            break;
+        case CSR_COMPARE:
+            cpu_riscv_store_compare(env, (uint32_t)val_to_write);
+            break;
+        case CSR_CYCLE:
+            // DO NOT WRITE TO CSR_CYCLE
+            break;
+        case CSR_FCSR:
+            env->helper_csr[CSR_FFLAGS] = val_to_write & 0x1F;
+            env->helper_csr[CSR_FRM] = (val_to_write >> 5) & 0x7;
+            break;
+        default:
+            env->helper_csr[csrno] = val_to_write;
+            break;
+    }
+}
+
+inline target_ulong csr_read_helper(CPURISCVState *env, target_ulong csrno)
+{
+    switch (csrno) {
+        case CSR_COUNT:
+            return cpu_riscv_get_count(env);
+            break;
+        case CSR_CYCLE:
+            return cpu_riscv_get_cycle(env);
+            break;
+        case CSR_FCSR:
+            return env->helper_csr[CSR_FFLAGS] | (env->helper_csr[CSR_FRM] << 5);
+            break;
+        default:
+            return env->helper_csr[csrno];
+            break;
+    }
+}
+
+target_ulong helper_csrrw(CPURISCVState *env, target_ulong src, target_ulong csr)
+{
+    uint64_t csr_backup = csr_read_helper(env, csr);
+    csr_write_helper(env, src, csr);
+    return csr_backup;
+}
+
+target_ulong helper_csrrs(CPURISCVState *env, target_ulong src, target_ulong csr)
+{
+    uint64_t csr_backup = csr_read_helper(env, csr);
+    csr_write_helper(env, src | csr_backup, csr);
+    return csr_backup;
+}
+
+target_ulong helper_csrrc(CPURISCVState *env, target_ulong src, target_ulong csr) {
+    uint64_t csr_backup = csr_read_helper(env, csr);
+    csr_write_helper(env, (~src) & csr_backup, csr);
+    return csr_backup;
+}
+
+target_ulong helper_sret(CPURISCVState *env)
+{
+    // first handle S/PS stack
+    if (env->helper_csr[CSR_STATUS] & SR_PS) {
+        env->helper_csr[CSR_STATUS] |= SR_S;
+    } else {
+        env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_S);
+    }
+
+    // handle EI/PEI stack
+    if (env->helper_csr[CSR_STATUS] & SR_PEI) {
+        env->helper_csr[CSR_STATUS] |= SR_EI;
+    } else {
+        env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_EI);
+    }
+
+    // finally, return EPC value to set cpu_PC
+    return env->helper_csr[CSR_EPC];
+}
+
+target_ulong helper_scall(CPURISCVState *env, target_ulong bad_pc)
+{
+    env->helper_csr[CSR_CAUSE] = RISCV_EXCP_SCALL;
+    if (env->helper_csr[CSR_STATUS] & SR_S) {
+        env->helper_csr[CSR_STATUS] |= SR_PS;
+    } else {
+        env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_PS);
+    }
+    env->helper_csr[CSR_STATUS] |= SR_S;
+
+    if (env->helper_csr[CSR_STATUS] & SR_EI) {
+        env->helper_csr[CSR_STATUS] |= SR_PEI;
+    } else {
+        env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_PEI);
+    }
+    env->helper_csr[CSR_STATUS] &= ~((uint64_t)SR_EI);
+    env->helper_csr[CSR_EPC] = bad_pc;
+    return env->helper_csr[CSR_EVEC];
+}
+
+#ifndef CONFIG_USER_ONLY
+/* TLB management */
+inline static void cpu_riscv_tlb_flush (CPURISCVState *env, int flush_global)
+{
+    RISCVCPU *cpu = riscv_env_get_cpu(env);
+
+    /* Flush qemu's TLB and discard all shadowed entries.  */
+    tlb_flush(CPU(cpu), flush_global);
+}
+
+
+void helper_tlb_flush(CPURISCVState *env)
+{
+    cpu_riscv_tlb_flush(env, 1);
+}
+
+
+#endif /* !CONFIG_USER_ONLY */
+
+// todo implement for RISC-V?
+/*void helper_wait(CPURISCVState *env)
+{
+    CPUState *cs = CPU(riscv_env_get_cpu(env));
+
+    cs->halted = 1;
+    cpu_reset_interrupt(cs, CPU_INTERRUPT_WAKE);
+    printf("NOT IMPLEMENTED FOR RISCV\n");
+    exit(1);
+}*/
+
+#if !defined(CONFIG_USER_ONLY)
+
+void riscv_cpu_do_unaligned_access(CPUState *cs, target_ulong addr,
+                                   int rw, int is_user, uintptr_t retaddr)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+
+    if (rw & 0x2) {
+        cs->exception_index = RISCV_EXCP_INST_ADDR_MIS;
+    } else if (rw == 0x1) {
+        cs->exception_index = RISCV_EXCP_STORE_ADDR_MIS;
+        env->helper_csr[CSR_BADVADDR] = addr;
+    } else {
+        cs->exception_index = RISCV_EXCP_LOAD_ADDR_MIS;
+        env->helper_csr[CSR_BADVADDR] = addr;
+    }
+    do_raise_exception_err(env, cs->exception_index, 0);
+}
+
+/* called by qemu's softmmu to fill the qemu tlb */
+void tlb_fill(CPUState *cs, target_ulong addr, int is_write, int mmu_idx,
+              uintptr_t retaddr)
+{
+    int ret;
+    ret = riscv_cpu_handle_mmu_fault(cs, addr, is_write, mmu_idx);
+    if (ret) {
+        RISCVCPU *cpu = RISCV_CPU(cs);
+        CPURISCVState *env = &cpu->env;
+        do_raise_exception_err(env, cs->exception_index, retaddr);
+    }
+}
+
+void riscv_cpu_unassigned_access(CPUState *cs, hwaddr addr,
+                                bool is_write, bool is_exec, int unused,
+                                unsigned size)
+{
+    printf("unassigned address was called?\n");
+    printf("with addr: %016lX\n", addr);
+
+    printf("not implemented for riscv\n");
+    exit(1);
+}
+#endif /* !CONFIG_USER_ONLY */
diff --git a/target-riscv/riscv-defs.h b/target-riscv/riscv-defs.h
new file mode 100644
index 0000000..f485a4e
--- /dev/null
+++ b/target-riscv/riscv-defs.h
@@ -0,0 +1,12 @@
+#if !defined (__QEMU_RISCV_DEFS_H__)
+#define __QEMU_RISCV_DEFS_H__
+
+/* Real pages are variable size... */
+#define TARGET_PAGE_BITS 13 // MODIFIED FOR RISCV RV64 8 KiB Pages
+#define RISCV_TLB_MAX 128
+
+#define TARGET_LONG_BITS 64 // this defs TCGv as TCGv_i64 in tcg/tcg-op.h
+#define TARGET_PHYS_ADDR_SPACE_BITS 32
+#define TARGET_VIRT_ADDR_SPACE_BITS 43
+
+#endif /* !defined (__QEMU_RISCV_DEFS_H__) */
diff --git a/target-riscv/translate.c b/target-riscv/translate.c
new file mode 100644
index 0000000..05bd26f
--- /dev/null
+++ b/target-riscv/translate.c
@@ -0,0 +1,2317 @@
+/*
+ *  RISC-V emulation for qemu: main translation routines.
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include "cpu.h"
+#include "disas/disas.h"
+#include "tcg-op.h"
+#include "exec/cpu_ldst.h"
+
+//#define DISABLE_CHAINING_BRANCH
+//#define DISABLE_CHAINING_JAL
+
+#define RISCV_DEBUG_DISAS 0
+
+inline static int csr_regno(int regno);
+
+/* Get regno in our csr reg array from actual csr regno
+ * Mapping:
+ * [
+ *  pos_in_array: regname (real reg number, input as int regno)
+ *    0: sup0 (0x500),
+ *    1: sup1 (0x501),
+ *    2: epc (0x502),
+ *    3: badvaddr (0x503),
+ *    4: ptbr (0x504),
+ *    5: asid (0x505),
+ *    6: count (0x506),
+ *    7: compare (0x507),
+ *    8: evec (0x508),
+ *    9: cause (0x509),
+ *    A: status (0x50A),
+ *    B: hartid (0x50B),
+ *    C: impl (0x50C),
+ *    D: fatc (0x50D),
+ *    E: send_ipi (0x50E),
+ *    F: clear_ipi (0x50F),
+ *   10: cycle (0xC00),
+ *   11: time (0xC01),
+ *   12: instret (0xC02),
+ *   13: fflags (0x1),
+ *   14: frm (0x2),
+ *   15: fcsr (0x3),
+ *   ...
+ *   1E: tohost (0x51E),
+ *   1F: fromhost (0x51F)
+ * ]
+ */
+inline static int csr_regno(int regno)
+{
+    if (regno < 0x4 && regno > 0x0) { //0x1, 0x2, 0x3
+        return regno + 0x12;
+    }
+    if (regno < 0xC00) {
+        // 0x5xx registers
+        return 0xFF & regno;
+    }
+    // 0xC0x registers
+    return (0xF & regno) + 0x10;
+}
+
+
+#define MASK_OP_MAJOR(op)  (op & 0x7F)
+enum {
+    /* rv32i, rv64i, rv32m */
+    OPC_RISC_LUI    = (0x37),
+    OPC_RISC_AUIPC  = (0x17),
+    OPC_RISC_JAL    = (0x6F),
+    OPC_RISC_JALR   = (0x67),
+    OPC_RISC_BRANCH = (0x63),
+    OPC_RISC_LOAD   = (0x03),
+    OPC_RISC_STORE  = (0x23),
+    OPC_RISC_ARITH_IMM  = (0x13),
+    OPC_RISC_ARITH      = (0x33),
+    OPC_RISC_FENCE      = (0x0F),
+    OPC_RISC_SYSTEM     = (0x73),
+
+    /* rv64i, rv64m */
+    OPC_RISC_ARITH_IMM_W = (0x1B),
+    OPC_RISC_ARITH_W = (0x3B),
+
+    /* rv32a, rv64a */
+    OPC_RISC_ATOMIC = (0x2F),
+
+    /* floating point */
+    OPC_RISC_FP_LOAD = (0x7),
+    OPC_RISC_FP_STORE = (0x27),
+    
+    OPC_RISC_FMADD = (0x43),
+    OPC_RISC_FMSUB = (0x47),
+    OPC_RISC_FNMSUB = (0x4B),
+    OPC_RISC_FNMADD = (0x4F),
+
+    OPC_RISC_FP_ARITH = (0x53),
+};
+
+#define MASK_OP_ARITH(op)   (MASK_OP_MAJOR(op) | (op & ((0x7 << 12) | (0x7F << 25))))
+enum {
+    OPC_RISC_ADD   = OPC_RISC_ARITH | (0x0 << 12) | (0x00 << 25),
+    OPC_RISC_SUB   = OPC_RISC_ARITH | (0x0 << 12) | (0x20 << 25),
+    OPC_RISC_SLL   = OPC_RISC_ARITH | (0x1 << 12) | (0x00 << 25),
+    OPC_RISC_SLT   = OPC_RISC_ARITH | (0x2 << 12) | (0x00 << 25),
+    OPC_RISC_SLTU  = OPC_RISC_ARITH | (0x3 << 12) | (0x00 << 25),
+    OPC_RISC_XOR   = OPC_RISC_ARITH | (0x4 << 12) | (0x00 << 25),
+    OPC_RISC_SRL   = OPC_RISC_ARITH | (0x5 << 12) | (0x00 << 25), 
+    OPC_RISC_SRA   = OPC_RISC_ARITH | (0x5 << 12) | (0x20 << 25),
+    OPC_RISC_OR    = OPC_RISC_ARITH | (0x6 << 12) | (0x00 << 25),
+    OPC_RISC_AND   = OPC_RISC_ARITH | (0x7 << 12) | (0x00 << 25),
+
+    /* RV64M */
+    OPC_RISC_MUL    = OPC_RISC_ARITH | (0x0 << 12) | (0x01 << 25),
+    OPC_RISC_MULH   = OPC_RISC_ARITH | (0x1 << 12) | (0x01 << 25),
+    OPC_RISC_MULHSU = OPC_RISC_ARITH | (0x2 << 12) | (0x01 << 25),
+    OPC_RISC_MULHU  = OPC_RISC_ARITH | (0x3 << 12) | (0x01 << 25),
+
+    OPC_RISC_DIV    = OPC_RISC_ARITH | (0x4 << 12) | (0x01 << 25),
+    OPC_RISC_DIVU   = OPC_RISC_ARITH | (0x5 << 12) | (0x01 << 25),
+    OPC_RISC_REM    = OPC_RISC_ARITH | (0x6 << 12) | (0x01 << 25),
+    OPC_RISC_REMU   = OPC_RISC_ARITH | (0x7 << 12) | (0x01 << 25),
+};
+
+
+#define MASK_OP_ARITH_IMM(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_ADDI   = OPC_RISC_ARITH_IMM | (0x0 << 12),
+    OPC_RISC_SLTI   = OPC_RISC_ARITH_IMM | (0x2 << 12),
+    OPC_RISC_SLTIU  = OPC_RISC_ARITH_IMM | (0x3 << 12),
+    OPC_RISC_XORI   = OPC_RISC_ARITH_IMM | (0x4 << 12),
+    OPC_RISC_ORI    = OPC_RISC_ARITH_IMM | (0x6 << 12),
+    OPC_RISC_ANDI   = OPC_RISC_ARITH_IMM | (0x7 << 12),
+    OPC_RISC_SLLI   = OPC_RISC_ARITH_IMM | (0x1 << 12), // additional part of IMM
+    OPC_RISC_SHIFT_RIGHT_I = OPC_RISC_ARITH_IMM | (0x5 << 12) // SRAI, SRLI
+};
+
+#define MASK_OP_BRANCH(op)     (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_BEQ  = OPC_RISC_BRANCH  | (0x0  << 12),
+    OPC_RISC_BNE  = OPC_RISC_BRANCH  | (0x1  << 12),
+    OPC_RISC_BLT  = OPC_RISC_BRANCH  | (0x4  << 12),
+    OPC_RISC_BGE  = OPC_RISC_BRANCH  | (0x5  << 12),
+    OPC_RISC_BLTU = OPC_RISC_BRANCH  | (0x6  << 12),
+    OPC_RISC_BGEU = OPC_RISC_BRANCH  | (0x7  << 12)
+};
+
+#define MASK_OP_ARITH_IMM_W(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_ADDIW   = OPC_RISC_ARITH_IMM_W | (0x0 << 12),
+    OPC_RISC_SLLIW   = OPC_RISC_ARITH_IMM_W | (0x1 << 12), // additional part of IMM
+    OPC_RISC_SHIFT_RIGHT_IW = OPC_RISC_ARITH_IMM_W | (0x5 << 12) // SRAI, SRLI
+};
+
+#define MASK_OP_ARITH_W(op)   (MASK_OP_MAJOR(op) | (op & ((0x7 << 12) | (0x7F << 25))))
+enum {
+    OPC_RISC_ADDW   = OPC_RISC_ARITH_W | (0x0 << 12) | (0x00 << 25),
+    OPC_RISC_SUBW   = OPC_RISC_ARITH_W | (0x0 << 12) | (0x20 << 25),
+    OPC_RISC_SLLW   = OPC_RISC_ARITH_W | (0x1 << 12) | (0x00 << 25),
+    OPC_RISC_SRLW   = OPC_RISC_ARITH_W | (0x5 << 12) | (0x00 << 25), 
+    OPC_RISC_SRAW   = OPC_RISC_ARITH_W | (0x5 << 12) | (0x20 << 25),
+
+    /* RV64M */
+    OPC_RISC_MULW   = OPC_RISC_ARITH_W | (0x0 << 12) | (0x01 << 25),
+    OPC_RISC_DIVW   = OPC_RISC_ARITH_W | (0x4 << 12) | (0x01 << 25),
+    OPC_RISC_DIVUW  = OPC_RISC_ARITH_W | (0x5 << 12) | (0x01 << 25),
+    OPC_RISC_REMW   = OPC_RISC_ARITH_W | (0x6 << 12) | (0x01 << 25),
+    OPC_RISC_REMUW  = OPC_RISC_ARITH_W | (0x7 << 12) | (0x01 << 25),
+};
+
+#define MASK_OP_LOAD(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_LB   = OPC_RISC_LOAD | (0x0 << 12),
+    OPC_RISC_LH   = OPC_RISC_LOAD | (0x1 << 12),
+    OPC_RISC_LW   = OPC_RISC_LOAD | (0x2 << 12),
+    OPC_RISC_LD   = OPC_RISC_LOAD | (0x3 << 12),
+    OPC_RISC_LBU  = OPC_RISC_LOAD | (0x4 << 12),
+    OPC_RISC_LHU  = OPC_RISC_LOAD | (0x5 << 12),
+    OPC_RISC_LWU  = OPC_RISC_LOAD | (0x6 << 12),
+};
+
+#define MASK_OP_STORE(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_SB   = OPC_RISC_STORE | (0x0 << 12),
+    OPC_RISC_SH   = OPC_RISC_STORE | (0x1 << 12),
+    OPC_RISC_SW   = OPC_RISC_STORE | (0x2 << 12),
+    OPC_RISC_SD   = OPC_RISC_STORE | (0x3 << 12),
+};
+
+#define MASK_OP_JALR(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+// no enum since OPC_RISC_JALR is the actual value
+
+#define MASK_OP_ATOMIC(op)   (MASK_OP_MAJOR(op) | (op & ((0x7 << 12) | (0x7F << 25))))
+#define MASK_OP_ATOMIC_NO_AQ_RL(op)   (MASK_OP_MAJOR(op) | (op & ((0x7 << 12) | (0x1F << 27))))
+enum {
+    OPC_RISC_LR_W        = OPC_RISC_ATOMIC | (0x2 << 12) | (0x02 << 27),
+    OPC_RISC_SC_W        = OPC_RISC_ATOMIC | (0x2 << 12) | (0x03 << 27),
+    OPC_RISC_AMOSWAP_W   = OPC_RISC_ATOMIC | (0x2 << 12) | (0x01 << 27),
+    OPC_RISC_AMOADD_W    = OPC_RISC_ATOMIC | (0x2 << 12) | (0x00 << 27),
+    OPC_RISC_AMOXOR_W    = OPC_RISC_ATOMIC | (0x2 << 12) | (0x04 << 27),
+    OPC_RISC_AMOAND_W    = OPC_RISC_ATOMIC | (0x2 << 12) | (0x0C << 27),
+    OPC_RISC_AMOOR_W     = OPC_RISC_ATOMIC | (0x2 << 12) | (0x08 << 27),
+    OPC_RISC_AMOMIN_W    = OPC_RISC_ATOMIC | (0x2 << 12) | (0x10 << 27),
+    OPC_RISC_AMOMAX_W    = OPC_RISC_ATOMIC | (0x2 << 12) | (0x14 << 27),
+    OPC_RISC_AMOMINU_W   = OPC_RISC_ATOMIC | (0x2 << 12) | (0x18 << 27),
+    OPC_RISC_AMOMAXU_W   = OPC_RISC_ATOMIC | (0x2 << 12) | (0x1C << 27),
+
+    OPC_RISC_LR_D        = OPC_RISC_ATOMIC | (0x3 << 12) | (0x02 << 27),
+    OPC_RISC_SC_D        = OPC_RISC_ATOMIC | (0x3 << 12) | (0x03 << 27),
+    OPC_RISC_AMOSWAP_D   = OPC_RISC_ATOMIC | (0x3 << 12) | (0x01 << 27),
+    OPC_RISC_AMOADD_D    = OPC_RISC_ATOMIC | (0x3 << 12) | (0x00 << 27),
+    OPC_RISC_AMOXOR_D    = OPC_RISC_ATOMIC | (0x3 << 12) | (0x04 << 27),
+    OPC_RISC_AMOAND_D    = OPC_RISC_ATOMIC | (0x3 << 12) | (0x0C << 27),
+    OPC_RISC_AMOOR_D     = OPC_RISC_ATOMIC | (0x3 << 12) | (0x08 << 27),
+    OPC_RISC_AMOMIN_D    = OPC_RISC_ATOMIC | (0x3 << 12) | (0x10 << 27),
+    OPC_RISC_AMOMAX_D    = OPC_RISC_ATOMIC | (0x3 << 12) | (0x14 << 27),
+    OPC_RISC_AMOMINU_D   = OPC_RISC_ATOMIC | (0x3 << 12) | (0x18 << 27),
+    OPC_RISC_AMOMAXU_D   = OPC_RISC_ATOMIC | (0x3 << 12) | (0x1C << 27),
+};
+
+#define MASK_OP_SYSTEM(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_SCALL       = OPC_RISC_SYSTEM | (0x0 << 12),
+    OPC_RISC_SBREAK      = OPC_RISC_SYSTEM | (0x0 << 12),
+    OPC_RISC_SRET        = OPC_RISC_SYSTEM | (0x0 << 12),
+    OPC_RISC_CSRRW       = OPC_RISC_SYSTEM | (0x1 << 12),
+    OPC_RISC_CSRRS       = OPC_RISC_SYSTEM | (0x2 << 12),
+    OPC_RISC_CSRRC       = OPC_RISC_SYSTEM | (0x3 << 12),
+    OPC_RISC_CSRRWI      = OPC_RISC_SYSTEM | (0x5 << 12),
+    OPC_RISC_CSRRSI      = OPC_RISC_SYSTEM | (0x6 << 12),
+    OPC_RISC_CSRRCI      = OPC_RISC_SYSTEM | (0x7 << 12),
+};
+
+#define MASK_OP_FP_LOAD(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_FLW   = OPC_RISC_FP_LOAD | (0x2 << 12),
+    OPC_RISC_FLD   = OPC_RISC_FP_LOAD | (0x3 << 12),
+};
+
+#define MASK_OP_FP_STORE(op)   (MASK_OP_MAJOR(op) | (op & (0x7 << 12)))
+enum {
+    OPC_RISC_FSW   = OPC_RISC_FP_STORE | (0x2 << 12),
+    OPC_RISC_FSD   = OPC_RISC_FP_STORE | (0x3 << 12),
+};
+
+#define MASK_OP_FP_FMADD(op)   (MASK_OP_MAJOR(op) | (op & (0x3 << 25)))
+enum {
+    OPC_RISC_FMADD_S = OPC_RISC_FMADD | (0x0 << 25),
+    OPC_RISC_FMADD_D = OPC_RISC_FMADD | (0x1 << 25),
+};
+
+#define MASK_OP_FP_FMSUB(op)   (MASK_OP_MAJOR(op) | (op & (0x3 << 25)))
+enum {
+    OPC_RISC_FMSUB_S = OPC_RISC_FMSUB | (0x0 << 25),
+    OPC_RISC_FMSUB_D = OPC_RISC_FMSUB | (0x1 << 25),
+};
+
+#define MASK_OP_FP_FNMADD(op)   (MASK_OP_MAJOR(op) | (op & (0x3 << 25)))
+enum {
+    OPC_RISC_FNMADD_S = OPC_RISC_FNMADD | (0x0 << 25),
+    OPC_RISC_FNMADD_D = OPC_RISC_FNMADD | (0x1 << 25),
+};
+
+#define MASK_OP_FP_FNMSUB(op)   (MASK_OP_MAJOR(op) | (op & (0x3 << 25)))
+enum {
+    OPC_RISC_FNMSUB_S = OPC_RISC_FNMSUB | (0x0 << 25),
+    OPC_RISC_FNMSUB_D = OPC_RISC_FNMSUB | (0x1 << 25),
+};
+
+#define MASK_OP_FP_ARITH(op)   (MASK_OP_MAJOR(op) | (op & (0x7F << 25)))
+enum {
+    // float
+    OPC_RISC_FADD_S    = OPC_RISC_FP_ARITH | (0x0 << 25),
+    OPC_RISC_FSUB_S    = OPC_RISC_FP_ARITH | (0x4 << 25),
+    OPC_RISC_FMUL_S    = OPC_RISC_FP_ARITH | (0x8 << 25),
+    OPC_RISC_FDIV_S    = OPC_RISC_FP_ARITH | (0xC << 25),
+
+    OPC_RISC_FSGNJ_S   = OPC_RISC_FP_ARITH | (0x10 << 25),
+    OPC_RISC_FSGNJN_S  = OPC_RISC_FP_ARITH | (0x10 << 25),
+    OPC_RISC_FSGNJX_S  = OPC_RISC_FP_ARITH | (0x10 << 25),
+
+    OPC_RISC_FMIN_S    = OPC_RISC_FP_ARITH | (0x14 << 25),
+    OPC_RISC_FMAX_S    = OPC_RISC_FP_ARITH | (0x14 << 25),
+
+    OPC_RISC_FSQRT_S   = OPC_RISC_FP_ARITH | (0x2C << 25),
+
+    OPC_RISC_FEQ_S     = OPC_RISC_FP_ARITH | (0x50 << 25),
+    OPC_RISC_FLT_S     = OPC_RISC_FP_ARITH | (0x50 << 25),
+    OPC_RISC_FLE_S     = OPC_RISC_FP_ARITH | (0x50 << 25),
+
+    OPC_RISC_FCVT_W_S  = OPC_RISC_FP_ARITH | (0x60 << 25),
+    OPC_RISC_FCVT_WU_S = OPC_RISC_FP_ARITH | (0x60 << 25),
+    OPC_RISC_FCVT_L_S  = OPC_RISC_FP_ARITH | (0x60 << 25),
+    OPC_RISC_FCVT_LU_S = OPC_RISC_FP_ARITH | (0x60 << 25),
+
+    OPC_RISC_FCVT_S_W  = OPC_RISC_FP_ARITH | (0x68 << 25),
+    OPC_RISC_FCVT_S_WU = OPC_RISC_FP_ARITH | (0x68 << 25),
+    OPC_RISC_FCVT_S_L  = OPC_RISC_FP_ARITH | (0x68 << 25),
+    OPC_RISC_FCVT_S_LU = OPC_RISC_FP_ARITH | (0x68 << 25),
+
+    OPC_RISC_FMV_X_S   = OPC_RISC_FP_ARITH | (0x70 << 25),
+    OPC_RISC_FCLASS_S  = OPC_RISC_FP_ARITH | (0x70 << 25),
+
+    OPC_RISC_FMV_S_X   = OPC_RISC_FP_ARITH | (0x78 << 25),
+
+    // double
+    OPC_RISC_FADD_D    = OPC_RISC_FP_ARITH | (0x1 << 25),
+    OPC_RISC_FSUB_D    = OPC_RISC_FP_ARITH | (0x5 << 25),
+    OPC_RISC_FMUL_D    = OPC_RISC_FP_ARITH | (0x9 << 25),
+    OPC_RISC_FDIV_D    = OPC_RISC_FP_ARITH | (0xD << 25),
+
+    OPC_RISC_FSGNJ_D   = OPC_RISC_FP_ARITH | (0x11 << 25),
+    OPC_RISC_FSGNJN_D  = OPC_RISC_FP_ARITH | (0x11 << 25),
+    OPC_RISC_FSGNJX_D  = OPC_RISC_FP_ARITH | (0x11 << 25),
+
+    OPC_RISC_FMIN_D    = OPC_RISC_FP_ARITH | (0x15 << 25),
+    OPC_RISC_FMAX_D    = OPC_RISC_FP_ARITH | (0x15 << 25),
+
+    OPC_RISC_FCVT_S_D = OPC_RISC_FP_ARITH | (0x20 << 25),
+
+    OPC_RISC_FCVT_D_S = OPC_RISC_FP_ARITH | (0x21 << 25),
+
+    OPC_RISC_FSQRT_D   = OPC_RISC_FP_ARITH | (0x2D << 25),
+
+    OPC_RISC_FEQ_D     = OPC_RISC_FP_ARITH | (0x51 << 25),
+    OPC_RISC_FLT_D     = OPC_RISC_FP_ARITH | (0x51 << 25),
+    OPC_RISC_FLE_D     = OPC_RISC_FP_ARITH | (0x51 << 25),
+
+    OPC_RISC_FCVT_W_D  = OPC_RISC_FP_ARITH | (0x61 << 25),
+    OPC_RISC_FCVT_WU_D = OPC_RISC_FP_ARITH | (0x61 << 25),
+    OPC_RISC_FCVT_L_D  = OPC_RISC_FP_ARITH | (0x61 << 25),
+    OPC_RISC_FCVT_LU_D = OPC_RISC_FP_ARITH | (0x61 << 25),
+
+    OPC_RISC_FCVT_D_W  = OPC_RISC_FP_ARITH | (0x69 << 25),
+    OPC_RISC_FCVT_D_WU = OPC_RISC_FP_ARITH | (0x69 << 25),
+    OPC_RISC_FCVT_D_L  = OPC_RISC_FP_ARITH | (0x69 << 25),
+    OPC_RISC_FCVT_D_LU = OPC_RISC_FP_ARITH | (0x69 << 25),
+
+    OPC_RISC_FMV_X_D   = OPC_RISC_FP_ARITH | (0x71 << 25),
+    OPC_RISC_FCLASS_D  = OPC_RISC_FP_ARITH | (0x71 << 25),
+
+    OPC_RISC_FMV_D_X   = OPC_RISC_FP_ARITH | (0x79 << 25),
+};
+
+
+/* global register indices */
+static TCGv_ptr cpu_env;
+static TCGv cpu_gpr[32], cpu_PC, cpu_fpr[32];
+
+#include "exec/gen-icount.h"
+
+typedef struct DisasContext {
+    struct TranslationBlock *tb;
+    target_ulong pc;
+    uint32_t opcode;
+    int singlestep_enabled;
+    /* Routine used to access memory */
+    int mem_idx;
+    int bstate;
+} DisasContext;
+
+static inline void kill_unknown(DisasContext *ctx, int excp);
+
+enum {
+    BS_NONE     = 0, // When seen outside of translation while loop, indicates
+                     // need to exit tb due to end of page.
+    BS_STOP     = 1, // Need to exit tb for syscall, sret, etc.
+    BS_BRANCH   = 2, // Need to exit tb for branch, jal, etc.
+};
+
+static const char * const regnames[] = {
+    "zero", "ra", "s0", "s1",  "s2",  "s3",  "s4",  "s5",
+    "s6",   "s7", "s8", "s9", "s10", "s11",  "sp",  "tp",
+    "v0",   "v1", "a0", "a1",  "a2",  "a3",  "a4",  "a5",
+    "a6",   "a7", "t0", "t1",  "t2",  "t3",  "t4",  "gp"
+};
+
+static const char * const cs_regnames[] = {
+    "sup0", "sup1", "epc", "badvaddr", "ptbr", "asid", "count", "compare",
+    "evec", "cause", "status", "hartid", "impl", "fatc", "send_ipi", "clear_ipi",
+    "cycle", "time", "instret", "fflags", "frm", "fcsr", "junk", "junk",
+    "junk", "junk", "junk", "junk", "junk", "junk", "tohost", "fromhost"
+};
+
+static const char * const fpr_regnames[] = {
+    "f0", "f1", "f2", "f3", "f4", "f5", "f6", "f7",
+    "f8", "f9", "f10", "f11", "f12", "f13", "f14", "f15",
+    "f16", "f17", "f18", "f19", "f20", "f21", "f22", "f23",
+    "f24", "f25", "f26", "f27", "f28", "f29", "f30", "f31",
+};
+
+#define RISCV_DEBUG(fmt, ...)                                                  \
+    do {                                                                      \
+        if (RISCV_DEBUG_DISAS) {                                               \
+            qemu_log_mask(CPU_LOG_TB_IN_ASM,                                  \
+                          TARGET_FMT_lx ": %08x " fmt "\n",                   \
+                          ctx->pc, ctx->opcode , ## __VA_ARGS__);             \
+        }                                                                     \
+    } while (0)
+
+#define LOG_DISAS(...)                                                        \
+    do {                                                                      \
+        if (RISCV_DEBUG_DISAS) {                                               \
+            qemu_log_mask(CPU_LOG_TB_IN_ASM, ## __VA_ARGS__);                 \
+        }                                                                     \
+    } while (0)
+
+static inline void generate_exception (DisasContext *ctx, int excp)
+{
+    tcg_gen_movi_tl(cpu_PC, ctx->pc);
+    TCGv_i32 helper_tmp = tcg_const_i32(excp);
+    gen_helper_raise_exception(cpu_env, helper_tmp);
+    tcg_temp_free_i32(helper_tmp);
+}
+
+// unknown instruction / fp disabled
+static inline void kill_unknown(DisasContext *ctx, int excp) {
+    generate_exception(ctx, excp);
+    ctx->bstate = BS_STOP;
+}
+
+static inline void gen_goto_tb(DisasContext *ctx, int n, target_ulong dest)
+{
+    TranslationBlock *tb;
+    tb = ctx->tb;
+    if ((tb->pc & TARGET_PAGE_MASK) == (dest & TARGET_PAGE_MASK)) {
+        // we only allow direct chaining when the jump is to the same page
+        // otherwise, we could produce incorrect chains when address spaces
+        // change. see
+        // http://lists.gnu.org/archive/html/qemu-devel/2007-06/msg00213.html
+        tcg_gen_goto_tb(n);
+        tcg_gen_movi_tl(cpu_PC, dest);
+        tcg_gen_exit_tb((uintptr_t)tb + n);
+    } else {
+        tcg_gen_movi_tl(cpu_PC, dest);
+        tcg_gen_exit_tb(0);
+    }
+}
+
+/* Wrapper for getting reg values - need to check of reg is zero since 
+ * cpu_gpr[0] is not actually allocated 
+ */
+static inline void gen_get_gpr (TCGv t, int reg_num)
+{
+    if (reg_num == 0) {
+        tcg_gen_movi_tl(t, 0);
+    } else {
+        tcg_gen_mov_tl(t, cpu_gpr[reg_num]);
+    }
+}
+
+/* Wrapper for setting reg values - need to check of reg is zero since 
+ * cpu_gpr[0] is not actually allocated. this is more for safety purposes,
+ * since we usually avoid calling the OP_TYPE_gen function if we see a write to 
+ * $zero
+ */
+static inline void gen_set_gpr (int reg_num_dst, TCGv t)
+{
+    if (reg_num_dst != 0) {
+        tcg_gen_mov_tl(cpu_gpr[reg_num_dst], t);
+    }
+}
+
+inline static void gen_arith(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int rs2)
+{
+    TCGv source1, source2;
+
+    source1 = tcg_temp_new();
+    source2 = tcg_temp_new();
+
+    gen_get_gpr(source1, rs1);
+    gen_get_gpr(source2, rs2);
+
+    switch (opc) {
+
+    case OPC_RISC_ADD:
+        tcg_gen_add_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_SUB:
+        tcg_gen_sub_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_SLL:
+        tcg_gen_andi_tl(source2, source2, 0x3F);
+        tcg_gen_shl_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_SLT:
+        tcg_gen_setcond_tl(TCG_COND_LT, source1, source1, source2);
+        break;
+    case OPC_RISC_SLTU:
+        tcg_gen_setcond_tl(TCG_COND_LTU, source1, source1, source2);
+        break;
+    case OPC_RISC_XOR:
+        tcg_gen_xor_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_SRL:
+        tcg_gen_andi_tl(source2, source2, 0x3F);
+        tcg_gen_shr_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_SRA:
+        tcg_gen_andi_tl(source2, source2, 0x3F);
+        tcg_gen_sar_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_OR:
+        tcg_gen_or_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_AND:
+        tcg_gen_and_tl(source1, source1, source2);
+        break;
+    case OPC_RISC_MUL:
+        tcg_gen_muls2_tl(source1, source2, source1, source2);
+        break;
+    case OPC_RISC_MULH:
+        tcg_gen_muls2_tl(source2, source1, source1, source2);
+        break;
+    case OPC_RISC_MULHSU:
+        gen_helper_mulhsu(source1, cpu_env, source1, source2);
+        break;
+    case OPC_RISC_MULHU:
+        tcg_gen_mulu2_tl(source2, source1, source1, source2);
+        break;
+    case OPC_RISC_DIV:
+        {
+            TCGv spec_source1, spec_source2;
+            TCGv cond1, cond2;
+            int handle_zero = gen_new_label();
+            int handle_overflow = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+            cond1 = tcg_temp_local_new();
+            cond2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // now, use temp reg to check if both overflow conditions satisfied
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond2, spec_source2, 0xFFFFFFFFFFFFFFFF); // divisor = -1
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond1, spec_source1, 0x8000000000000000);
+            tcg_gen_and_tl(cond1, cond1, cond2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, cond1, 1, handle_overflow);
+            // normal case
+            tcg_gen_div_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_movi_tl(spec_source1, -1);
+            tcg_gen_br(done);
+            // special overflow case
+            gen_set_label(handle_overflow);
+            tcg_gen_movi_tl(spec_source1, 0x8000000000000000); 
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+        }                
+        break;
+    case OPC_RISC_DIVU:
+        {
+            TCGv spec_source1, spec_source2;
+            int handle_zero = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // normal case
+            tcg_gen_divu_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_movi_tl(spec_source1, -1);
+            tcg_gen_br(done);
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+        }                
+        break;
+    case OPC_RISC_REM:
+        {
+            TCGv spec_source1, spec_source2;
+            TCGv cond1, cond2;
+            int handle_zero = gen_new_label();
+            int handle_overflow = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+            cond1 = tcg_temp_local_new();
+            cond2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // now, use temp reg to check if both overflow conditions satisfied
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond2, spec_source2, 0xFFFFFFFFFFFFFFFF); // divisor = -1
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond1, spec_source1, 0x8000000000000000);
+            tcg_gen_and_tl(cond1, cond1, cond2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, cond1, 1, handle_overflow);
+            // normal case
+            tcg_gen_rem_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_mov_tl(spec_source1, spec_source1); // even though it's a nop, just for clarity
+            tcg_gen_br(done);
+            // special overflow case
+            gen_set_label(handle_overflow);
+            tcg_gen_movi_tl(spec_source1, 0); 
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+        }                
+        break;
+    case OPC_RISC_REMU:
+        {
+            TCGv spec_source1, spec_source2;
+            int handle_zero = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // normal case
+            tcg_gen_remu_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_mov_tl(spec_source1, spec_source1); // even though it's a nop, just for clarity
+            tcg_gen_br(done);
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+        }                
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+
+    // set and free
+    gen_set_gpr(rd, source1);
+    tcg_temp_free(source1);
+    tcg_temp_free(source2);
+}
+
+/* lower 12 bits of imm are valid */
+inline static void gen_arith_imm(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int16_t imm)
+{
+    TCGv source1;
+    source1 = tcg_temp_new();
+    gen_get_gpr(source1, rs1);
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+
+    switch (opc) {
+    case OPC_RISC_ADDI:
+        tcg_gen_addi_tl(source1, source1, uimm);
+        break;
+    case OPC_RISC_SLTI:
+        tcg_gen_setcondi_tl(TCG_COND_LT, source1, source1, uimm);
+        break;
+    case OPC_RISC_SLTIU:
+        tcg_gen_setcondi_tl(TCG_COND_LTU, source1, source1, uimm);
+        break;
+    case OPC_RISC_XORI:
+        tcg_gen_xori_tl(source1, source1, uimm);
+        break;
+    case OPC_RISC_ORI:
+        tcg_gen_ori_tl(source1, source1, uimm);
+        break;
+    case OPC_RISC_ANDI:
+        tcg_gen_andi_tl(source1, source1, uimm);
+        break;
+    case OPC_RISC_SLLI: // TODO: add immediate upper bits check?
+        tcg_gen_shli_tl(source1, source1, uimm);
+        break;
+    case OPC_RISC_SHIFT_RIGHT_I: // SRLI, SRAI, TODO: upper bits check
+        // differentiate on IMM
+        if (uimm & 0x400) {
+            // SRAI
+            tcg_gen_sari_tl(source1, source1, uimm ^ 0x400);
+        } else {
+            tcg_gen_shri_tl(source1, source1, uimm);
+        }
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+
+    gen_set_gpr(rd, source1);
+    tcg_temp_free(source1);
+}
+
+
+
+/* lower 12 bits of imm are valid */
+inline static void gen_arith_imm_w(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int16_t imm)
+{
+    TCGv source1;
+    source1 = tcg_temp_new();
+    gen_get_gpr(source1, rs1);
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+
+    switch (opc) {
+    case OPC_RISC_ADDIW:
+        tcg_gen_addi_tl(source1, source1, uimm);
+        tcg_gen_ext32s_tl(source1, source1);
+        break;
+    case OPC_RISC_SLLIW: // TODO: add immediate upper bits check?
+        tcg_gen_shli_tl(source1, source1, uimm);
+        tcg_gen_ext32s_tl(source1, source1);
+        break;
+    case OPC_RISC_SHIFT_RIGHT_IW: // SRLIW, SRAIW, TODO: upper bits check
+        // differentiate on IMM
+        if (uimm & 0x400) {
+            // SRAI
+            // first, trick to get it to act like working on 32 bits:
+            tcg_gen_shli_tl(source1, source1, 32);
+            // now shift back to the right by shamt + 32 to get proper upper 
+            // bits filling
+            tcg_gen_sari_tl(source1, source1, (uimm ^ 0x400) + 32);
+            tcg_gen_ext32s_tl(source1, source1);
+        } else {
+            // first, trick to get it to act like working on 32 bits (get rid 
+            // of upper 32):
+            tcg_gen_shli_tl(source1, source1, 32);
+            // now shift back to the right by shamt + 32 to get proper upper 
+            // bits filling
+            tcg_gen_shri_tl(source1, source1, uimm + 32);
+            tcg_gen_ext32s_tl(source1, source1);
+        }
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    gen_set_gpr(rd, source1);
+    tcg_temp_free(source1);
+}
+
+inline static void gen_arith_w(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int rs2)
+{
+    TCGv source1, source2;
+    source1 = tcg_temp_new();
+    source2 = tcg_temp_new();
+    gen_get_gpr(source1, rs1);
+    gen_get_gpr(source2, rs2);
+
+    switch (opc) {
+    case OPC_RISC_ADDW:
+        tcg_gen_add_tl(source1, source1, source2);
+        tcg_gen_ext32s_tl(source1, source1);
+        break;
+    case OPC_RISC_SUBW:
+        tcg_gen_sub_tl(source1, source1, source2);
+        tcg_gen_ext32s_tl(source1, source1);
+        break;
+    case OPC_RISC_SLLW:
+        tcg_gen_andi_tl(source2, source2, 0x1F);
+        tcg_gen_shl_tl(source1, source1, source2);
+        tcg_gen_ext32s_tl(source1, source1);
+        break;
+    case OPC_RISC_SRLW:
+        tcg_gen_andi_tl(source1, source1, 0x00000000FFFFFFFFLL); // clear upper 32
+        tcg_gen_andi_tl(source2, source2, 0x1F);
+        tcg_gen_shr_tl(source1, source1, source2); // do actual right shift
+        tcg_gen_ext32s_tl(source1, source1); // sign ext
+        break;
+    case OPC_RISC_SRAW:
+        // first, trick to get it to act like working on 32 bits (get rid of 
+        // upper 32)
+        tcg_gen_shli_tl(source1, source1, 32); // clear upper 32
+        tcg_gen_sari_tl(source1, source1, 32); // smear the sign bit into upper 32
+        tcg_gen_andi_tl(source2, source2, 0x1F);
+        tcg_gen_sar_tl(source1, source1, source2); // do the actual right shift
+        tcg_gen_ext32s_tl(source1, source1); // sign ext
+        break;
+    case OPC_RISC_MULW:
+        tcg_gen_muls2_tl(source1, source2, source1, source2);
+        tcg_gen_ext32s_tl(source1, source1);
+        break;
+    case OPC_RISC_DIVW:
+        {
+            TCGv spec_source1, spec_source2;
+            TCGv cond1, cond2;
+            int handle_zero = gen_new_label();
+            int handle_overflow = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+            cond1 = tcg_temp_local_new();
+            cond2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_ext32s_tl(spec_source1, spec_source1);
+            tcg_gen_ext32s_tl(spec_source2, spec_source2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // now, use temp reg to check if both overflow conditions satisfied
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond2, spec_source2, 0xFFFFFFFFFFFFFFFF); // divisor = -1
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond1, spec_source1, 0x8000000000000000);
+            tcg_gen_and_tl(cond1, cond1, cond2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, cond1, 1, handle_overflow);
+            // normal case
+            tcg_gen_div_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_movi_tl(spec_source1, -1);
+            tcg_gen_br(done);
+            // special overflow case
+            gen_set_label(handle_overflow);
+            tcg_gen_movi_tl(spec_source1, 0x8000000000000000); 
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+            tcg_gen_ext32s_tl(source1, source1);
+        }                
+        break;
+    case OPC_RISC_DIVUW:
+        {
+            TCGv spec_source1, spec_source2;
+            int handle_zero = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_ext32u_tl(spec_source1, spec_source1);
+            tcg_gen_ext32u_tl(spec_source2, spec_source2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // normal case
+            tcg_gen_divu_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_movi_tl(spec_source1, -1);
+            tcg_gen_br(done);
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+            tcg_gen_ext32s_tl(source1, source1);
+        }                
+        break;
+    case OPC_RISC_REMW:
+        {
+            TCGv spec_source1, spec_source2;
+            TCGv cond1, cond2;
+            int handle_zero = gen_new_label();
+            int handle_overflow = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+            cond1 = tcg_temp_local_new();
+            cond2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_ext32s_tl(spec_source1, spec_source1);
+            tcg_gen_ext32s_tl(spec_source2, spec_source2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+
+            // now, use temp reg to check if both overflow conditions satisfied
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond2, spec_source2, 0xFFFFFFFFFFFFFFFF); // divisor = -1
+            tcg_gen_setcondi_tl(TCG_COND_EQ, cond1, spec_source1, 0x8000000000000000);
+            tcg_gen_and_tl(cond1, cond1, cond2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, cond1, 1, handle_overflow);
+            // normal case
+            tcg_gen_rem_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_mov_tl(spec_source1, spec_source1); // even though it's a nop, just for clarity
+            tcg_gen_br(done);
+            // special overflow case
+            gen_set_label(handle_overflow);
+            tcg_gen_movi_tl(spec_source1, 0); 
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+            tcg_gen_ext32s_tl(source1, source1);
+        }                
+        break;
+    case OPC_RISC_REMUW:
+        {
+            TCGv spec_source1, spec_source2;
+            int handle_zero = gen_new_label();
+            int done = gen_new_label();
+            spec_source1 = tcg_temp_local_new();
+            spec_source2 = tcg_temp_local_new();
+
+            gen_get_gpr(spec_source1, rs1);
+            gen_get_gpr(spec_source2, rs2);
+            tcg_gen_ext32u_tl(spec_source1, spec_source1);
+            tcg_gen_ext32u_tl(spec_source2, spec_source2);
+
+            tcg_gen_brcondi_tl(TCG_COND_EQ, spec_source2, 0x0, handle_zero);
+// normal case
+            tcg_gen_remu_tl(spec_source1, spec_source1, spec_source2);
+            tcg_gen_br(done);
+            // special zero case
+            gen_set_label(handle_zero);
+            tcg_gen_mov_tl(spec_source1, spec_source1); // even though it's a nop, just for clarity
+            tcg_gen_br(done);
+            // done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, spec_source1);
+            tcg_temp_free(spec_source1);
+            tcg_temp_free(spec_source2);
+            tcg_gen_ext32s_tl(source1, source1);
+        }                
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    gen_set_gpr(rd, source1);
+    tcg_temp_free(source1);
+    tcg_temp_free(source2);
+}
+
+inline static void gen_branch(DisasContext *ctx, uint32_t opc, 
+                       int rs1, int rs2, int16_t bimm) {
+
+    int l = gen_new_label();
+    TCGv source1, source2;
+    source1 = tcg_temp_new();
+    source2 = tcg_temp_new();
+    gen_get_gpr(source1, rs1);
+    gen_get_gpr(source2, rs2);
+    target_ulong ubimm = (target_long)bimm; /* sign ext 16->64 bits */
+
+    switch (opc) {
+    case OPC_RISC_BEQ:
+        tcg_gen_brcond_tl(TCG_COND_EQ, source1, source2, l);
+        break;
+    case OPC_RISC_BNE:
+        tcg_gen_brcond_tl(TCG_COND_NE, source1, source2, l);
+        break;
+    case OPC_RISC_BLT:
+        tcg_gen_brcond_tl(TCG_COND_LT, source1, source2, l);
+        break;
+    case OPC_RISC_BGE:
+        tcg_gen_brcond_tl(TCG_COND_GE, source1, source2, l);
+        break;
+    case OPC_RISC_BLTU:
+        tcg_gen_brcond_tl(TCG_COND_LTU, source1, source2, l);
+        break;
+    case OPC_RISC_BGEU:
+        tcg_gen_brcond_tl(TCG_COND_GEU, source1, source2, l);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+
+#ifdef DISABLE_CHAINING_BRANCH
+    tcg_gen_movi_tl(cpu_PC, ctx->pc + 4);
+    tcg_gen_exit_tb(0);
+#else
+    gen_goto_tb(ctx, 1, ctx->pc + 4); // must use this for safety
+#endif
+    gen_set_label(l); // branch taken
+#ifdef DISABLE_CHAINING_BRANCH
+    tcg_gen_movi_tl(cpu_PC, ctx->pc + ubimm);
+    tcg_gen_exit_tb(0);
+#else
+    gen_goto_tb(ctx, 0, ctx->pc + ubimm); // must use this for safety
+#endif
+    ctx->bstate = BS_BRANCH;
+}
+
+inline static void gen_load(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int16_t imm)
+{
+
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+
+    TCGv t0 = tcg_temp_new();
+    gen_get_gpr(t0, rs1);
+    tcg_gen_addi_tl(t0, t0, uimm); // 
+    
+    switch (opc) {
+
+    case OPC_RISC_LB:
+        tcg_gen_qemu_ld8s(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_LH:
+        tcg_gen_qemu_ld16s(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_LW:
+        tcg_gen_qemu_ld32s(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_LD:
+        tcg_gen_qemu_ld64(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_LBU:
+        tcg_gen_qemu_ld8u(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_LHU:
+        tcg_gen_qemu_ld16u(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_LWU:
+        tcg_gen_qemu_ld32u(t0, t0, ctx->mem_idx);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+
+    gen_set_gpr(rd, t0);
+    tcg_temp_free(t0);
+}
+
+
+inline static void gen_store(DisasContext *ctx, uint32_t opc, 
+                      int rs1, int rs2, int16_t imm)
+{
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+
+    TCGv t0 = tcg_temp_new();
+    TCGv dat = tcg_temp_new();
+    gen_get_gpr(t0, rs1);
+    tcg_gen_addi_tl(t0, t0, uimm);
+    gen_get_gpr(dat, rs2);
+
+    switch (opc) {
+
+    case OPC_RISC_SB:
+        tcg_gen_qemu_st8(dat, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_SH:
+        tcg_gen_qemu_st16(dat, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_SW:
+        tcg_gen_qemu_st32(dat, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_SD:
+        tcg_gen_qemu_st64(dat, t0, ctx->mem_idx);
+        break;
+
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+
+    tcg_temp_free(t0);
+    tcg_temp_free(dat);
+}
+
+inline static void gen_jalr(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int16_t imm)
+{
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+    TCGv t0, t1;
+    t0 = tcg_temp_new();
+    t1 = tcg_temp_new();
+
+    switch (opc) {
+    
+    case OPC_RISC_JALR: // CANNOT HAVE CHAINING WITH JALR
+        gen_get_gpr(t0, rs1);
+        tcg_gen_addi_tl(t0, t0, uimm);
+        tcg_gen_andi_tl(t0, t0, 0xFFFFFFFFFFFFFFFEll);
+
+        // store pc+4 to rd as necessary
+        tcg_gen_movi_tl(t1, 4);
+        tcg_gen_addi_tl(t1, t1, ctx->pc);
+        gen_set_gpr(rd, t1);
+
+        tcg_gen_mov_tl(cpu_PC, t0);
+        tcg_gen_exit_tb(0); // NO CHAINING FOR JALR
+        ctx->bstate = BS_BRANCH;
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+    tcg_temp_free(t0);
+    tcg_temp_free(t1);
+}
+
+inline static void gen_atomic(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int rs2)
+{
+    // TODO: handle aq, rl bits? - for now just get rid of them:
+    opc = MASK_OP_ATOMIC_NO_AQ_RL(opc);
+
+    TCGv source1, source2, dat;
+
+    source1 = tcg_temp_new();
+    source2 = tcg_temp_new();
+    dat = tcg_temp_new();
+
+    gen_get_gpr(source1, rs1);
+    gen_get_gpr(source2, rs2);
+
+    switch (opc) {
+        // all currently implemented as non-atomics
+    case OPC_RISC_LR_W:
+        tcg_gen_qemu_ld32s(source1, source1, ctx->mem_idx);
+        break;
+    case OPC_RISC_SC_W:
+        tcg_gen_qemu_st32(source2, source1, ctx->mem_idx);
+        tcg_gen_movi_tl(source1, 0);
+        break;
+    case OPC_RISC_AMOSWAP_W:
+        tcg_gen_qemu_ld32s(dat, source1, ctx->mem_idx);
+        tcg_gen_qemu_st32(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOADD_W:   
+        tcg_gen_qemu_ld32s(dat, source1, ctx->mem_idx);
+        tcg_gen_add_tl(source2, dat, source2);
+        tcg_gen_qemu_st32(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOXOR_W:
+        tcg_gen_qemu_ld32s(dat, source1, ctx->mem_idx);
+        tcg_gen_xor_tl(source2, dat, source2);
+        tcg_gen_qemu_st32(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOAND_W:
+        tcg_gen_qemu_ld32s(dat, source1, ctx->mem_idx);
+        tcg_gen_and_tl(source2, dat, source2);
+        tcg_gen_qemu_st32(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOOR_W:
+        tcg_gen_qemu_ld32s(dat, source1, ctx->mem_idx);
+        tcg_gen_or_tl(source2, dat, source2); 
+        tcg_gen_qemu_st32(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOMIN_W:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld32s(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_LT, dat_l, source2_l, j);
+            tcg_gen_qemu_st32(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st32(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_AMOMAX_W:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld32s(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_GT, dat_l, source2_l, j);
+            tcg_gen_qemu_st32(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st32(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_AMOMINU_W:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld32s(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_LTU, dat_l, source2_l, j);
+            tcg_gen_qemu_st32(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st32(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_AMOMAXU_W:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld32s(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_GTU, dat_l, source2_l, j);
+            tcg_gen_qemu_st32(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st32(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_LR_D:
+        tcg_gen_qemu_ld64(source1, source1, ctx->mem_idx);
+        break;
+    case OPC_RISC_SC_D:       
+        tcg_gen_qemu_st64(source2, source1, ctx->mem_idx);
+        tcg_gen_movi_tl(source1, 0); // assume always success
+        break;
+    case OPC_RISC_AMOSWAP_D:
+        tcg_gen_qemu_ld64(dat, source1, ctx->mem_idx);
+        tcg_gen_qemu_st64(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOADD_D:
+        tcg_gen_qemu_ld64(dat, source1, ctx->mem_idx);
+        tcg_gen_add_tl(source2, dat, source2);
+        tcg_gen_qemu_st64(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOXOR_D: 
+        tcg_gen_qemu_ld64(dat, source1, ctx->mem_idx);
+        tcg_gen_xor_tl(source2, dat, source2);
+        tcg_gen_qemu_st64(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOAND_D:
+        tcg_gen_qemu_ld64(dat, source1, ctx->mem_idx);
+        tcg_gen_and_tl(source2, dat, source2);
+        tcg_gen_qemu_st64(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOOR_D:
+        tcg_gen_qemu_ld64(dat, source1, ctx->mem_idx);
+        tcg_gen_or_tl(source2, dat, source2);
+        tcg_gen_qemu_st64(source2, source1, ctx->mem_idx);
+        tcg_gen_mov_tl(source1, dat);
+        break;
+    case OPC_RISC_AMOMIN_D:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld64(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_LT, dat_l, source2_l, j);
+            tcg_gen_qemu_st64(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st64(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_AMOMAX_D:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld64(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_GT, dat_l, source2_l, j);
+            tcg_gen_qemu_st64(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st64(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_AMOMINU_D:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld64(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_LTU, dat_l, source2_l, j);
+            tcg_gen_qemu_st64(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st64(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    case OPC_RISC_AMOMAXU_D:
+        {
+            TCGv source1_l, source2_l, dat_l;
+            source1_l = tcg_temp_local_new();
+            source2_l = tcg_temp_local_new();
+            dat_l = tcg_temp_local_new();
+            int j = gen_new_label();
+            int done = gen_new_label();
+            tcg_gen_mov_tl(source1_l, source1);
+            tcg_gen_mov_tl(source2_l, source2);
+            tcg_gen_qemu_ld64(dat_l, source1_l, ctx->mem_idx);
+            tcg_gen_brcond_tl(TCG_COND_GTU, dat_l, source2_l, j);
+            tcg_gen_qemu_st64(source2_l, source1_l, ctx->mem_idx);
+            tcg_gen_br(done);
+            // here we store the thing on the left
+            gen_set_label(j);
+            tcg_gen_qemu_st64(dat_l, source1_l, ctx->mem_idx);
+            //done
+            gen_set_label(done);
+            tcg_gen_mov_tl(source1, dat_l);
+            tcg_temp_free(source1_l);
+            tcg_temp_free(source2_l);
+            tcg_temp_free(dat_l);
+        }
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+
+    // set and free
+    gen_set_gpr(rd, source1);
+    tcg_temp_free(source1);
+    tcg_temp_free(source2);
+    tcg_temp_free(dat);
+}
+
+
+inline static void gen_csr_htif(DisasContext *ctx, uint32_t opc, int addr, int rd, int rs1) {
+    TCGv source1, csr_store, htif_addr;
+    source1 = tcg_temp_new();
+    csr_store = tcg_temp_new();
+    htif_addr = tcg_temp_new();
+    gen_get_gpr(source1, rs1); // load rs1 val
+    tcg_gen_movi_tl(htif_addr, addr);
+    tcg_gen_qemu_ld64(csr_store, htif_addr, ctx->mem_idx); // get htif "reg" val
+
+    switch (opc) {
+
+    case OPC_RISC_CSRRW:
+        break;
+    case OPC_RISC_CSRRS:
+        tcg_gen_or_tl(source1, csr_store, source1);
+        break;
+    case OPC_RISC_CSRRC:
+        tcg_gen_not_tl(source1, source1);
+        tcg_gen_and_tl(source1, csr_store, source1);
+        break;
+    case OPC_RISC_CSRRWI:
+        tcg_gen_movi_tl(source1, rs1);
+        break;
+    case OPC_RISC_CSRRSI:
+        tcg_gen_ori_tl(source1, csr_store, rs1);
+        break;
+    case OPC_RISC_CSRRCI:
+        tcg_gen_andi_tl(source1, csr_store, ~((uint64_t)rs1));
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+    tcg_gen_qemu_st64(source1, htif_addr, ctx->mem_idx);
+    gen_set_gpr(rd, csr_store);
+}
+
+inline static void gen_system(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int csr)
+{
+    // get index into csr array
+    int backup_csr = csr;
+
+    csr = csr_regno(csr);
+
+    if (unlikely(backup_csr == 0x50D || backup_csr == 0x505)) {
+        gen_helper_tlb_flush(cpu_env);
+    } else if (unlikely(csr == CSR_TOHOST)) {
+        gen_csr_htif(ctx, opc, 0x400, rd, rs1);
+        return;
+    } else if (unlikely(csr == CSR_FROMHOST)) {
+        gen_csr_htif(ctx, opc, 0x408, rd, rs1);
+        return;
+    }
+
+    TCGv source1, csr_store, dest;
+    source1 = tcg_temp_new();
+    csr_store = tcg_temp_new();
+    dest = tcg_temp_new();
+    gen_get_gpr(source1, rs1);
+    tcg_gen_movi_tl(csr_store, csr); // copy into temp reg to feed to helper
+
+    switch (opc) {
+
+    case OPC_RISC_SCALL:
+        switch (backup_csr) {
+            case 0x0: // SCALL
+                generate_exception(ctx, RISCV_EXCP_SCALL);
+                ctx->bstate = BS_STOP;
+                break;
+            case 0x1: // SBREAK
+                kill_unknown(ctx, RISCV_EXCP_BREAK);
+                ctx->bstate = BS_STOP;
+                break;
+            case 0x800: // SRET
+                gen_helper_sret(cpu_PC, cpu_env);
+                tcg_gen_exit_tb(0); // no chaining
+                ctx->bstate = BS_BRANCH;
+                break;
+            default:
+                kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+                break;
+        }
+        break;
+    case OPC_RISC_CSRRW:
+        gen_helper_csrrw(dest, cpu_env, source1, csr_store);
+        gen_set_gpr(rd, dest);
+        break;
+    case OPC_RISC_CSRRS:
+        gen_helper_csrrs(dest, cpu_env, source1, csr_store);
+        gen_set_gpr(rd, dest);
+        break;
+    case OPC_RISC_CSRRC:
+        gen_helper_csrrc(dest, cpu_env, source1, csr_store);
+        gen_set_gpr(rd, dest);
+        break;
+    case OPC_RISC_CSRRWI:
+        {
+            TCGv imm_rs1 = tcg_temp_new();
+            tcg_gen_movi_tl(imm_rs1, rs1);
+            gen_helper_csrrw(dest, cpu_env, imm_rs1, csr_store);
+            gen_set_gpr(rd, dest);
+            tcg_temp_free(imm_rs1);
+        }
+        break;
+    case OPC_RISC_CSRRSI:
+        {
+            TCGv imm_rs1 = tcg_temp_new();
+            tcg_gen_movi_tl(imm_rs1, rs1);
+            gen_helper_csrrs(dest, cpu_env, imm_rs1, csr_store);
+            gen_set_gpr(rd, dest);
+            tcg_temp_free(imm_rs1);
+        }
+        break;
+    case OPC_RISC_CSRRCI:
+        {
+            TCGv imm_rs1 = tcg_temp_new();
+            tcg_gen_movi_tl(imm_rs1, rs1);
+            gen_helper_csrrc(dest, cpu_env, imm_rs1, csr_store);
+            gen_set_gpr(rd, dest);
+            tcg_temp_free(imm_rs1);
+        }
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+    tcg_temp_free(source1);
+    tcg_temp_free(dest);
+    tcg_temp_free(csr_store);
+}
+
+
+inline static void gen_fp_load(DisasContext *ctx, uint32_t opc, 
+                      int rd, int rs1, int16_t imm)
+{
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+
+    TCGv t0 = tcg_temp_new();
+    gen_get_gpr(t0, rs1);
+    tcg_gen_addi_tl(t0, t0, uimm);
+    
+    switch (opc) {
+
+    case OPC_RISC_FLW:
+        tcg_gen_qemu_ld32u(t0, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_FLD:
+        tcg_gen_qemu_ld64(t0, t0, ctx->mem_idx);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+
+    }
+    tcg_gen_mov_tl(cpu_fpr[rd], t0); // probably can just get rid of this and 
+                                     // store directly to cpu_fpr[rd]
+    tcg_temp_free(t0);
+}
+
+inline static void gen_fp_store(DisasContext *ctx, uint32_t opc, 
+                      int rs1, int rs2, int16_t imm)
+{
+    target_ulong uimm = (target_long)imm; /* sign ext 16->64 bits */
+
+    TCGv t0 = tcg_temp_new();
+    TCGv dat = tcg_temp_new();
+    gen_get_gpr(t0, rs1);
+    tcg_gen_addi_tl(t0, t0, uimm);
+    tcg_gen_mov_tl(dat, cpu_fpr[rs2]);
+
+    switch (opc) {
+
+    case OPC_RISC_FSW:
+        tcg_gen_qemu_st32(dat, t0, ctx->mem_idx);
+        break;
+    case OPC_RISC_FSD:
+        tcg_gen_qemu_st64(dat, t0, ctx->mem_idx);
+        break;
+
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+
+    tcg_temp_free(t0);
+    tcg_temp_free(dat);
+}
+
+inline static void gen_fp_fmadd(DisasContext *ctx, uint32_t opc,
+                    int rd, int rs1, int rs2, int rs3, int rm)
+{
+    TCGv rm_reg = tcg_temp_new();
+    tcg_gen_movi_tl(rm_reg, rm);
+
+    switch (opc) {
+
+    case OPC_RISC_FMADD_S:
+        gen_helper_fmadd_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    case OPC_RISC_FMADD_D:
+        gen_helper_fmadd_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    tcg_temp_free(rm_reg);
+
+}
+
+inline static void gen_fp_fmsub(DisasContext *ctx, uint32_t opc,
+                    int rd, int rs1, int rs2, int rs3, int rm)
+{
+    TCGv rm_reg = tcg_temp_new();
+    tcg_gen_movi_tl(rm_reg, rm);
+
+    switch (opc) {
+
+    case OPC_RISC_FMSUB_S:
+        gen_helper_fmsub_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    case OPC_RISC_FMSUB_D:
+        gen_helper_fmsub_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    tcg_temp_free(rm_reg);
+}
+
+inline static void gen_fp_fnmsub(DisasContext *ctx, uint32_t opc,
+                    int rd, int rs1, int rs2, int rs3, int rm)
+{
+    TCGv rm_reg = tcg_temp_new();
+    tcg_gen_movi_tl(rm_reg, rm);
+
+    switch (opc) {
+
+    case OPC_RISC_FNMSUB_S:
+        gen_helper_fnmsub_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    case OPC_RISC_FNMSUB_D:
+        gen_helper_fnmsub_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    tcg_temp_free(rm_reg);
+}
+
+inline static void gen_fp_fnmadd(DisasContext *ctx, uint32_t opc,
+                    int rd, int rs1, int rs2, int rs3, int rm)
+{
+    TCGv rm_reg = tcg_temp_new();
+    tcg_gen_movi_tl(rm_reg, rm);
+
+    switch (opc) {
+
+    case OPC_RISC_FNMADD_S:
+        gen_helper_fnmadd_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    case OPC_RISC_FNMADD_D:
+        gen_helper_fnmadd_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], cpu_fpr[rs3], rm_reg);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    tcg_temp_free(rm_reg);
+}
+
+inline static void gen_fp_arith(DisasContext *ctx, uint32_t opc, 
+                    int rd, int rs1, int rs2, int rm) 
+{
+    TCGv rm_reg = tcg_temp_new();
+    TCGv write_int_rd = tcg_temp_new();
+    tcg_gen_movi_tl(rm_reg, rm);
+
+    switch (opc) {
+    case OPC_RISC_FADD_S:
+        gen_helper_fadd_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FSUB_S:
+        gen_helper_fsub_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FMUL_S:
+        gen_helper_fmul_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FDIV_S:
+        gen_helper_fdiv_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FSGNJ_S:
+        // also handles: OPC_RISC_FSGNJN_S, OPC_RISC_FSGNJX_S  
+        // TODO: probably don't need to use helpers here
+        if (rm == 0x0) {
+            gen_helper_fsgnj_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);  
+        } else if (rm == 0x1) {
+            gen_helper_fsgnjn_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);  
+        } else if (rm == 0x2) {
+            gen_helper_fsgnjx_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);  
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+    case OPC_RISC_FMIN_S:
+        // also handles: OPC_RISC_FMAX_S
+        if (rm == 0x0) {
+            gen_helper_fmin_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else if (rm == 0x1) {
+            gen_helper_fmax_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+
+    case OPC_RISC_FSQRT_S:
+        gen_helper_fsqrt_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], rm_reg);
+        break;
+
+    case OPC_RISC_FEQ_S:
+        // also handles: OPC_RISC_FLT_S, OPC_RISC_FLE_S
+        if (rm == 0x0) {
+            gen_helper_fle_s(write_int_rd, cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else if (rm == 0x1) {
+            gen_helper_flt_s(write_int_rd, cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else if (rm == 0x2) {
+            gen_helper_feq_s(write_int_rd, cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        gen_set_gpr(rd, write_int_rd);
+        break;
+
+    case OPC_RISC_FCVT_W_S:
+        // also OPC_RISC_FCVT_WU_S, OPC_RISC_FCVT_L_S, OPC_RISC_FCVT_LU_S
+        if (rs2 == 0x0) { // FCVT_W_S
+            gen_helper_fcvt_w_s(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else if (rs2 == 0x1) { // FCVT_WU_S
+            gen_helper_fcvt_wu_s(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else if (rs2 == 0x2) { // FCVT_L_S
+            gen_helper_fcvt_l_s(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else if (rs2 == 0x3) { // FCVT_LU_S
+            gen_helper_fcvt_lu_s(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        gen_set_gpr(rd, write_int_rd);
+        break;
+
+    case OPC_RISC_FCVT_S_W:
+        // also OPC_RISC_FCVT_S_WU, OPC_RISC_FCVT_S_L, OPC_RISC_FCVT_S_LU
+        gen_get_gpr(write_int_rd, rs1);
+        if (rs2 == 0) { // FCVT_S_W
+            gen_helper_fcvt_s_w(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else if (rs2 == 0x1) { // FCVT_S_WU
+            gen_helper_fcvt_s_wu(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else if (rs2 == 0x2) { // FCVT_S_L
+            gen_helper_fcvt_s_l(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else if (rs2 == 0x3) { // FCVT_S_LU
+            gen_helper_fcvt_s_lu(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+
+    case OPC_RISC_FMV_X_S:
+        // also OPC_RISC_FCLASS_S
+        if (rm == 0x0) { // FMV
+            tcg_gen_ext32s_tl(write_int_rd, cpu_fpr[rs1]);
+        } else if (rm == 0x1) {
+            gen_helper_fclass_s(write_int_rd, cpu_env, cpu_fpr[rs1]);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        gen_set_gpr(rd, write_int_rd);
+        break;
+
+    case OPC_RISC_FMV_S_X:
+        gen_get_gpr(write_int_rd, rs1);
+        tcg_gen_mov_tl(cpu_fpr[rd], write_int_rd);
+        break;
+
+    // double
+    case OPC_RISC_FADD_D:
+        gen_helper_fadd_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FSUB_D:
+        gen_helper_fsub_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FMUL_D:
+        gen_helper_fmul_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FDIV_D:
+        gen_helper_fdiv_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2], rm_reg);
+        break;
+    case OPC_RISC_FSGNJ_D:
+        // also OPC_RISC_FSGNJN_D, OPC_RISC_FSGNJX_D  
+        if (rm == 0x0) {
+            gen_helper_fsgnj_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);  
+        } else if (rm == 0x1) {
+            gen_helper_fsgnjn_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);  
+        } else if (rm == 0x2) {
+            gen_helper_fsgnjx_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);  
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+    case OPC_RISC_FMIN_D:
+        // also OPC_RISC_FMAX_D    
+        if (rm == 0x0) {
+            gen_helper_fmin_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else if (rm == 0x1) {
+            gen_helper_fmax_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+    case OPC_RISC_FCVT_S_D:
+        if (rs2 == 0x1) {
+            gen_helper_fcvt_s_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], rm_reg);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+    case OPC_RISC_FCVT_D_S:
+        if (rs2 == 0x0) {
+            gen_helper_fcvt_d_s(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], rm_reg);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+    case OPC_RISC_FSQRT_D:
+        gen_helper_fsqrt_d(cpu_fpr[rd], cpu_env, cpu_fpr[rs1], rm_reg);
+        break;
+    case OPC_RISC_FEQ_D:
+        // also OPC_RISC_FLT_D, OPC_RISC_FLE_D     
+        if (rm == 0x0) {
+            gen_helper_fle_d(write_int_rd, cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else if (rm == 0x1) {
+            gen_helper_flt_d(write_int_rd, cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else if (rm == 0x2) {
+            gen_helper_feq_d(write_int_rd, cpu_env, cpu_fpr[rs1], cpu_fpr[rs2]);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        gen_set_gpr(rd, write_int_rd);
+        break;
+    case OPC_RISC_FCVT_W_D:
+        // also OPC_RISC_FCVT_WU_D, OPC_RISC_FCVT_L_D, OPC_RISC_FCVT_LU_D 
+        if (rs2 == 0x0) {
+            gen_helper_fcvt_w_d(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else if (rs2 == 0x1) {
+            gen_helper_fcvt_wu_d(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else if (rs2 == 0x2) {
+            gen_helper_fcvt_l_d(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else if (rs2 == 0x3) {
+            gen_helper_fcvt_lu_d(write_int_rd, cpu_env, cpu_fpr[rs1], rm_reg);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        gen_set_gpr(rd, write_int_rd);
+        break;
+    case OPC_RISC_FCVT_D_W:
+        // also OPC_RISC_FCVT_D_WU, OPC_RISC_FCVT_D_L, OPC_RISC_FCVT_D_LU
+        gen_get_gpr(write_int_rd, rs1);
+        if (rs2 == 0x0) {
+            gen_helper_fcvt_d_w(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else if (rs2 == 0x1) {
+            gen_helper_fcvt_d_wu(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else if (rs2 == 0x2) {
+            gen_helper_fcvt_d_l(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else if (rs2 == 0x3) {
+            gen_helper_fcvt_d_lu(cpu_fpr[rd], cpu_env, write_int_rd, rm_reg);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        break;
+    case OPC_RISC_FMV_X_D:
+        // also OPC_RISC_FCLASS_D  
+        if (rm == 0x0) { // FMV
+            tcg_gen_mov_tl(write_int_rd, cpu_fpr[rs1]);
+        } else if (rm == 0x1) {
+            gen_helper_fclass_d(write_int_rd, cpu_env, cpu_fpr[rs1]);
+        } else {
+            kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        }
+        gen_set_gpr(rd, write_int_rd);
+        break;
+    case OPC_RISC_FMV_D_X:
+        gen_get_gpr(write_int_rd, rs1);
+        tcg_gen_mov_tl(cpu_fpr[rd], write_int_rd);
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+    tcg_temp_free(rm_reg);
+    tcg_temp_free(write_int_rd);
+}
+
+#define GET_B_IMM(inst)              ((int16_t)((((inst >> 25) & 0x3F) << 5) | ((((int32_t)inst) >> 31) << 12) | (((inst >> 8) & 0xF) << 1) | (((inst >> 7) & 0x1) << 11)))  /* THIS BUILDS 13 bit imm (implicit zero is tacked on here), also note that bit #12 is obtained in a special way to get sign extension */
+#define GET_STORE_IMM(inst)           ((int16_t)(((((int32_t)inst) >> 25) << 5) | ((inst >> 7) & 0x1F)))
+#define GET_JAL_IMM(inst)             ((int32_t)((inst & 0xFF000) | (((inst >> 20) & 0x1) << 11) | (((inst >> 21) & 0x3FF) << 1) | ((((int32_t)inst) >> 31) << 20)))
+#define GET_RM(inst)                  ((inst >> 12) & 0x7)
+#define GET_RS3(inst)                 ((inst >> 27) & 0x1F)
+static void decode_opc (CPURISCVState *env, DisasContext *ctx)
+{
+    int rs1;
+    int rs2;
+    int rd;
+    uint32_t op;
+    int16_t imm;
+    target_ulong ubimm;
+
+    /* make sure instructions are on a word boundary */
+    if (unlikely(ctx->pc & 0x3)) { 
+        // NOT tested for RISCV
+        printf("misaligned instruction, not yet implemented for riscv\n");
+        exit(1);
+        return;
+    }
+
+    op = MASK_OP_MAJOR(ctx->opcode);
+    rs1 = (ctx->opcode >> 15) & 0x1f;
+    rs2 = (ctx->opcode >> 20) & 0x1f;
+    rd = (ctx->opcode >> 7) & 0x1f;
+    imm = (int16_t)(((int32_t)ctx->opcode) >> 20); /* sign extends */
+    switch (op) {
+
+    case OPC_RISC_LUI:
+        if (rd == 0) {
+            break; // NOP
+        }
+        tcg_gen_movi_tl(cpu_gpr[rd], (ctx->opcode & 0xFFFFF000));
+        tcg_gen_ext32s_tl(cpu_gpr[rd], cpu_gpr[rd]);
+        break;
+    case OPC_RISC_AUIPC:
+        if (rd == 0) {
+            break; // NOP
+        }
+        tcg_gen_movi_tl(cpu_gpr[rd], (ctx->opcode & 0xFFFFF000));
+        tcg_gen_ext32s_tl(cpu_gpr[rd], cpu_gpr[rd]);
+        tcg_gen_add_tl(cpu_gpr[rd], cpu_gpr[rd], tcg_const_tl(ctx->pc));
+        break;
+    case OPC_RISC_JAL:
+        ubimm = (target_long) (GET_JAL_IMM(ctx->opcode));
+        if (rd != 0) {
+            tcg_gen_movi_tl(cpu_gpr[rd], 4);
+            tcg_gen_addi_tl(cpu_gpr[rd], cpu_gpr[rd], ctx->pc);
+        }
+#ifdef DISABLE_CHAINING_JAL
+        tcg_gen_movi_tl(cpu_PC, ctx->pc + ubimm);
+        tcg_gen_exit_tb(0);
+#else
+        gen_goto_tb(ctx, 0, ctx->pc + ubimm); // must use this for safety
+#endif
+        ctx->bstate = BS_BRANCH;
+        break;
+    case OPC_RISC_JALR:
+        gen_jalr(ctx, MASK_OP_JALR(ctx->opcode), rd, rs1, imm);
+        break;
+    case OPC_RISC_BRANCH:
+        gen_branch(ctx, MASK_OP_BRANCH(ctx->opcode), rs1, rs2, GET_B_IMM(ctx->opcode));
+        break;
+    case OPC_RISC_LOAD:
+        gen_load(ctx, MASK_OP_LOAD(ctx->opcode), rd, rs1, imm);
+        break;
+    case OPC_RISC_STORE:
+        gen_store(ctx, MASK_OP_STORE(ctx->opcode), rs1, rs2, GET_STORE_IMM(ctx->opcode));
+        break;
+    case OPC_RISC_ARITH_IMM:
+        if (rd == 0) {
+            break; // NOP
+        }
+        gen_arith_imm(ctx, MASK_OP_ARITH_IMM(ctx->opcode), rd, rs1, imm);
+        break;
+    case OPC_RISC_ARITH:
+        if (rd == 0) {
+            break; // NOP
+        }
+        gen_arith(ctx, MASK_OP_ARITH(ctx->opcode), rd, rs1, rs2);
+        break;
+    case OPC_RISC_ARITH_IMM_W:
+        if (rd == 0) {
+            break; // NOP
+        }
+        gen_arith_imm_w(ctx, MASK_OP_ARITH_IMM_W(ctx->opcode), rd, rs1, imm);
+        break;
+    case OPC_RISC_ARITH_W:
+        if (rd == 0) {
+            break; // NOP
+        }
+        gen_arith_w(ctx, MASK_OP_ARITH_W(ctx->opcode), rd, rs1, rs2);
+        break;
+    case OPC_RISC_FENCE:
+        /* fences are nops for us */
+        break;
+    case OPC_RISC_SYSTEM:
+        gen_system(ctx, MASK_OP_SYSTEM(ctx->opcode), rd, rs1, (ctx->opcode & 0xFFF00000) >> 20);
+        break;
+    case OPC_RISC_ATOMIC:
+        gen_atomic(ctx, MASK_OP_ATOMIC(ctx->opcode), rd, rs1, rs2);
+        break;        
+    case OPC_RISC_FP_LOAD:
+        gen_fp_load(ctx, MASK_OP_FP_LOAD(ctx->opcode), rd, rs1, imm);
+        break;
+    case OPC_RISC_FP_STORE:
+        gen_fp_store(ctx, MASK_OP_FP_STORE(ctx->opcode), rs1, rs2, GET_STORE_IMM(ctx->opcode));
+        break;
+    case OPC_RISC_FMADD:
+        gen_fp_fmadd(ctx, MASK_OP_FP_FMADD(ctx->opcode), rd, rs1, rs2, GET_RS3(ctx->opcode), GET_RM(ctx->opcode));
+        break;
+    case OPC_RISC_FMSUB:
+        gen_fp_fmsub(ctx, MASK_OP_FP_FMSUB(ctx->opcode), rd, rs1, rs2, GET_RS3(ctx->opcode), GET_RM(ctx->opcode));
+        break;
+    case OPC_RISC_FNMSUB:
+        gen_fp_fnmsub(ctx, MASK_OP_FP_FNMSUB(ctx->opcode), rd, rs1, rs2, GET_RS3(ctx->opcode), GET_RM(ctx->opcode));
+        break;
+    case OPC_RISC_FNMADD:
+        gen_fp_fnmadd(ctx, MASK_OP_FP_FNMADD(ctx->opcode), rd, rs1, rs2, GET_RS3(ctx->opcode), GET_RM(ctx->opcode));
+        break;
+    case OPC_RISC_FP_ARITH:
+        gen_fp_arith(ctx, MASK_OP_FP_ARITH(ctx->opcode), rd, rs1, rs2, GET_RM(ctx->opcode));
+        break;
+    default:
+        kill_unknown(ctx, RISCV_EXCP_ILLEGAL_INST);
+        break;
+    }
+}
+
+static inline void
+gen_intermediate_code_internal(RISCVCPU *cpu, TranslationBlock *tb,
+                               bool search_pc)
+{
+    CPUState *cs = CPU(cpu);
+    CPURISCVState *env = &cpu->env;
+    DisasContext ctx;
+    target_ulong pc_start;
+    uint16_t *gen_opc_end;
+    CPUBreakpoint *bp;
+    int j, lj = -1;
+    int num_insns;
+    int max_insns;
+    if (search_pc) {
+        qemu_log("search pc %d\n", search_pc);
+    }
+    pc_start = tb->pc;
+    gen_opc_end = tcg_ctx.gen_opc_buf + OPC_MAX_SIZE;
+    ctx.pc = pc_start;
+    ctx.singlestep_enabled = cs->singlestep_enabled;
+    ctx.tb = tb;
+    ctx.bstate = BS_NONE;
+#ifdef CONFIG_USER_ONLY
+        ctx.mem_idx = 0;
+#else
+        ctx.mem_idx = env->helper_csr[CSR_STATUS] & SR_S;
+#endif
+    num_insns = 0;
+    max_insns = tb->cflags & CF_COUNT_MASK;
+    if (max_insns == 0) {
+        max_insns = CF_COUNT_MASK;
+    }
+    gen_tb_start();
+    while (ctx.bstate == BS_NONE) {
+        if (unlikely(!QTAILQ_EMPTY(&cs->breakpoints))) {
+            QTAILQ_FOREACH(bp, &cs->breakpoints, entry) {
+                if (bp->pc == ctx.pc) {
+                    tcg_gen_movi_tl(cpu_PC, ctx.pc);
+                    ctx.bstate = BS_BRANCH;
+                    TCGv_i32 helper_tmp = tcg_const_i32(EXCP_DEBUG);
+                    gen_helper_raise_exception(cpu_env, helper_tmp);
+                    tcg_temp_free_i32(helper_tmp);
+                    ctx.pc += 4;
+                    goto done_generating;
+                }
+            }
+        }
+
+        if (search_pc) {
+            j = tcg_ctx.gen_opc_ptr - tcg_ctx.gen_opc_buf;
+            if (lj < j) {
+                lj++;
+                while (lj < j)
+                    tcg_ctx.gen_opc_instr_start[lj++] = 0;
+            }
+            tcg_ctx.gen_opc_pc[lj] = ctx.pc;
+            tcg_ctx.gen_opc_instr_start[lj] = 1;
+            tcg_ctx.gen_opc_icount[lj] = num_insns;
+        }
+        if (num_insns + 1 == max_insns && (tb->cflags & CF_LAST_IO)) {
+            gen_io_start();
+        }
+
+        ctx.opcode = cpu_ldl_code(env, ctx.pc);
+        decode_opc(env, &ctx);
+        ctx.pc += 4;
+        num_insns++;
+
+        if (unlikely((ctx.pc & (TARGET_PAGE_SIZE - 1)) == 0)) { 
+            // handle tb at the end of a page
+            break;
+        }
+        if (unlikely(tcg_ctx.gen_opc_ptr >= gen_opc_end)) {
+            break;
+        }
+        if (unlikely(num_insns >= max_insns)) {
+            break;
+        }
+        if (unlikely(singlestep)) {
+            printf("singlestep is not currently supported for riscv\n");
+            exit(1);
+            break;
+        }
+    }
+    if (tb->cflags & CF_LAST_IO) {
+        gen_io_end();
+    }
+    switch (ctx.bstate) {
+    case BS_STOP:
+        gen_goto_tb(&ctx, 0, ctx.pc);
+        break;
+    case BS_NONE:
+        // DO NOT CHAIN. This is for END-OF-PAGE. See gen_goto_tb.
+        tcg_gen_movi_tl(cpu_PC, ctx.pc); // NOT PC+4, that was already done
+        tcg_gen_exit_tb(0);
+        break;
+    case BS_BRANCH:
+    default:
+        break;
+    }
+done_generating:
+    gen_tb_end(tb, num_insns);
+    *tcg_ctx.gen_opc_ptr = INDEX_op_end;
+    if (search_pc) {
+        j = tcg_ctx.gen_opc_ptr - tcg_ctx.gen_opc_buf;
+        lj++;
+        while (lj <= j)
+            tcg_ctx.gen_opc_instr_start[lj++] = 0;
+    } else {
+        tb->size = ctx.pc - pc_start;
+        tb->icount = num_insns;
+    }
+#ifdef DEBUG_DISAS // TODO: riscv disassembly
+    LOG_DISAS("\n");
+    if (qemu_loglevel_mask(CPU_LOG_TB_IN_ASM)) {
+        qemu_log("IN: %s\n", lookup_symbol(pc_start));
+        log_target_disas(env, pc_start, ctx.pc - pc_start, 0);
+        qemu_log("\n");
+    }
+#endif
+}
+
+void gen_intermediate_code (CPURISCVState *env, struct TranslationBlock *tb)
+{
+    gen_intermediate_code_internal(riscv_env_get_cpu(env), tb, false);
+}
+
+void gen_intermediate_code_pc (CPURISCVState *env, struct TranslationBlock *tb)
+{
+    gen_intermediate_code_internal(riscv_env_get_cpu(env), tb, true);
+}
+
+void riscv_cpu_dump_state(CPUState *cs, FILE *f, fprintf_function cpu_fprintf,
+                         int flags)
+{
+    RISCVCPU *cpu = RISCV_CPU(cs);
+    CPURISCVState *env = &cpu->env;
+    int i;
+
+    cpu_fprintf(f, "pc=0x" TARGET_FMT_lx "\n", env->active_tc.PC);
+    for (i = 0; i < 32; i++) {
+        if ((i & 3) == 0) {
+            cpu_fprintf(f, "GPR%02d:", i);
+        }
+        cpu_fprintf(f, " %s " TARGET_FMT_lx, regnames[i], env->active_tc.gpr[i]);
+        if ((i & 3) == 3) {
+            cpu_fprintf(f, "\n");
+        }
+    }
+    for (i = 0; i < 32; i++) {
+        if ((i & 3) == 0) {
+            cpu_fprintf(f, "CSR%02d:", i);
+        }
+        if (i == CSR_COUNT) {
+            cpu_fprintf(f, " %s " TARGET_FMT_lx, cs_regnames[i], (target_ulong)cpu_riscv_get_count(env));
+
+        } else if (i == CSR_CYCLE) {
+            cpu_fprintf(f, " %s " TARGET_FMT_lx, cs_regnames[i], cpu_riscv_get_cycle(env));
+        } else if (i == CSR_FCSR) {
+             cpu_fprintf(f, " %s " TARGET_FMT_lx, cs_regnames[i], env->helper_csr[CSR_FFLAGS] | (env->helper_csr[CSR_FRM] << 5));
+        } else {
+            cpu_fprintf(f, " %s " TARGET_FMT_lx, cs_regnames[i], env->helper_csr[i]);
+        }
+        if ((i & 3) == 3) {
+            cpu_fprintf(f, "\n");
+        }
+    }
+
+    for (i = 0; i < 32; i++) {
+        if ((i & 3) == 0) {
+            cpu_fprintf(f, "FPR%02d:", i);
+        }
+        cpu_fprintf(f, " %s " TARGET_FMT_lx, fpr_regnames[i], env->active_tc.fpr[i]);
+        if ((i & 3) == 3) {
+            cpu_fprintf(f, "\n");
+        }
+    }
+}
+
+void riscv_tcg_init(void)
+{
+    int i;
+    static int inited;
+
+    /* Initialize various static tables. */
+    if (inited) {
+        return;
+    }
+
+    cpu_env = tcg_global_reg_new_ptr(TCG_AREG0, "env");
+
+    // WARNING: cpu_gpr[0] is not allocated ON PURPOSE. Do not use it.
+    // Use the gen_set_gpr and gen_get_gpr helper functions when accessing
+    // registers, unless you specifically block reads/writes to reg 0
+    TCGV_UNUSED(cpu_gpr[0]);
+    for (i = 1; i < 32; i++) {
+        cpu_gpr[i] = tcg_global_mem_new(TCG_AREG0,
+                                        offsetof(CPURISCVState, active_tc.gpr[i]),
+                                        regnames[i]);
+    }
+
+    for (i = 0; i < 32; i++) {
+        cpu_fpr[i] = tcg_global_mem_new(TCG_AREG0,
+                                        offsetof(CPURISCVState, active_tc.fpr[i]),
+                                        fpr_regnames[i]);
+    }
+
+    cpu_PC = tcg_global_mem_new(TCG_AREG0,
+                                offsetof(CPURISCVState, active_tc.PC), "PC");
+    inited = 1;
+}
+
+#include "translate_init.c"
+
+RISCVCPU *cpu_riscv_init(const char *cpu_model)
+{
+    RISCVCPU *cpu;
+    CPURISCVState *env;
+    const riscv_def_t *def;
+
+    def = cpu_riscv_find_by_name(cpu_model);
+    if (!def)
+        return NULL;
+    cpu = RISCV_CPU(object_new(TYPE_RISCV_CPU));
+    env = &cpu->env;
+    env->cpu_model = def;
+
+    // set status register from def 
+    env->helper_csr[CSR_STATUS] = def->init_status_reg;
+
+    object_property_set_bool(OBJECT(cpu), true, "realized", NULL);
+
+    return cpu;
+}
+
+void cpu_state_reset(CPURISCVState *env)
+{
+    RISCVCPU *cpu = riscv_env_get_cpu(env);
+    CPUState *cs = CPU(cpu);
+
+    env->active_tc.PC = RISCV_START_PC; // STARTING PC VALUE def'd in cpu.h
+    cs->exception_index = EXCP_NONE;
+}
+
+void restore_state_to_opc(CPURISCVState *env, TranslationBlock *tb, int pc_pos)
+{
+    env->active_tc.PC = tcg_ctx.gen_opc_pc[pc_pos];
+}
diff --git a/target-riscv/translate_init.c b/target-riscv/translate_init.c
new file mode 100644
index 0000000..49d76fc
--- /dev/null
+++ b/target-riscv/translate_init.c
@@ -0,0 +1,52 @@
+/*
+ *  RISC-V emulation for qemu: CPU initialisation routines.
+ *
+ *  Author: Sagar Karandikar, skarandikar@berkeley.edu
+ *  Based on the MIPS target
+ *
+ * This library is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This library is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this library; if not, see <http://www.gnu.org/licenses/>.
+ */
+
+struct riscv_def_t {
+    const char *name;
+    uint64_t init_status_reg;
+};
+
+/* RISC-V CPU definitions */
+static const riscv_def_t riscv_defs[] =
+{
+    {  
+        .name = "riscv-generic",
+        .init_status_reg = SR_S64 | SR_U64 | SR_EF | SR_S,
+    },
+};
+
+static const riscv_def_t *cpu_riscv_find_by_name (const char *name)
+{
+    int i;
+    for (i = 0; i < ARRAY_SIZE(riscv_defs); i++) {
+        if (strcasecmp(name, riscv_defs[i].name) == 0) {
+            return &riscv_defs[i];
+        }
+    }
+    return NULL;
+}
+
+void riscv_cpu_list (FILE *f, fprintf_function cpu_fprintf)
+{
+    int i;
+    for (i = 0; i < ARRAY_SIZE(riscv_defs); i++) {
+        (*cpu_fprintf)(f, "RISCV '%s'\n", riscv_defs[i].name);
+    }
+}
